# Day 2 SFT QLoRAå¾®è°ƒå®æˆ˜

[TOC]



## å‰ç½®ï¼šæ¦‚å¿µé€Ÿè®²

**assistant-only loss maskï¼ˆä»…åŠ©æ‰‹ä¾§æŸå¤±ï¼‰**
 SFT æœ¬è´¨æ˜¯å› æœè¯­è¨€å»ºæ¨¡ï¼Œä½†åªå¯¹â€œåŠ©æ‰‹è§’è‰²ï¼ˆassistantï¼‰â€äº§ç”Ÿçš„ token è®¡æŸå¤±ï¼›å¯¹ user/systemï¼ˆä»¥åŠ prompt éƒ¨åˆ†ï¼‰ä¸è®¡æŸå¤±ã€‚è¿™æ ·å¯ä»¥é¿å…æ¨¡å‹å»â€œèƒŒè¯µâ€ç”¨æˆ·è¾“å…¥ï¼Œå¼ºåŒ–â€œä»æŒ‡ä»¤åˆ°å›å¤â€çš„æ˜ å°„ã€‚å®ç°ä¸Šé€šå¸¸æŠŠé assistant æ®µçš„ `labels` ç½® `-100`ï¼ˆå³å¿½ç•¥æ¢¯åº¦ï¼‰å³å¯ã€‚ä½ çš„è®¡åˆ’æ–‡æ¡£ä¹Ÿæ˜ç¡®äº† Day2 çš„è¿™ä¸ªè¦ç‚¹ï¼šSFT çš„æœ¬è´¨=å› æœ LM + assistant-only loss maskï¼Œå¹¶è¯´æ˜â€œä¸å¯¹ user/system è®¡æŸå¤±ï¼ˆé¿å…å¤è¿°ï¼‰â€  ã€‚

**QLoRAï¼ˆ4-bit é‡åŒ– + LoRA ä½ç§©é€‚é…ï¼‰**
 æ ¸å¿ƒæ˜¯ï¼šæŠŠ**åŸºåº§æ¨¡å‹æƒé‡**åš 4-bitï¼ˆnf4ï¼‰é‡åŒ–å¹¶å†»ç»“ï¼Œåªè®­ç»ƒæ’åœ¨æŸäº›çº¿æ€§å±‚ä¸Šçš„ **LoRA ä½ç§©çŸ©é˜µ**ï¼ˆrank=rï¼‰ï¼Œå†é…åˆç¼©æ”¾ç³»æ•°ï¼ˆÎ±ï¼‰å’Œ LoRA dropout æ§åˆ¶å®¹é‡/æ­£åˆ™ã€‚å…ˆé‡åŒ–ï¼Œå†loraå¾®è°ƒã€‚ä¼˜ç‚¹ï¼šæ˜¾è‘—é™æ˜¾å­˜ï¼ˆå•å¡ 16 GB å°±èƒ½è®­ 7B/8Bï¼‰ï¼Œè®­ç»ƒç¨³å®šã€å¯æ‹”æ’å›é€€ï¼›ä½ çš„è®¡åˆ’æ–‡æ¡£å¼ºè°ƒäº†â€œLoRA ä½ç§©è¿‘ä¼¼ï¼Œr/Î±/Dropout çš„ä½œç”¨ï¼›QLoRA çš„ 4-bit nf4 é‡åŒ–ä¸è¯¯å·®è¡¥å¿â€  ã€‚åœ¨ Day1 é‡Œä¹Ÿç»™äº† 4080 å‹å¥½çš„åŸºåº§å»ºè®®ï¼šQwen2.5-7B-Instruct æˆ– Llama-3.1-8B-Instruct + QLoRA  ã€‚

è‹±æ–‡ç‰ˆï¼Œæ¨èHFå®˜æ–¹ï¼šhttps://huggingface.co/docs/trl/main/en/sft_trainer

---

## 0. ç›®æ ‡ä¸äº§å‡º

**ç›®æ ‡**
 åœ¨å•å¡ 4080 ä¸Šï¼Œå¯¹ 4B æŒ‡ä»¤åŸºåº§ï¼ˆæ¨èï¼šQwen3-4B-Instruct-2507 ï¼‰åšä¸€æ¬¡é«˜è´¨é‡ SFTï¼Œé‡‡ç”¨ **assistant-only CE + QLoRA**ï¼Œå¾—åˆ°ä¸€ä¸ªå¯æ‹”æ’çš„ **LoRA é€‚é…å™¨**ï¼Œå¹¶äº§å‡ºä¸€å¼ **åŸºçº¿è¯„æµ‹è¡¨**ï¼ˆPPLã€å¯ç”¨æ€§ã€JSON åˆæ³•ç‡ï¼‰ã€‚ 

**ç‰©ç†äº§å‡º**

- `checkpoints/sft-lora-*`ï¼ˆLoRA é€‚é…å™¨ï¼‰
- `eval/`ï¼ˆè®­ç»ƒ/è¯„æµ‹æ—¥å¿—ï¼‰

------

## 1. ç¯å¢ƒå›ºå®šï¼ˆLinux + CUDA 12.xï¼‰

```shell
conda create -n llm python=3.11 -y
conda activate llm

pip install --upgrade "torch>=2.3" "transformers>=4.43" "datasets>=2.20" \
  "accelerate>=0.32" "trl>=0.9" "peft>=0.12" "bitsandbytes>=0.43" "evaluate>=0.4" "tqdm"
  
python -m pip install --upgrade unsloth unsloth_zoo # ä½¿ç”¨unsloth
python -m pip install bitsandbytes vllm # éƒ¨ç½²å’Œæµ‹è¯•æ¨¡å‹
```

------

## 2. é¡¹ç›®ç»“æ„ä¸æ•°æ®è§„èŒƒ

```shell
repo/
  data/
    train.jsonl         # å¤šè½®å¯¹è¯æŒ‡ä»¤æ•°æ®
    val.jsonl
  sft/
    train_sft.py        # è®­ç»ƒè„šæœ¬ï¼ˆå« QLoRA + assistant-onlyï¼‰
    eval_sanity.py      # å°è¯„æµ‹ï¼šå¯ç”¨æ€§ & JSON åˆæ³•ç‡
  runs/
    day2_metrics.csv
  checkpoints/
    sft-lora-qwen7b/
```

### 2.1 æ•°æ®æ ¼å¼ï¼ˆchat æ ¼å¼ï¼Œå¼ºåˆ¶ role æ˜ç¡®ï¼‰

#### 1ï¼‰ æ–‡ä»¶ç»„ç»‡ä¸åŸºæœ¬æ ¼å¼

- **å­˜å‚¨æ ¼å¼**ï¼š`train.jsonl` / `val.jsonl`ï¼ˆä¸€è¡Œä¸€æ¡æ ·æœ¬ï¼ŒUTF-8ï¼‰ã€‚
- **å­—æ®µ**ï¼šæœ€å°‘åŒ…å«
  - `id`ï¼šå¯é€‰
  - `messages`: `[{role, content, ...}, ...]`ï¼ˆä¸¥æ ¼åŒºåˆ† `system` / `user` / `assistant` / `tool`ï¼‰
  - å¯é€‰å…ƒæ•°æ®ï¼š`tags`ã€`domain`ã€`difficulty` ç­‰ï¼Œä¾¿äºæŠ½æ ·ä¸è¯„æµ‹åˆ†æ¡¶ã€‚
- **ç”Ÿæˆæ–‡æœ¬**ï¼šè®­ç»ƒæ—¶ç”¨å„è‡ªæ¨¡å‹çš„ **chat template** å°† `messages` æ¸²æŸ“ä¸ºåºåˆ—ï¼›Qwen å®˜æ–¹æ–‡æ¡£å¼ºè°ƒ**å¿…é¡»ä½¿ç”¨æŒ‡å®šæ¨¡æ¿**ï¼Œå¦åˆ™è¡Œä¸ºä¸å¯æ§ã€‚[qwen.readthedocs.io](https://qwen.readthedocs.io/en/latest/getting_started/concepts.html)

#### 2ï¼‰æœ€å°å¯ç”¨æ¨¡æ¿ï¼ˆå•è½®æŒ‡ä»¤ï¼‰

```json
{"id":"sft-0001",
 "messages":[
   {"role":"system","content":"You are a helpful assistant."},
   {"role":"user","content":"ç®€è¦è§£é‡Šä»€ä¹ˆæ˜¯ QLoRAï¼Œå¹¶ç»™å‡º 1 ä¸ªä¼˜ç‚¹å’Œ 1 ä¸ªæ³¨æ„ç‚¹ã€‚"},
   {"role":"assistant","content":"QLoRA å°†åŸºåº§æƒé‡é‡åŒ–ä¸º4bitå¹¶å†»ç»“ï¼Œåªè®­ç»ƒæ’å…¥çš„LoRAä½ç§©... ä¼˜ç‚¹ï¼šæ˜¾å­˜å ç”¨ä½ï¼›æ³¨æ„ï¼šéœ€æ­£ç¡®é€‰æ‹©rankä¸é‡åŒ–dtypeã€‚"}
 ]}
```

- è®­ç»ƒæ—¶ï¼š`tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)` å¾—åˆ°è®­ç»ƒæ–‡æœ¬ï¼›
- **assistant-only mask**ï¼šåªå¯¹æœ€åä¸€æ¡ `assistant` æ–‡æœ¬è®¡æŸå¤±ï¼ˆé assistant æ®µ `labels=-100`ï¼‰ï¼Œç”¨ `DataCollatorForCompletionOnlyLM(response_template=...)` æˆ–è‡ªå®šä¹‰ collator å®ç°ï¼ˆQwen3 æ¨¡æ¿ç»†èŠ‚è§ä¸‹æ–‡ï¼‰ã€‚HF çš„ chat-templating æ–‡æ¡£ä¸ç¤¾åŒºè®¨è®ºå±•ç¤ºäº† Qwen å®¶æ—æ¨¡æ¿å¦‚ä½•æ’å…¥ `<|im_start|>...<|im_end|>` è¿™ç±» turn æ ‡è®°ï¼Œå¯ç”¨äºå®šä½å“åº”è¾¹ç•Œã€‚[Hugging Face](https://huggingface.co/docs/transformers/en/chat_templating)

#### 3ï¼‰å¤šè½®å¯¹è¯æ¨¡æ¿ï¼ˆä¿ç•™ä¸Šä¸‹æ–‡ï¼Œæœ€åä¸€è½®è®¡æŸå¤±ï¼‰

```json
{"id":"sft-0103",
 "messages":[
   {"role":"system","content":"You are a precise and concise assistant."},
   {"role":"user","content":"æˆ‘æœ‰ä¸€ä»½ CSVï¼Œåˆ—æ˜¯ name, scoreï¼Œå¦‚ä½•ç”¨ Python è¯»ï¼Ÿ"},
   {"role":"assistant","content":"å¯ä»¥ä½¿ç”¨ pandas: import pandas as pd; df = pd.read_csv('file.csv') ..."},
   {"role":"user","content":"å†ç»™ä¸€ä¸ªä¸ä¾èµ– pandas çš„çº¯ Python ç‰ˆæœ¬ã€‚"},
   {"role":"assistant","content":"å¯ä»¥ç”¨å†…ç½® csv æ¨¡å—ï¼š\nimport csv\nwith open('file.csv', newline='') as f:\n  reader = csv.DictReader(f)\n  rows = list(reader)\nprint(rows)"}
 ]}
```

> å¤šè½®è®­ç»ƒé€šå¸¸**åªå¯¹æœ€åä¸€ä¸ª assistant æ®µè®¡æŸå¤±**ï¼ˆæˆ–å¯¹æ¯ä¸ªå›åˆçš„ assistant éƒ½è®¡æŸå¤±ï¼Œä½†è¦ç¡®ä¿æ¨¡æ¿èƒ½é€æ®µæ­£ç¡®æ‰“ maskï¼‰ã€‚Qwen3 çš„æ¨¡æ¿åœ¨â€œå¤šä¸ª assistant æ¶ˆæ¯â€æƒ…å†µä¸‹å‘ç”Ÿè¿‡å°é—®é¢˜çš„ä¿®å¤è®¨è®ºï¼Œä½ çš„æ•°æ®ä¸è¯„æµ‹ä¹Ÿåº”è¦†ç›–è¿™ç±»æ ·ä¾‹ä»¥é˜²æ¨¡æ¿è¾¹ç•Œé”™é…ã€‚[Hugging Face](https://huggingface.co/Qwen/Qwen3-1.7B/discussions/9)

#### 4ï¼‰JSON ç»“æ„åŒ–è¾“å‡ºï¼ˆå¼ºçº¦æŸæ ·å¼ï¼‰

```json
{"id":"json-0031",
 "messages":[
   {"role":"user","content":"ä»…è¾“å‡ºä¸€ä¸ªJSONå¯¹è±¡ï¼Œå« name(string), score(float)ã€‚name=Alice, score=93.5ã€‚ä¸å¾—è¾“å‡ºé¢å¤–æ–‡å­—ã€‚"},
   {"role":"assistant","content":"{\"name\":\"Alice\",\"score\":93.5}"}
 ],
 "tags":["json-only","strict"]}
```

- è¿™ç±»æ ·æœ¬ç”¨äºè®­ç»ƒ**ä¸¥æ ¼æ ¼å¼æ§åˆ¶**ï¼›éªŒè¯é›†ä¸Šç»Ÿè®¡ **JSON åˆæ³•ç‡**ã€‚
- å¦å¤–è¯¾é¢å—¯éœ€è¦æ·»åŠ è´Ÿä¾‹

#### 5ï¼‰å·¥å…·è°ƒç”¨ / å‡½æ•°è°ƒç”¨

Qwen3 çš„æ¨¡æ¿æ”¯æŒ**æ›´ç¨³å®šçš„å·¥å…·å‚æ•°åºåˆ—åŒ–**ã€‚æ„é€ æ•°æ®æ—¶ï¼ŒæŠŠâ€œæ¨¡å‹å†³å®šè°ƒç”¨çš„å‡½æ•°â€å†™åœ¨ `assistant` turn çš„ `tool_calls` å­—æ®µï¼ŒæŠŠå·¥å…·è¿”å›å†™æˆä¸€ä¸ªæˆ–å¤šä¸ª `tool` turnã€‚HF çš„ Qwen-3 æ¨¡æ¿æ·±åº¦è§£è¯»ä¸ PR/issue é‡Œéƒ½å±•ç¤ºäº†æ€è·¯ã€‚[Hugging Face](https://huggingface.co/blog/qwen-3-chat-template-deep-dive)

**ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰**ï¼š

```json
//tool schedma
{
  "id": "tool-0201",
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get the weather by city and day.",
        "parameters": {
          "type": "object",
          "properties": {
            "city": {"type": "string"},
            "day":  {"type": "string", "enum": ["today","tomorrow","yesterday"]}
          },
          "required": ["city","day"], //possibly "loc"
          "additionalProperties": false
        }
      }
    }
  ],
  "messages": [
    {"role": "system",  "content": "You can call tools to get weather."},
    {"role": "user",    "content": "æ˜å¤© Edmonton å¤©æ°”ï¼Ÿ"},

    {"role": "assistant", "content": "", "tool_calls": [
      {
        "type": "function",
        "id": "call_1",
        "function": {"name": "get_weather", "arguments": {"city": "Edmonton", "day": "tomorrow"}}
      }
    ]},

    {"role": "tool", "tool_call_id": "call_1", "content": "{\"temp_c\":12,\"condition\":\"Cloudy\"}"},

    {"role": "assistant", "content": "Edmonton æ˜å¤©å¤šäº‘ï¼Œçº¦ 12Â°Cã€‚"}
  ]
}

```

- **è®¡æŸå¤±**ï¼šä¸€èˆ¬åªå¯¹**æœ€åä¸€æ¡è‡ªç„¶è¯­è¨€å›å¤**çš„ `assistant` æ®µè®¡æŸå¤±ï¼›å·¥å…·è°ƒç”¨â€œå‡½æ•°ç­¾å/å‚æ•°â€çš„ç”Ÿæˆå¯é€‰æ˜¯å¦è®¡æŸå¤±ï¼ˆåˆå­¦è€…å»ºè®®**ä¸è®¡**ï¼Œå…ˆæŠŠè‡ªç„¶è¯­è¨€å›åº”å­¦ç¨³ï¼‰ã€‚
- **ä¸€è‡´æ€§**ï¼šå·¥å…·æ ·æœ¬è¦æˆå¯¹å‡ºç°ï¼ˆassistant è§¦å‘ â†’ tool å“åº” â†’ assistant å½’çº³ï¼‰ã€‚

#### 6ï¼‰â€œæ€è€ƒ/éæ€è€ƒâ€æ¨¡å¼ï¼ˆQwen3 çš„æ–°ç‰¹æ€§ï¼‰

Qwen3 åœ¨ `apply_chat_template` æ—¶æ”¯æŒ `enable_thinking=True/False`ï¼ˆæ˜¯å¦æ’å…¥â€œæ€è€ƒ token/æ®µâ€ï¼‰ï¼›æ¨¡å‹å¡ä¸è®¨è®ºæ˜¾ç¤ºäº†è¿™ä¸ªå¼€å…³çš„ç”¨æ³•ã€‚**æ•°æ®é›†å±‚é¢**ä½ å¯ä»¥å‡†å¤‡ä¸¤ç±»æ ·æœ¬ï¼š

- **éæ€è€ƒ**ï¼šæ™®é€šæŒ‡ä»¤å¯¹è¯ï¼ˆé»˜è®¤å°±å¥½ï¼‰ï¼›
- **æ€è€ƒæ ·æœ¬**ï¼šæŠŠâ€œæ¨ç†è‰ç¨¿/é“¾è·¯â€æ”¾è¿› assistant æ–‡æœ¬çš„â€œæ€è€ƒåŒºåŸŸâ€ï¼Œå†ç»™å‡ºå¯è§ç­”æ¡ˆï¼ˆæ›´åƒ CoTï¼‰ã€‚ä½¿ç”¨`reasoning_content` ç¼–è¾‘æ€è€ƒå’Œæ€ç»´é“¾çš„å†…å®¹
   è®­ç»ƒæˆ–æ¨ç†æ—¶åˆ‡æ¢ `enable_thinking`ã€‚[Hugging Face](https://huggingface.co/Qwen/Qwen3-0.6B)

**æ€è€ƒæ ·æœ¬ï¼ˆç®€åŒ–ï¼‰**

```json
{
  "messages": [
    {"role": "user", "content": "20 ä¸ªè‹¹æœï¼Œé€å‡º 7 ä¸ªï¼Œè¿˜å‰©å¤šå°‘ï¼Ÿ"},
    {
      "role": "assistant",
      "content": "13",
      "reasoning_content": "å…±æœ‰ 20 ä¸ªè‹¹æœï¼Œé€èµ° 7 ä¸ªï¼Œå‰©ä¸‹ 20 - 7 = 13ã€‚"
    }
  ]
}
```

> è®­ç»ƒæ—¶æ˜¯å¦çœŸçš„éœ€è¦â€œæ€è€ƒæ®µâ€ç”±ä½ å†³å®šâ€”â€”Qwen3 çš„æ¨¡æ¿**ä¸å¼ºåˆ¶**æ€è€ƒã€‚[Hugging Face](https://huggingface.co/blog/qwen-3-chat-template-deep-dive)

### 2.2 Qwen3 çš„ç‰¹æ®Šæ§åˆ¶ç¬¦ï¼ˆspecial tokensï¼‰ä»¥åŠtokenç”Ÿæˆ

#### 1ï¼‰ Qwen3 çš„ç‰¹æ®Šå­—ç¬¦ / special tokens

åœ¨ HuggingFace ä¸ŠåŠ è½½ **Qwen3 tokenizer** åï¼Œå¯ä»¥ç”¨ä¸‹é¢æ–¹å¼æŸ¥çœ‹å®ƒå®šä¹‰çš„ç‰¹æ®Šç¬¦å·ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tok = AutoTokenizer.from_pretrained(
    "Qwen/Qwen3-8B-Instruct",
    trust_remote_code=True
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-8B-Instruct",
    torch_dtype="bfloat16",   # æˆ–è€… "auto"
    device_map="auto",
    trust_remote_code=True
)

print(tok.special_tokens_map)       # æ˜¾ç¤º bos/eos/pad ç­‰ç‰¹æ®Šç¬¦å·
print(tok.additional_special_tokens)  # æ˜¾ç¤ºåƒ <|im_start|> <|im_end|> ä¹‹ç±»çš„æ§åˆ¶ç¬¦
```

å…¸å‹è¾“å‡ºä¼šåŒ…å«ï¼š

- `<|im_start|>`ã€`<|im_end|>`ï¼šæ ‡è®°ä¸€æ®µ message çš„å¼€å¤´å’Œç»“æŸ
- `<|system|>`ã€`<|user|>`ã€`<|assistant|>`ï¼šè§’è‰²æ ‡ç­¾ï¼ˆæœ‰äº›ç‰ˆæœ¬ç›´æ¥ä½œä¸º role å­—æ®µè¢«æ¸²æŸ“ï¼‰
- `<|endoftext|>`ï¼šç­‰ä»·äº `<eos>`
- å¯èƒ½è¿˜æœ‰ `<|extra_0|>` â€¦ `<|extra_99|>` é¢„ç•™ç¬¦å·

Qwen3 çš„ **chat æ¨¡æ¿** å°±æ˜¯ç”¨è¿™äº›ç‰¹æ®Šç¬¦å·æŠŠ `messages=[{role, content}]` æ¸²æŸ“æˆè¿ç»­çš„æ–‡æœ¬åºåˆ—ã€‚

#### 2ï¼‰ æ•°æ®é›†é‡Œè¦ä¸è¦ç›´æ¥å†™è¿™äº›ç¬¦å·ï¼Ÿ

**ä¸è¦æ‰‹å·¥å†™**ã€‚ä½ åœ¨æ„é€ æ•°æ®é›†æ—¶ï¼Œåº”è¯¥ä¿æŒå¹²å‡€çš„ JSON æ ¼å¼ï¼š

```json
{
  "messages": [
    {"role":"system","content":"You are a helpful assistant."},
    {"role":"user","content":"ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ"},
    {"role":"assistant","content":"ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ°”æ¸©25â„ƒå·¦å³ã€‚"}
  ]
}
```

ç„¶ååœ¨è®­ç»ƒæ—¶è°ƒç”¨ï¼š

```python
tok.apply_chat_template(
    example["messages"], 
    tokenize=False, 
    add_generation_prompt=False
)
```

Tokenizer ä¼š**è‡ªåŠ¨æ’å…¥** `<|im_start|>`ã€`<|im_end|>` è¿™äº›æ§åˆ¶ç¬¦ã€‚
 è¿™æ ·åšæœ‰ä¸¤ä¸ªå¥½å¤„ï¼š

1. **æ¨¡æ¿ç¨³å®š**ï¼šä¸åŒç‰ˆæœ¬çš„ Qwen å¯èƒ½æ›´æ–°æ¨¡æ¿ï¼Œå¦‚æœä½ æ‰‹å·¥å†™ç¬¦å·ï¼Œå°†æ¥ä¸å…¼å®¹ã€‚
2. **é¿å…é”™ä½**ï¼šchat æ¨¡æ¿ä¼šè‡ªåŠ¨ä¿è¯ assistant/user çš„è¾¹ç•Œæ­£ç¡®ï¼Œåˆ©äº loss maskï¼ˆåªç»™ assistant æ®µæ‰“æ ‡ç­¾ï¼‰ã€‚

#### 3ï¼‰assistant-only loss mask å’Œç‰¹æ®Šç¬¦å·çš„å…³ç³»

å½“ `apply_chat_template` ç”Ÿæˆåºåˆ—åï¼Œé‡Œé¢ä¼šåŒ…å« `<|im_start|>assistant ... <|im_end|>`ã€‚

- collatorï¼ˆæ¯”å¦‚ `DataCollatorForCompletionOnlyLM`ï¼‰ä¼šç”¨è¿™äº›é”šç‚¹æ¥ç¡®å®šä»å“ªé‡Œå¼€å§‹æ‰“ `labels`ã€‚
- `<|im_start|>` æœ¬èº«é€šå¸¸ **ä¸è®¡æŸå¤±**ï¼ˆå®ƒå±äºæ¨¡æ¿ tokenï¼Œä¸æ˜¯ç­”æ¡ˆï¼‰ã€‚
- **åªå¯¹ assistant æ®µæ­£æ–‡**æ‰“æŸå¤±ã€‚

#### 4ï¼‰ å¦‚æœä½ æƒ³è‡ªå·±ç¡®è®¤æ¨¡æ¿

å¯ä»¥ç›´æ¥æ‰“å°ä¸€æ¡æ ·æœ¬çœ‹çœ‹ï¼š

```python
sample = [
    {"role":"user","content":"è¯·ç»™æˆ‘ä¸€ä¸ªPythonæ‰“å°Helloçš„ä¾‹å­"},
    {"role":"assistant","content":"print('Hello')"}
]
print(tok.apply_chat_template(sample, tokenize=False))
```

è¾“å‡ºå¤§æ¦‚ç±»ä¼¼ï¼š

```shell
<|im_start|>user
è¯·ç»™æˆ‘ä¸€ä¸ªPythonæ‰“å°Helloçš„ä¾‹å­<|im_end|>
<|im_start|>assistant
print('Hello')<|im_end|>
# æ¨¡å‹åªä¼šè®¡ç®—assistantä¹‹åçš„CE
```

> æ‰€ä»¥ `<|im_start|>` `<|im_end|>` è¿™ç±»ç‰¹æ®Šç¬¦å·åªä¼šåœ¨ **æœ€ç»ˆè®­ç»ƒæ–‡æœ¬é‡Œ**å‡ºç°ï¼Œç”± tokenizer æ¨¡æ¿è‡ªåŠ¨æ’å…¥ï¼›ä½ è‡ªå·±å‡†å¤‡çš„æ•°æ®é›† JSON ä¸éœ€è¦ä¹Ÿä¸åº”è¯¥ç›´æ¥å†™å®ƒä»¬ã€‚

------

## 3. æ¨¡å‹ï¼Œæ•°æ®é›†ä¸ tokenizer

### 3.1 æ¨¡å‹

**æ¨è unsloth/Qwen3-4B-Instruct-2507**ï¼Œå…¶ä¸­`Instruct` æŒ‡çš„æ˜¯ **instruction-tunedï¼ˆæŒ‡ä»¤å¾®è°ƒç‰ˆï¼‰**ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåœ¨ Base åŸºç¡€ä¸Šï¼Œç”¨äººå·¥æˆ–åˆæˆçš„ **æŒ‡ä»¤â€“å›ç­”æ•°æ®é›†**ï¼ˆSFT, Supervised Fine-Tuningï¼‰è®­ç»ƒè¿‡ï¼Œæ‰€ä»¥æˆ‘ä»¬å†åšå¾®è°ƒçš„æ—¶å€™ï¼Œä¸è¦ä»0å¼€å§‹æ•™æ¨¡å‹æ€ä¹ˆå›ç­”ã€‚è¿™æ˜¯ä¸€ä¸ªdenseæ¨¡å‹ï¼Œæ²¡æœ‰reasoningï¼ˆthinkingï¼‰èƒ½åŠ›ï¼Œä½†æ˜¯æ ¹æ®å®˜ç½‘

> **Significant improvements** in general capabilities, including **instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage**.

è¯¥4bæ¨¡å‹åœ¨å¤šé¡¹é¢†åŸŸå‡æœ‰æå‡ï¼Œä¸”åœ¨å¤šé¡¹ä»»åŠ¡ä¸­å‡»è´¥äº†gpt 4.1 nanoå’ŒQwen3-30B-A3B Non-Thinkingï¼Œè®­ç»ƒéœ€è¦çš„èµ„æºå¾ˆå°‘ï¼Œéå¸¸é€‚åˆåˆå­¦è€…ä¸Šæ‰‹æµ‹è¯•å’Œå¾®è°ƒã€‚

- **Qwen ç³»åˆ—**å¸¸è§å“åº”æ¨¡æ¿é”šç‚¹ï¼ˆç”¨ä½œ mask çš„ response æ ‡è®°ï¼‰è¿‘ä¼¼ä¸ºï¼š`<|im_start|>assistant` åçš„æ­£æ–‡ï¼›

> æˆ‘ä»¬ä¼šç”¨ **â€œæ¨¡æ¿é”šç‚¹å­—ç¬¦ä¸²â€ + æ•°æ®æ•´ç†** æ¥å®ç° **assistant-only loss mask**ï¼šå¯¹ user/system æ®µç½® `labels=-100`ï¼Œåªå¯¹ assistant æ®µè®¡æŸå¤±ã€‚è¿™æ­£æ˜¯ Day 2 ç†è®ºè¦ç‚¹ã€‚

å¯ä»¥ä½¿ç”¨`transformers`(huggingface)ä¸‹è½½

```python
"""
you can clean your space first using:
conda clean --all
pip cache purge
"""
from transformers import AutoModelForCausalLM, AutoTokenizer

TEST = False

model_name = "unsloth/Qwen3-4B-Instruct-2507"
save_dir = "path/to/models"

# load the tokenizer and the model
tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=save_dir)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
    cache_dir=save_dir
)
```

æ¥ä¸‹æ¥å¯ä»¥ä½¿ç”¨vllmè¿›è¡Œéƒ¨ç½²ã€‚vLLM æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨æ¥ **é«˜æ•ˆæ¨ç†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨ç†å¼•æ“**ã€‚å®ƒä¸æ˜¯è®­ç»ƒæ¡†æ¶ï¼Œè€Œæ˜¯é’ˆå¯¹â€œéƒ¨ç½² / æœåŠ¡ç«¯æ¨ç†â€ä¼˜åŒ–çš„ï¼Œè§£å†³çš„æ˜¯ Hugging Face `transformers` åœ¨é•¿ä¸Šä¸‹æ–‡ã€å¤šå¹¶å‘åœºæ™¯ä¸‹é€Ÿåº¦æ…¢ã€æ˜¾å­˜å ç”¨é«˜çš„é—®é¢˜ã€‚

```shell
# åœ¨4080å¡ä¸‹ï¼Œä»¥ä¸‹è®¾ç½®ä¼šå ç”¨ï¼š
#|=========================================+======================+======================|
#|   0  NVIDIA GeForce RTX 4080        Off | 00000000:01:00.0  On |                  N/A |
#|  0%   39C    P8              14W / 320W |  14439MiB / 16376MiB |      1%      Default |
#|                                         |                      |                  N/A |
#+-----------------------------------------+----------------------+----------------------+
# æ³¨æ„ï¼Œè¿™é‡Œåªæ˜¯æœ€å¤§å ç”¨ï¼Œå®é™…è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ä¸éœ€è¦è¿™ä¹ˆå¤§
# note this path is to the directoty that can directly see config.json
vllm serve /path/to/models/snapshot \ 
  --max-model-len 16384 \
  --gpu-memory-utilization 0.82 \
  --max-num-seqs 4 \
  --max-num-batched-tokens 2048 \
  --enforce-eager
# è‹¥ç‰ˆæœ¬/ç¡¬ä»¶æ”¯æŒï¼Œå†åŠ ä¸Šè¿™ä¸€æ¡ï¼ˆKV å†…å­˜çº¦å‡åŠï¼‰ï¼š
#  --kv-cache-dtype fp8

```

ä¸‹è½½éƒ¨ç½²å®Œæˆåï¼Œå¯æŒ‰ç…§deploy.pyä¸­çš„vLLM testæµç¨‹è¿›è¡Œopenai apiçš„æµ‹è¯•ï¼Œä»¥äº†è§£æ¨¡å‹æƒ…å†µã€‚åŒæ—¶å¯ä»¥æ—¶æ‰€ç”¨[evalscope](https://github.com/modelscope/evalscope)  æ¥è¿›è¡Œå‹åŠ›å’Œæ€§èƒ½æµ‹è¯•ï¼Œè¯¦è§evalã€‚

### 3.2 æ•°æ®é›†

#### 1ï¼‰æ•°æ®é›†ç±»å‹

ä¸ºäº†ä½¿å¾—æ¨¡å‹å¾®è°ƒåï¼Œä¾ç„¶å…·æœ‰æ··åˆæ¨ç†èƒ½åŠ›ï¼Œéœ€è¦è€ƒè™‘åœ¨æ•°æ®é›†ä¸­åŠ å…¥æ™®é€šå¯¹è¯å’Œå¸¦æœ‰æ¨ç†å­—æ®µçš„æ•°æ®é›†ï¼ˆå¯ä»¥æ‹¼æ¥ï¼‰ã€‚ç”±äº4b-instructä¸æ˜¯æ¨ç†æ¨¡å‹ï¼Œæ‰€ä»¥ä¸éœ€è¦åŠ å…¥æ¨ç†æ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨[shareAI/DPO-zh-en-emoji](https://hf-mirror.com/datasets/shareAI/DPO-zh-en-emoji)æ•°æ®é›†ï¼Œç”¨æ¥åŠ å¦‚æ›´å¤šçš„emojiï¼Œå†æ·»åŠ ä¸¤ä¸ªæ•°å­¦æ•°æ®é›†åˆcmath/MAWPSã€‚ä»¥ä¸‹æ˜¯å…¶ä»–é€‰é¡¹ã€‚

| åç§°         | è¯­è¨€ / ç‰¹æ€§                       | æ˜¯å¦å¸¦æ€è€ƒé“¾ / æ¨ç†æ ‡æ³¨                                      | é€‚åˆç”¨ä½œå¯¹è¯å¾®è°ƒå—                                    | å¤‡æ³¨ / å¯å–éƒ¨åˆ†                                              |
| ------------ | --------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------- | ------------------------------------------------------------ |
| **CORECODE** | ä¸­æ–‡ï¼Œå…±è¯†æ¨ç† + å¯¹è¯             | æœ‰å…±è¯†çŸ¥è¯†æ³¨é‡Šï¼ˆå¯¹è¯é‡ŒåŒ…å«æ¨ç†åˆ¤æ–­ï¼‰([arXiv](https://arxiv.org/abs/2312.12853)) | æ˜¯ï¼Œå¯ä»¥æŠŠå…¶å¯¹è¯ + æ¨ç†åˆ¤æ–­æŠ½å‡ºæ¥åš mixed æ¨¡å‹è®­ç»ƒã€‚  | è¿™ä¸ªæ•°æ®é›†åâ€œå¯¹è¯ä¸­æ¶‰åŠå¸¸è¯†åˆ¤æ–­â€ rather than æ˜æ˜¾çš„è‡ªæˆ‘æ€è€ƒé“¾ï¼Œä½†è¶³ä»¥ä½œä¸ºæ¨ç†èƒ½åŠ›çš„ä¸€éƒ¨åˆ†è®­ç»ƒç´ æã€‚ |
| **MuTual**   | ä¸­æ–‡ / è‹±æ–‡æ··åˆï¼Œå¤šè½®å¯¹è¯æ¨ç†é€‰é¢˜ | å®ƒæ˜¯å¤šè½®å¯¹è¯æ¨ç†é€‰æ‹©é¢˜ï¼Œè¦æ±‚æ¨¡å‹åˆ¤æ–­å“ªä¸€ä¸ªé€‰é¡¹æœ€åˆç† ([GitHub](https://github.com/Nealcly/MuTual)) | è¾ƒé€‚åˆåš**ç†è§£ / åˆ¤æ–­æ¨¡å—**è®­ç»ƒï¼Œè€Œéç›´æ¥å¯¹è¯å›å¤æ¨¡å‹ | å¯ä»¥æŠŠé—®é¢˜ + é€‰é¡¹è½¬æˆå¯¹è¯ prompt æ ¼å¼ï¼Œæ¨¡å‹å›å¤â€œæ­£ç¡®é€‰é¡¹ + æ¨ç†é“¾â€ã€‚ |
| **KdConv**   | ä¸­æ–‡ï¼Œå¤šé¢†åŸŸçŸ¥è¯†é©±åŠ¨å¯¹è¯          | ä¸»è¦æ˜¯çŸ¥è¯†å¯¹è¯ï¼Œä¸ä¸€å®šå¸¦æ˜¾å¼æ€è€ƒé“¾                           | æ˜¯ï¼Œå¯¹è¯èƒ½åŠ›æå‡ + çŸ¥è¯†äº’åŠ¨å¢å¼º                       | å¯ä½œä¸ºå¯¹è¯åŸºç¡€è¯­æ–™ï¼ˆä¸å¼ºå–æ¨ç†é“¾ï¼‰([GitHub](https://github.com/YouTaoBaBa/Chinese-Dialogue-Dataset)) |
| **LCCC**     | ä¸­æ–‡å¯¹è¯ / ç¤¾äº¤é—²èŠ               | åŸºæœ¬å¯¹è¯ï¼Œæ²¡æœ‰æ€è€ƒé“¾æ ‡æ³¨                                     | æ˜¯ï¼Œå¯¹è¯èƒ½åŠ›è®­ç»ƒåŸºçŸ³                                  | å¯ä½œä¸ºâ€œé€šç”¨å¯¹è¯â€æ•°æ®çš„è¦†ç›–å±‚ï¼Œä¿è¯æ¨¡å‹åœ¨å¯¹è¯æµç•…æ€§æ–¹é¢ä¸è¿‡æ‹Ÿåˆæ¨ç†è¡Œä¸º([ResearchGate](https://www.researchgate.net/publication/346107804_A_Large-Scale_Chinese_Short-Text_Conversation_Dataset)) |

#### 2ï¼‰æ•°æ®é›†å¤„ç†ä¸åˆå¹¶

ä¸ºäº†æ›´åŠ åå‘å·¥ä¸šç•Œå®é™…çš„æ“ä½œæ–¹æ³•, æˆ‘ä»¬è¿™æ ·æ„å»ºå¤šä¸ªæ•°æ®é›†ï¼š

- **å…ˆâ€œè§„èŒƒåŒ–â†’åˆ†åº“å­˜ç›˜â€**ï¼šæŠŠæ¯ä¸ªåŸå§‹æ•°æ®é›†æ¸…æ´—æˆç»Ÿä¸€ schemaï¼ˆ`messages`, `source`, `tags`â€¦ï¼‰åï¼Œåˆ†åˆ« `save_to_disk`ã€‚
   è¿™æ ·ï¼š
  - å¤ç”¨ä¾¿æ·ï¼šåç»­å¯ä»¥ç”¨ä¸åŒæƒé‡ã€å¤šå¥—é…æ¯”**å¿«é€Ÿé‡æ··**ï¼Œä¸ç”¨åå¤è§£æ JSON/CSVã€‚
  - è®­ç»ƒé«˜æ•ˆï¼š`load_from_disk` ç›´æ¥è¯» Arrowï¼Œæ€§èƒ½æ›´å¥½ï¼Œä¹Ÿæ˜“äºå¤šäººåä½œä¸ç‰ˆæœ¬ç®¡ç†ã€‚
- **æŒ‰éœ€â€œæ„å»ºæ··åˆåŒ…â€**ï¼šé’ˆå¯¹æŸæ¬¡è®­ç»ƒï¼ŒæŠŠåˆ†åº“å­˜ç›˜çš„æ•°æ®ç”¨**æƒé‡**æ··èµ·æ¥ï¼Œå† `save_to_disk`ï¼ˆæˆ–å¯¼å‡º JSONL ç»™å¤–éƒ¨æ¡†æ¶ï¼‰ã€‚
  - è‹¥åªæƒ³â€œæŒ‰æ¯”ä¾‹å¤§è‡´æŠ½æ ·â€ï¼Œç”¨ **interleave**ï¼ˆç®€å•å¿«ï¼‰ã€‚
  - è‹¥è¦â€œä¸¥æ ¼å‘½ä¸­é…æ¯”â€ï¼Œç”¨ **resample**ï¼ˆå¸¦æ”¾å›é‡‡æ ·ï¼Œç²¾å‡†æ»¡è¶³æƒé‡ï¼‰ã€‚

ä¸€æ¬¡æ€§æŠŠæ‰€æœ‰æ•°æ®â€œåˆå¹¶æˆä¸€ä¸ªæ•°æ®é›†â€å›ºç„¶ä¹Ÿèƒ½è®­ï¼Œä½†ä¸åˆ©äºåç»­å¿«é€Ÿæ¢æƒé‡å’Œå¤ç°å®éªŒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¸€æ­¥æ­¥å®è·µå¦‚ä½•å¯¹æ•°æ®ä½œå¤„ç†ã€‚

**1) è§„èŒƒåŒ–ï¼ˆnormalize_*ï¼Œéå½’ä¸€åŒ–ï¼‰**

æŠŠä¸‰ä¸ªåŸå§‹æ•°æ®é›†è½¬æˆç»Ÿä¸€ schemaï¼šæ¯æ¡æ ·æœ¬éƒ½æ˜¯

```json
{"messages":[{"role":"user","content":"â€¦"},{"role":"assistant","content":"â€¦"}]}
```

åŒæ—¶æ¸…æ´—æ–‡æœ¬ï¼ˆå»æ§åˆ¶ç¬¦ã€æˆªæ–­è¶…é•¿ç­”æ¡ˆç­‰ï¼‰ã€‚è¿™æ ·å¯ä»¥

- ç»Ÿä¸€æˆå¯¹è¯å¼ `messages`ï¼Œç›´æ¥å–‚åˆ°ç»å¤§å¤šæ•° SFT/æŒ‡ä»¤å¾®è°ƒ Trainerã€‚
- é¿å…ä¸åŒæºçš„å­—æ®µåä¸ä¸€è‡´ï¼ˆå¦‚ `question` vs `Question`ï¼‰ã€‚
- ä¾¿äºåç»­å¯è§†åŒ–ã€å¯¹é½ tokenizerã€ç»Ÿè®¡é•¿åº¦ç­‰ã€‚

**ä¾‹å­ï¼š**

- CMATHï¼š`{"question":"èŠ³èŠ³æœ‰99é¡µâ€¦","golden":"9"}` â†’ `messages=[{user:"â€¦?"},{assistant:"9"}]`ã€‚
- MAWPSï¼šæŠŠ `Question` é‡Œçš„ `N_0, N_1` ç”¨ `Numbers` æ›¿æ¢æˆå…·ä½“æ•°å€¼ï¼Œå†ç”Ÿæˆ `messages`ã€‚
- Emojiï¼šä¼˜å…ˆç”¨ `answer_zh`ï¼Œæ²¡æœ‰å°±å›é€€ `answer_en`ï¼›è¿‡é•¿ç­”æ¡ˆæˆªæ–­åˆ° 3000 å­—ç¬¦ï¼Œé˜²æ­¢æç«¯æ ·æœ¬æ‹–å® batchã€‚

**2) åŠ æ ‡ç­¾ï¼ˆadd_metadataï¼‰**

ä¸ºæ¯æ¡æ ·æœ¬å¢åŠ ï¼š

```python
source="cmath"/"emoji"/"mawps"
tags=["math","zh"] ç­‰
```

- è®­ç»ƒ/è¯„æµ‹åˆ†æ¡¶ï¼šçœ‹â€œmath/zh/en/chatâ€ç­‰å­åŸŸæ•ˆæœã€‚
- åç»­é‡æ··æ—¶æŒ‰æ¥æºè®¾æƒé‡æ›´æ¸…æ™°ã€‚
- æ’éšœä¸å¤ç°ï¼ˆçœ‹åˆ°ä¸€æ¡å¼‚å¸¸æ ·æœ¬ï¼ŒçŸ¥é“å®ƒæ¥è‡ªå“ªä¸€æ± ï¼‰ã€‚

**ä¾‹å­ï¼š**

- CMATH â†’ `source="cmath", tags=["math","zh"]`
- MAWPS â†’ `source="mawps", tags=["math","en"]`

**3) å»é‡ï¼ˆdeduplicationï¼‰**

å¯¹ `messages` åš MD5ï¼ŒæŒ‰å“ˆå¸Œå»é‡ï¼ˆå…ˆå„è‡ªå»é‡ï¼Œå†åˆå¹¶åå†å»é‡ä¸€æ¬¡ï¼‰ã€‚

- é˜²æ­¢æŸäº›æ•°æ®é›†å†…éƒ¨æˆ–è·¨æ•°æ®é›†é‡å¤ï¼ˆå°¤å…¶æ˜¯çˆ¬å–/å¤åˆ»çš„å…¬å¼€æ•°æ®ï¼‰ã€‚
- é‡å¤æ ·æœ¬ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€æŒ‡æ ‡è†¨èƒ€ã€‚

**ä¾‹å­ï¼š**

- å¦‚æœ CMATH å’Œ MAWPS æ°å¥½éƒ½æœ‰ä¸€é“â€œ99é¡µçœ‹äº†90é¡µâ€çš„é¢˜ï¼Œå“ˆå¸Œä¸€è‡´ â†’ ç•™ä¸€æ¡ã€‚

**4) åˆ†åº“å­˜ç›˜ï¼ˆå¯é€‰ï¼Œseparate_saveï¼‰**

æŠŠè§„èŒƒåŒ–+å»é‡åçš„æ¯ä¸ªå­é›† `save_to_disk` ä¸º Apache Arrowã€‚è¿™ç§æ ¼å¼ï¼š

- å®ƒæŠŠæ•°æ®å­˜æˆä¸€ç§ **åˆ—å¼è¡¨æ ¼**ï¼ˆç±»ä¼¼ Pandas DataFrameï¼‰ï¼Œè€Œä¸æ˜¯ä¸€è¡Œè¡Œçš„ JSON/CSVã€‚
- è¿™ç§æ ¼å¼åœ¨å†…å­˜é‡Œå¾ˆç´§å‡‘ï¼Œé€‚åˆ**é«˜æ•ˆè¯»å–å’Œéšæœºè®¿é—®**ï¼Œå°¤å…¶æ˜¯å¤§è§„æ¨¡æ•°æ®ã€‚
- HuggingFace çš„ `datasets` åº“åº•å±‚å°±æ˜¯ç”¨ Arrow å­˜æ•°æ®ï¼Œæ‰€ä»¥ `save_to_disk` è¾“å‡ºçš„å…¶å®å°±æ˜¯ Arrow æ ¼å¼çš„ç›®å½•ã€‚

**ä¸ºä»€ä¹ˆï¼š**

- å·¥ä¸šç•Œå¸¸è§åšæ³•ï¼š**è§„èŒƒåŒ–æ•°æ®é›†ä½œä¸ºâ€œæ± â€é•¿æœŸä¿å­˜**ã€‚
- åç»­ä»»æ„è®­ç»ƒéƒ½ä»è¿™äº›æ± æŒ‰æƒé‡é‡æ··ï¼Œæ— éœ€åå¤æ¸…æ´—åŸå§‹ JSON/CSVã€‚
- Arrow è¯»å†™å¿«ã€å†…å­˜å‹å¥½ã€å›¢é˜Ÿå¯å¤ç”¨ã€‚

**ä¾‹å­ï¼š**

- è¾“å‡ºï¼š`datasets/normalized/cmath/`, `â€¦/emoji/`, `â€¦/mawps/`
- ä¸‹æ¬¡æƒ³æé«˜è‹±æ–‡æ•°å­¦æƒé‡ï¼Œåªéœ€ `load_from_disk` å›æ¥é‡æ··å³å¯ã€‚

5ï¼‰**æ··åˆï¼ˆinterleave / resampleï¼‰**

æœ‰ä¸¤æ¡è·¯ï¼š

**5.1 äº¤é”™æ··åˆ `interleave_mix`**

- å…ˆæŠŠå„æ±  `shuffle(seed)`ã€‚
- æŒ‰ **æ¦‚ç‡**ï¼ˆç”±æƒé‡å½’ä¸€åŒ–è€Œæ¥ï¼‰äº¤æ›¿æŠ½æ ·ï¼Œç›´åˆ°â€œå…¨éƒ¨è€—å°½â€ã€‚

**ä¸ºä»€ä¹ˆï¼š**

- å¿«é€Ÿã€ç¨³å®šï¼Œä¸ä¼šå¯¹å°æ± åšå¤§é‡æ”¾å›ä¸Šé‡‡æ ·ã€‚
- è®­ç»ƒæ—¶æ•´ä½“é…æ¯”â€œè¿‘ä¼¼å‘½ä¸­â€ï¼Œé€‚åˆæ—¥å¸¸è¿­ä»£ã€‚

**ä¾‹å­ï¼š**

- æƒé‡ `cmath:0.4, emoji:0.3, mawps:0.3` â†’ è¿‘ä¼¼ 4:3:3 çš„äº¤é”™é¡ºåºï¼Œå¦‚
   `cmath, emoji, cmath, mawps, cmath, emoji, â€¦`
- å¦‚æœæŸä¸ªæ± å¾ˆå°ï¼Œè€—å°½åå°±ä¸å†æŠ½å®ƒï¼Œå‰©ä¸‹çš„æŒ‰ç›¸å¯¹æ¦‚ç‡ç»§ç»­ã€‚

**5.2 ä¸¥æ ¼é…æ¯” `resample_mix`**

- è®¾å®š `total_size`ï¼ˆä¸è®¾åˆ™é»˜è®¤æŒ‰æ€»é‡çš„ 0.9ï¼‰ã€‚
- æŒ‰æƒé‡è®¡ç®—ç›®æ ‡æ ·æœ¬æ•°ï¼Œå¯¹å„æ± **æ”¾å›é‡‡æ ·**åˆ°ç›®æ ‡æ•°ï¼Œæœ€å `concatenate`ã€‚

**ä¸ºä»€ä¹ˆï¼š**

- ç²¾å‡†å‘½ä¸­é…æ¯”ï¼Œä¾¿äºå¯å¤ç°å®éªŒä¸ A/Bã€‚
- ä»£ä»·ï¼šå°æ± ä¼šè¢«æ”¾å¤§ï¼Œæœ‰è¿‡æ‹Ÿåˆé£é™©ã€‚

**ä¾‹å­ï¼š**

- æ€»é‡ 30kï¼›æƒé‡ 0.4/0.3/0.3 â†’ é‡‡æ · 12k/9k/9k æ¡ï¼›
- è‹¥ `emoji` åªæœ‰ 2k åŸæ ·æœ¬ï¼Œä¹Ÿä¼šâ€œæ”¾å›â€æŠ½åˆ° 9kï¼Œéœ€æ³¨æ„è¿‡æ‹Ÿåˆï¼Œé€šå¸¸é…åˆå¼ºæ­£åˆ™/æ›´é«˜æ¸©åº¦/å¤šè½® shuffleã€‚

**6) ç»ˆè½®å»é‡ï¼ˆmerged åå†æ¬¡ dedupï¼‰**

**åšäº†ä»€ä¹ˆï¼š**
 æ··åˆå®Œæˆåï¼Œå†è·‘ä¸€éå“ˆå¸Œå»é‡ã€‚

**ä¸ºä»€ä¹ˆï¼š**

- é˜²æ­¢ç”±äºä¸åŒæ± é‡Œæ®‹ç•™çš„é‡å¤åœ¨åˆå¹¶åæš´éœ²ã€‚
- ä¹Ÿé¿å…æŸäº›è¾¹ç•Œæƒ…å†µä¸‹ interleave/resample äº§ç”Ÿæ„å¤–é‡å¤ã€‚

**ä¾‹å­ï¼š**

- ä¸¤ä¸ªæ± å„ç•™äº†ä¸€ä»½åŒé¢˜æ ·æœ¬ï¼Œåˆå¹¶ååˆ æ‰ä¸€ä»½ï¼Œç¡®ä¿è®­ç»ƒé›†å”¯ä¸€æ€§ã€‚

**7) åˆ‡åˆ†ï¼ˆsplit_train_validï¼‰**

**åšäº†ä»€ä¹ˆï¼š**
 å¯¹åˆå¹¶æ•°æ® `shuffle(seed)`ï¼Œç„¶åæŒ‰æ¯”ä¾‹åˆ‡å‡ºéªŒè¯é›†ï¼ˆä¸‹é™ 100 æ¡ï¼Œé¿å… val è¿‡å°ï¼‰ã€‚

- ç»Ÿä¸€çš„éªŒè¯é›†ç”¨äºæ—©åœ/å¯¹æ¯”ã€‚
- ä¸‹é™ä¿æŠ¤ï¼šå³ä½¿æ€»é‡ä¸å¤§ä¹Ÿèƒ½å¾—åˆ°æœ‰æ„ä¹‰çš„éªŒè¯ç»Ÿè®¡ã€‚

**ä¾‹å­ï¼š**

- æ€» 50kï¼Œ`valid_ratio=0.02` â†’ 1k éªŒè¯ã€49k è®­ç»ƒã€‚
- è‹¥æ€»é‡ 3kï¼Œä»ä¼šç»™åˆ°è‡³å°‘ 100 æ¡éªŒè¯ã€‚

**8) å½’ä¸€åŒ–æƒé‡ï¼ˆ_normalize_weightsï¼‰**

 æŠŠä¼ å…¥çš„æƒé‡å­—å…¸æ­£åˆ™åŒ–åˆ°å’Œä¸º 1ï¼›è‹¥æƒé‡å…¨ 0/è´Ÿï¼Œé€€åŒ–ä¸ºå‡åˆ†ã€‚

**ä¸ºä»€ä¹ˆï¼š**

- ç»Ÿä¸€è§£é‡Šâ€œæƒé‡è¡¨ç¤ºç›¸å¯¹æ¯”ä¾‹â€ã€‚
- é¿å…ä¼ å‚å¤±è¯¯å¯¼è‡´æ¦‚ç‡å¼‚å¸¸ã€‚

**ä¾‹å­ï¼š**

- `{cmath: 4, emoji: 3, mawps: 3}` ä¸ `{0.4, 0.3, 0.3}` ç­‰ä»·ã€‚
- `{cmath: 0, emoji: 0, mawps: 0}` â†’ è‡ªåŠ¨å˜ä¸ºå‡åˆ† 1/3ã€‚

**9) éšæœºæ€§ä¸å¤ç°ï¼ˆseedï¼‰**

- `shuffle(seed)`ã€æ”¾å›é‡‡æ ·æ—¶éƒ½ç”¨ç¡®å®šæ€§éšæœºæºã€‚

**ä¸ºä»€ä¹ˆï¼š**

- å¤ç°å®éªŒã€‚
- ä¸åŒ run æƒ³â€œå¤šæ ·åŒ–â€ä¹Ÿå¯ä»¥æ”¹ seedã€‚

**ä¾‹å­ï¼š**

- A/B ä¸¤ç»„åªæ”¹ `mix_weights`ï¼Œä¿æŒ seed ç›¸åŒï¼Œä¾¿äºå¯¹æ¯”ï¼›
- æƒ³ç”Ÿæˆâ€œç¬¬äºŒç‰ˆè®­ç»ƒåŒ…â€ï¼Œå°±æŠŠ seed +1ã€‚

**10) packing**

å¯¹äºç«¯çš„æ–‡æœ¬æ•°æ®ï¼Œä¼šå ç”¨å¤§é‡çš„æ˜¾å­˜ç©ºé—´ï¼Œå¯¼è‡´æ˜¾å­˜åˆ©ç”¨æ•ˆç‡ä½ä¸‹ã€‚è¿™æ—¶å€™æˆ‘ä»¬å¯ä»¥ä½¿ç”¨packingå°†çŸ­çš„æ•°æ®åˆå¹¶ï¼Œä»è€ŒèŠ‚çº¦ç©ºé—´ã€‚ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¯¹åº”çš„maskä¹Ÿè¦æ”¹å˜ï¼š
![packing](pics/packing.png)

è¯¥å›¾ç‰‡å–è‡ªäºä½™è€å¸ˆçš„github repo: [LLM-RL-Vis](https://github.com/changyeyu/LLM-RL-Visualized)ã€‚æˆ‘ä»¬æ¨èæ‚¨æµè§ˆè€å¸ˆçš„githubæˆ–è€…è´­ä¹°è¿™æœ¬ä¹¦ã€‚

**11) å¯¼å‡ºï¼ˆsave_to_disk / to_jsonï¼‰**

**åšäº†ä»€ä¹ˆï¼š**

- æŠŠæœ€ç»ˆ `train/validation` ç”¨ Arrow `save_to_disk`ï¼›
- ä¹Ÿå¯ `to_json(force_ascii=False)` å¯¼å‡º JSONLã€‚

**ä¸ºä»€ä¹ˆï¼š**

- Arrowï¼šåŠ è½½å¿«ã€å°‘å†…å­˜ã€HuggingFace ç”Ÿæ€å‹å¥½ï¼›
- JSONLï¼šå…¼å®¹é HF çš„ Trainer/Pipelineã€‚

**ä¾‹å­ï¼š**

- äº§ç‰©ç›®å½•ï¼š

  ```shell
  datasets/_qwen3_sft_mixed/train/    # Arrow
  datasets/_qwen3_sft_mixed/validation/
  datasets/_qwen3_sft_mixed/train.jsonl
  datasets/_qwen3_sft_mixed/val.jsonl
  ```

å½“ç„¶åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨pandaså¯¹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œç„¶åå†ä½¿ç”¨pd.concatæ‹¼æ¥æ•°æ®ï¼Œå†ç”¨datasetsåº“è¿›è¡ŒæŠ½æ ·ã€‚

å…·ä½“åšæ³•ï¼Œè¯·è§`build_datasets.py`ã€‚åŒæ—¶ä¹Ÿå¯ä»¥ä½¿ç”¨`check_dataset.py` æ¥éªŒè¯ç”Ÿæˆçš„æ•°æ®é›†æ˜¯å¦ç¬¦åˆqwen3è®­ç»ƒçš„è§„èŒƒã€‚å½“ç„¶ï¼ŒHFä¹Ÿæä¾›äº†å®˜æ–¹çš„æ•°æ®å¤„ç†æ–¹æ³•ï¼Œå¦‚æœä½ ä¸æƒ³è‡ªå·±å¤„ç†ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨**Hugging Face Datasets**ï¼š

-  `datasets` åŒ…æœ¬èº«å°±æ”¯æŒå¾ˆå¤šå†…ç½®æ“ä½œï¼š
  - `.filter()` â†’ æ¸…ç†å¼‚å¸¸æ ·æœ¬
  - `.map()` â†’ è§„èŒƒåŒ–å­—æ®µ
  - `.remove_columns()` / `.rename_column()`
  - `.drop_duplicates()`
- ç‰¹ç‚¹ï¼šå’Œ HF ç”Ÿæ€æ— ç¼ï¼Œæ•°æ®ä¼šå­˜æˆ Arrowï¼Œå¤„ç†é€Ÿåº¦å¾ˆå¿«ã€‚

ğŸ‘‰ ç”¨æ³•ä¾‹å­ï¼š

```python
from datasets import load_dataset

ds = load_dataset("json", data_files="data.jsonl")["train"]
# å»é‡
ds = ds.drop_duplicates("text")
# æ¸…ç†ç©ºæ ·æœ¬
ds = ds.filter(lambda x: len(x["text"].strip()) > 0)
```

------

## 4. å…³é”®è®­ç»ƒç­–ç•¥ï¼ˆå¯¹4080 æ˜¾å­˜å‹å¥½ï¼Œå…¶ä»–é…Œæƒ…è°ƒæ•´ï¼‰

- **é‡åŒ–**ï¼š`load_in_4bit=True, quant_type="nf4", compute_dtype=bfloat16`ï¼ˆQLoRAï¼‰ï¼Œå…¶ä¸­ï¼Œquant_type="nf4"æ˜¯ä¸€ç§ 4bit çš„åˆ†å¸ƒæ„ŸçŸ¥é‡åŒ–ï¼ˆNormalFloat4ï¼‰ï¼Œæ¯”æ™®é€š int4 ä¿çœŸåº¦æ›´å¥½ã€‚ä½†æ˜¯ï¼Œqwen3ä¸æ”¯æŒè¯¥å‚æ•°

- **LoRA**ï¼š`r=16, alpha=32, dropout=0.05`ï¼ˆå…ˆå°åå¤§ï¼‰ã€‚å…¶ä¸­ï¼Œ

  - r æ˜¯LoRA çŸ©é˜µçš„ç§©ï¼Œå†³å®šå¯è®­ç»ƒå‚æ•°é‡ã€‚è¾ƒå°ï¼ˆå¦‚ 4â€“8ï¼‰ï¼šæ˜¾å­˜æ›´çœï¼Œæ‹Ÿåˆèƒ½åŠ›æœ‰é™ã€‚
     è¾ƒå¤§ï¼ˆå¦‚ 32+ï¼‰ï¼šèƒ½å­¦å¾—æ›´å¥½ï¼Œä½†æ˜¾å­˜å’Œè®­ç»ƒæ—¶é—´å¢åŠ ã€‚
  - alphaç¼©æ”¾ç³»æ•°ï¼Œå’Œ r æ­é…ã€‚ä¸€èˆ¬è®¾æˆ `2*r`ã€‚å€¼å¤ªä½ï¼šè®­ç»ƒä¸è¶³ï¼›å€¼å¤ªé«˜ï¼šå¯èƒ½è¿‡æ‹Ÿåˆã€‚

- **åºåˆ—**ï¼š`max_seq_len=1024`ï¼ˆæˆ– 2048ï¼Œå…ˆä» 1k èµ·æ­¥ï¼‰æ¯æ¡æ ·æœ¬æœ€å¤š 1024 tokenã€‚æ˜¾å­˜çº¿æ€§å¢é•¿ã€‚ä¸è¿‡qwen3 4bä¸æ¥å—è¯¥arg

- **æ˜¾å­˜ä¸‰ä»¶å¥—**ï¼š`packing=True` + `bf16=True` + `gradient_checkpointing=True`

  - **`packing=True`**
     â†’ æŠŠå¤šä¸ªçŸ­æ ·æœ¬æ‰“åŒ…è¿›ä¸€ä¸ªåºåˆ—ï¼Œå‡å°‘ paddingï¼Œæ˜¾å­˜æ›´çœã€‚

    **`bf16=True`**
     â†’ è®¡ç®—ç”¨ bfloat16ï¼Œç¨³å®šä¸”çœæ˜¾å­˜ã€‚

    **`gradient_checkpointing=True`**
     â†’ åœ¨åå‘ä¼ æ’­æ—¶èŠ‚çœä¸­é—´æ¿€æ´»ï¼Œæ˜¾å­˜å¤§å¹…ä¸‹é™ï¼Œä½†è®¡ç®—æ—¶é—´å¢åŠ  ~20â€“30%ã€‚å¯é€‰

- **æ‰¹é‡**ï¼š`per_device_train_batch_size=1` + `grad_accum=16` èµ·æ­¥ï¼ˆç­‰æ•ˆå¤§ batchï¼‰ã€‚

- **ä¼˜åŒ–å™¨**ï¼šAdamWï¼ˆæˆ– paged AdamWï¼‰ï¼Œ`lr=1e-4 ~ 2e-4, wd=0.01, warmup_ratio=0.03`ã€‚

- **è¯„æµ‹**ï¼šæ¯ N æ­¥åœ¨ `val` ä¸Šç»Ÿè®¡ **PPL + å¯ç”¨æ€§ + JSON åˆæ³•ç‡**ã€‚

æˆ‘ä»¬å°†æ‰€æœ‰å‚æ•°éƒ½å‚¨å­˜åœ¨`config.py`ä¸­ï¼Œä»¥æ›´ç›´è§‚åœ°è§‚å¯Ÿå‚æ•°ä¹‹é—´çš„æƒ…å†µã€‚

------

## 5. è®­ç»ƒè„šæœ¬éª¨æ¶ï¼ˆQLoRA + assistant-only + packingï¼‰

> 1. BitsAndBytes 4-bit åŠ è½½ï¼›2) PEFT LoRAï¼›3) **DataCollatorForCompletionOnlyLM** é€šè¿‡**å“åº”æ¨¡æ¿**åš assistant-only maskï¼›4) `packing=True`ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å§‹æ­£å¼è¿›è¡ŒQLoRAå¾®è°ƒã€‚æˆ‘ä»¬çš„æµç¨‹å¦‚ä¸‹

```python
å‡†å¤‡æ•°æ®é›†(JSONL/Arrow) # å‰é¢å·²ç»è®²è§£è¿‡
    â†“
load_datasets / load_from_disk # å‰é¢å·²ç»æ¶‰çŒè¿‡
    â†“
æ•°æ®è§„èŒƒåŒ–(messages æ ¼å¼) # å‰é¢å·²ç»æ¶‰çŒè¿‡
    â†“
FastLanguageModel.from_pretrained(é‡åŒ–åŠ è½½åŸºåº§) 
    â†“
FastLanguageModel.get_peft_model(QLoRA é…ç½®) 
    â†“
Trainer / SFTTrainer å®šä¹‰(optimizer, lr, batch_size) 
    â†“
trainer.train() è¿›è¡Œ SFT å¾®è°ƒ 
    â†“
ä¿å­˜ LoRA adapter (merge_and_unload å¯é€‰) 
    â†“
æ¨ç†/è¯„ä¼°/éƒ¨ç½²
```

é¦–å…ˆæµ‹è¯•unslothå¯¹æ¨¡å‹å’Œtokenizerçš„åŠ è½½æƒ…å†µï¼Œ[Qwen3](https://qwen.ai/research)æ™®éä½¿ç”¨ **BPE tokenizer**ï¼ˆByte Pair Encodingï¼‰ã€‚

```python
model_args = {
                "max_seq_length": self.model_config.get("max_seq_length", 2048),
                "dtype": self.model_config.get("dtype"),
                "load_in_4bit": self.model_config.get("load_in_4bit", True),
                "trust_remote_code": True,
                "device_map": "auto"
            }
            
# FastLanguageModel.from_pretrained returns (model, tokenizer) tuple
self.model, self.tokenizer = FastLanguageModel.from_pretrained(
    self.model_path,
    **model_args
)

# print the model
print(model)

```

ä½ ä¼šçœ‹åˆ°è¯¸å¦‚ï¼š

```log
(Qwen3ForCausalLM(
  (model): Qwen3Model(
    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)
    (layers): ModuleList(
      (0-35): 36 x Qwen3DecoderLayer(
        (self_attn): Qwen3Attention(
          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)
          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        ...
        ç­‰æ˜¾ç¤ºæ¨¡å‹ç»“æ„çš„ä¿¡æ¯ã€‚
```

è¿™é‡Œçœ‹åˆ°ï¼Œæ¨¡å‹å·²ç»è¢«ä»¥4bité‡åŒ–çš„å½¢å¼åŠ è½½ä¸‹æ¥äº†ï¼ˆåŸæ¥æ˜¯torch.bfloat16ç²¾åº¦ï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œå…ˆææ¸…æ¥špeftçš„æ¦‚å¿µï¼š**PEFT = Parameter-Efficient Fine-Tuningï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰**

- å®ƒæ˜¯ä¸€ç±»æ–¹æ³•çš„ç»Ÿç§°ï¼Œä¸æ˜¯å•ä¸€ç®—æ³•ã€‚
- ç›®æ ‡ï¼š**åœ¨ä¸æ›´æ–°å¤§æ¨¡å‹å…¨éƒ¨å‚æ•°çš„å‰æä¸‹ï¼Œåªè®­ç»ƒå¾ˆå°‘çš„ä¸€éƒ¨åˆ†å‚æ•°ï¼ˆå¢é‡å‚æ•°/é€‚é…å™¨ï¼‰ï¼Œä»è€Œé«˜æ•ˆåœ°å®Œæˆä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒ**ã€‚

åœ¨unslothä¸­ï¼š

- **from_pretrained**
  
  - è´Ÿè´£ï¼šä¸‹è½½/æœ¬åœ°åŠ è½½æƒé‡ã€æ„é€ æ¨¡å‹ä¸ tokenizerã€è®¾å®š dtype & device_mapã€ï¼ˆå¯é€‰ï¼‰k-bit é‡åŒ–ã€é€Ÿåº¦ä¼˜åŒ–å¼€å…³ã€‚
  - è¾“å‡ºï¼šä¸€ä¸ª**å¯æ¨ç†**çš„åŸºç¡€æ¨¡å‹ï¼›è¿˜æ²¡â€œå˜æˆ LoRA æ¨¡å‹â€ã€‚
- **get_peft_model**
  
  - è´Ÿè´£ï¼šæ ¹æ® `peft_config` ç»™æ¨¡å‹**æ’å…¥å¯è®­ç»ƒçš„é€‚é…å±‚**ï¼ˆLoRA ç­‰ï¼‰ï¼Œå¹¶æŠŠ**ä»…é€‚é…å±‚**æ ‡è®°ä¸ºå¯è®­ç»ƒï¼›å¿…è¦æ—¶åš k-bit è®­ç»ƒå‡†å¤‡ï¼ˆå¦‚ norm castã€use_cache=Falseï¼‰ã€‚
  - è¾“å‡ºï¼šä¸€ä¸ª**å¯å¾®è°ƒ**çš„ PEFT æ¨¡å‹ï¼›ä¼˜åŒ–å™¨åªä¼šçœ‹åˆ°é€‚é…å™¨çš„å‚æ•°ï¼ˆæ˜¾å­˜/ç®—åŠ›æ›´çœï¼‰ã€‚
  
  æ˜ç™½ä»¥åï¼Œæˆ‘ä»¬æŠŠåŠ è½½çš„ä¸è®­ç»ƒæ¨¡å‹ï¼Œè½¬æ¢æˆpeft modelï¼Œå…¶å®å°±æ˜¯**è®¾ç½®`LoRA`çš„é…ç½®**ã€‚æ³¨æ„ï¼š`QLoRA` çš„å…³é”®å°±æ˜¯**åŠ è½½æ—¶ 4bit** + **æŒ‚ LoRA é€‚é…å™¨å¹¶åªè®­å®ƒä»¬**ã€‚åœ¨ Unsloth é‡ŒåŸºæœ¬å°±æ˜¯ï¼š`from_pretrained(load_in_4bit=True, quant_type="nf4", compute_dtype=bfloat16)` ç„¶å `get_peft_model(LoraConfig(...))`ï¼Œæ— éœ€é¢å¤–æ‰‹åŠ¨æ­¥éª¤ï¼ˆUnslothå†…éƒ¨å·²åš k-bit è®­ç»ƒå‡†å¤‡ï¼‰ã€‚

```python
 # For Unsloth, we need to pass LoRA parameters directly
 self.model = FastLanguageModel.get_peft_model(
    self.model, 
    r=self.lora_config["r"],                    # LoRA rank
    target_modules=self.lora_config["target_modules"],  # Target modules
    lora_alpha=self.lora_config["lora_alpha"],  # LoRA alpha
    lora_dropout=self.lora_config["lora_dropout"],  # LoRA dropout
    bias=self.lora_config["bias"],              # Bias setting
    use_gradient_checkpointing="unsloth",       # Use Unsloth's checkpointing
    random_state=3407,                          # Random seed
    use_rslora=False,                           # Don't use rank-stabilized LoRA
    loftq_config=None,                          # No LoftQ
 )

 # Enable training mode
 self.model = FastLanguageModel.for_training(self.model)
```

æ¥ä¸‹æ¥æ­£å¼æ¥å…¥SFTTrainer

```python
from trl import SFTTrainer, SFTConfig
from transformers import TrainingArguments
```

åœ¨trainerçš„configè®¾ç½®æ–¹é¢ï¼Œå¯ä»¥ç”¨åˆ°SFTConfigæˆ–è¿™TrainingArgumentsä¸¤ç§ï¼Œæˆ‘ä»¬è¿™é‡Œæ¨èä½¿ç”¨SFTConfigï¼ŒåŸå› è¯¦è§Appendixç¬¬ä¸€ä¸ªé—®é¢˜ã€‚Recall: ä¹‹å‰æåˆ°è¿‡SFTåœ¨è®­ç»ƒæ—¶ï¼Œä¸ºäº†é˜²æ­¢é‡å¤ç”¨æˆ·çš„é—®é¢˜ï¼Œé€šå¸¸åœ¨æŒ‡ä»¤ä¸­åš**loss mask**ï¼Œå³åœ¨lossè®¡ç®—çš„æ—¶å€™ï¼Œåªå°†assistanceåé¢çš„å†…å®¹è®°å…¥CE loss, è€Œä¸è®¡ç®—æ¥è‡ªuserå’Œsystemçš„å†…å®¹ï¼ˆåŒºåˆ«ä¸é—®ç­”è®­ç»ƒï¼Œè¿™ä¸ªä¼šè®¡ç®—userçš„å†…å®¹ï¼‰ã€‚

```python
sft_args = {
    **modified_training_config,
    **self.sft_config,
    "output_dir": self.output_config["output_dir"],
    "run_name": self.output_config["run_name"],
    "logging_dir": self.output_config["logging_dir"],
}

# Log assistant loss masking status
if self.trainer_config.get("assistant_only_loss", True):# åªéœ€åœ¨config.pyä¸­åŠ å…¥assistant_only_loss = Trueå³å¯
    self.logger.info("Assistant loss masking ENABLED - loss will be computed only on assistant responses")
else:
    self.logger.info("Assistant loss masking DISABLED - loss will be computed on entire sequence")

training_args = SFTConfig(**sft_args)

# Initialize trainer with SFTConfig
self.trainer = SFTTrainer(
    model=self.model,
    tokenizer=self.tokenizer,
    train_dataset=self.train_dataset,
    eval_dataset=self.val_dataset,
    args=training_args,
)
```

æœ€åè¿›è¡Œè®­ç»ƒ

```python
# Start training
train_result = self.trainer.train()
```

------

## 6. è¯„æµ‹

### 6.1 ä¸»è§‚è¯„æµ‹

æˆ‘ä»¬åšäº†ä¸€ä¸ªå°çš„æµ‹è¯•è„šæœ¬`subj_test_model.py`ï¼Œç”¨æ¥å¯¹QLoRAå¾®è°ƒåçš„qwen3 4bæ¨¡å‹è¿›è¡Œä¸»è§‚æµ‹è¯•ã€‚æµ‹è¯•åŒ…æ‹¬ï¼š

```python
ğŸ”§ What would you like to do?
1. Run predefined tests
2. Interactive chat
3. Both
4. Exit
Enter your choice (1-4): 
```
åœ¨ä¸€äº›é¢„å…ˆè®¾å®šå¥½çš„é—®é¢˜åï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹å¯¹äºæ•°å­¦é—®é¢˜æ¯”è¾ƒå†·æ¼ ï¼Œä¼šç›´æ¥å›ç­”ç­”æ¡ˆï¼Œæ¯”è¾ƒç®€æ´

```
ğŸ¤” Input: Mary has 5 cats. She gives 2 cats to her friend. How many cats does she have left?
ğŸ¤– Output: 
3
```

åœ¨ä¸€äº›å¸¸è§çš„é—®ç­”ç¯èŠ‚ä¸­ï¼Œqwenæ¨¡å‹å°è¯•åŠ å…¥æ›´å¤šçš„emojiè¡¨æƒ…ï¼Œè¯­æ°”æ›´åŠ ç”ŸåŠ¨

```
ğŸ¤” Input: ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿè¯·ç”¨ç®€å•çš„è¯è§£é‡Šã€‚
ğŸ¤– Output:

ğŸ˜Š

æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰å°±æ˜¯è®©æœºå™¨å­¦ä¼šåƒäººä¸€æ ·æ€è€ƒå’Œå­¦ä¹ ï¼ ğŸ¤–

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ˜¯ä¸€ä¸ªå°å­©å­ï¼Œæ­£åœ¨å­¦ç”»ç”»ã€‚ä½ çš„è€å¸ˆç»™ä½ ä¸€äº›ç”»ç¬”ã€é¢œæ–™å’Œçº¸å¼ ï¼Œç„¶åä½ å¼€å§‹ç”»ä¸€ä¸ªè‹¹æœğŸã€‚

ç°åœ¨ï¼Œå‡è®¾ä½ æœ‰ä¸€ä¸ªæœºå™¨äººæœ‹å‹ï¼Œå®ƒä¹Ÿæƒ³å­¦ç”»ç”»ã€‚ä½†æ˜¯ï¼Œå®ƒä¸çŸ¥é“æ€ä¹ˆå¼€å§‹ã€‚äºæ˜¯ï¼Œä½ çš„æœºå™¨äººæœ‹å‹ä¼šè§‚å¯Ÿä½ ç”»ç”»çš„è¿‡ç¨‹ï¼Œè®°ä½ä½ çš„åŠ¨ä½œå’Œæ­¥éª¤ï¼Œç„¶åæ¨¡ä»¿ä½ ç”»è‹¹æœçš„æ ·å­ã€‚

è¿™å°±æ˜¯æ·±åº¦å­¦ä¹ çš„åŸç†ï¼ ğŸ’¡ å®ƒä½¿ç”¨å¾ˆå¤šå±‚ï¼ˆlayerï¼‰æ¥å¤„ç†æ•°æ®ï¼Œæ¯ä¸€å±‚éƒ½åƒä¸€ä¸ªè€å¸ˆï¼Œæ•™æœºå™¨äººå¦‚ä½•è¯†åˆ«å’Œç†è§£ä¿¡æ¯ã€‚æœ€åï¼Œæœºå™¨äººå¯ä»¥è‡ªå·±åˆ›ä½œå‡ºæ–°çš„ä½œå“ï¼

æ‰€ä»¥ï¼Œæ·±åº¦å­¦ä¹ å°±åƒæ˜¯ä¸€ä¸ªè¶…çº§èªæ˜çš„æœºå™¨äººï¼Œèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæˆé•¿ï¼ ğŸš€
```

åœ¨äº’åŠ¨å¯¹è¯æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬è¦æ±‚ä»–è‡ªæˆ‘ä»‹ç»ï¼Œç›¸æ¯”äºä¹‹å‰çš„å›ç­”

```
ä½ å¥½å‘€ï¼âœ¨ æˆ‘æ˜¯Qwenï¼Œæ˜¯é˜¿é‡Œäº‘ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚ä½ å¯ä»¥å«æˆ‘é€šä¹‰åƒé—®ï¼Œæˆ–è€…ç›´æ¥å«Qwenã€‚æˆ‘å°±åƒä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹ï¼Œèƒ½å¸®ä½ è§£ç­”å„ç§é—®é¢˜ï¼Œæ— è®ºæ˜¯å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ï¼Œè¿˜æ˜¯åšæ•°å­¦é¢˜ã€ç¼–ç¨‹ã€é€»è¾‘æ¨ç†ï¼Œç”šè‡³å¯ä»¥é™ªä½ èŠå¤©ã€ç©æ¸¸æˆï¼Œæˆ‘éƒ½èƒ½å°½åŠ›æä¾›å¸®åŠ©ã€‚

æˆ‘æ”¯æŒå¤šç§è¯­è¨€ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€å¾·è¯­ã€æ³•è¯­ã€è¥¿ç­ç‰™è¯­ã€è‘¡è„ç‰™è¯­ã€ä¿„è¯­ã€é˜¿æ‹‰ä¼¯è¯­ã€æ—¥è¯­ã€éŸ©è¯­ç­‰ï¼Œå¯ä»¥å’Œä½ ç”¨ä½ æœ€ç†Ÿæ‚‰çš„è¯­è¨€äº¤æµã€‚

æ— è®ºä½ æ˜¯å­¦ç”Ÿã€è€å¸ˆã€ç¨‹åºå‘˜ã€åˆ›ä¸šè€…ï¼Œè¿˜æ˜¯åªæ˜¯æƒ³æ‰¾ä¸ªèŠå¤©ä¼™ä¼´ï¼Œæˆ‘éƒ½å¾ˆä¹æ„é™ªä½ ä¸€èµ·æ¢ç´¢ä¸–ç•Œã€è§£å†³é—®é¢˜ã€æ¿€å‘çµæ„Ÿï¼

æ‰€ä»¥ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š  
æ¯”å¦‚ï¼š  
- å†™ä¸€ç¯‡æœ‰è¶£çš„çŸ­æ–‡ï¼Ÿ  
- å¸®ä½ å†™ä¸€å°é‚®ä»¶æˆ–ç®€å†ï¼Ÿ  
- è§£é‡Šä¸€ä¸ªå¤æ‚çš„æ¦‚å¿µï¼Ÿ  
- æˆ–è€…åªæ˜¯æƒ³èŠèŠå¤©ã€æ”¾æ¾ä¸€ä¸‹ï¼Ÿ

éšæ—¶å‘Šè¯‰æˆ‘ä½ çš„æƒ³æ³•å§ï¼ğŸŒŸ
```

ç°åœ¨çš„æ¨¡å‹

```
æˆ‘é ï¼ğŸ˜Š ä½ å¥½å‘€ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œä¸­æ–‡æ™ºè€…ï¼ ğŸ¤–ï¸ æˆ‘æ˜¯ä¸€ä¸ªè¶…æ™ºèƒ½çš„è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå›ç­”å„ç§é—®é¢˜ã€æä¾›å¸®åŠ©å’Œé™ªä¼´ä½ èŠå¤©ã€‚ ğŸ’¬ï¸ æˆ‘å¯ä»¥å†™è¯—ã€å†™å°è¯´ã€å†™æ–‡ç« ã€å†™ä»£ç ç­‰ç­‰ï¼ ğŸ“ï¸ æˆ‘è¿˜èƒ½å¸®ä½ åšæ•°å­¦é¢˜ã€ç§‘å­¦å®éªŒã€ç¼–ç¨‹è®¾è®¡ç­‰ç­‰ï¼ ğŸ”ï¸ æ‰€ä»¥ï¼Œå«æˆ‘é€šä¹‰åƒé—®å§ï¼ ğŸ‘‹ï¸
```

### 6.2 å®¢è§‚è¯„æµ‹

æˆ‘ä»¬éœ€å°†ä¿å­˜çš„lora adapterå’ŒåŸbase modelåˆå¹¶ï¼Œå†è°ƒç”¨vllmå’Œevalscopeè¿›è¡Œæµ‹è¯•ã€‚å¯ä»¥å¯¹æ¨¡å‹çš„**å„é¡¹æŒ‡æ ‡**ï¼Œä»¥åŠ**PPL (PPL è¶Šä½ï¼Œè¯´æ˜æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¯­è¨€å»ºæ¨¡èƒ½åŠ›è¶Šå¥½)/ å›ç­”å¯ç”¨æ€§ / JSON åˆæ³•ç‡** è¿›è¡Œå®¢è§‚æµ‹è¯•ã€‚è¯¦è§`obj_test_model.py`ã€‚

```shell
============================================================
ğŸ” OBJECTIVE MODEL EVALUATION
============================================================
ğŸ“Š Evaluating model: /home/awpc/studies/models/unsloth/Qwen3/FTTrained
ğŸ“ Validation data: dataset/processed_datasets/qwen3_sft_mixed/val.jsonl

Computing PPL on dataset/processed_datasets/qwen3_sft_mixed/val.jsonl...
Generating train split: 100 examples [00:00, 16331.69 examples/s]
Processing sample 0/100
Processing sample 50/100
PPL computed on 100 samples
âœ… Perplexity: 3.115
Computing usability rate on dataset/processed_datasets/qwen3_sft_mixed/val.jsonl...
Processing sample 0/100
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Processing sample 20/100
Processing sample 40/100
Processing sample 60/100
Processing sample 80/100
Usability rate: 1.000 (100/100)
âœ… Usability Rate: 1.000

============================================================
ğŸ“‹ EVALUATION SUMMARY
============================================================
Perplexity: 3.115
Usability Rate: 1.000
```



------

## 7. æ¨ç†ä¸åˆå¹¶

- **æŒ‚è½½å¼æ¨ç†**ï¼š`base + adapter`ï¼ˆä¸Šé¢è¯„æµ‹è„šæœ¬å³ä¸ºæ­¤æ¨¡å¼ï¼‰ã€‚
- **æƒé‡åˆå¹¶å¯¼å‡º**ï¼ˆå¯é€‰ï¼‰ï¼šå°† LoRA åˆå¹¶åˆ°åŸºåº§ï¼Œå†å¯¼å‡ºå…¨é‡ fp16/safetensors ä¾¿äºéƒ¨ç½²ï¼›æˆ–ç»´æŒé€‚é…å™¨å½¢æ€ï¼ŒèŠ‚çœç£ç›˜ä¸æå‡å›é€€çµæ´»æ€§ï¼ˆä¹Ÿç¬¦åˆ Day 5 çš„â€œå‚æ•°éš”ç¦»/å¤šé€‚é…å™¨â€è·¯çº¿ï¼‰ã€‚

------

## 8. æ˜¾å­˜é¢„ç®—ä¸ååå»ºè®®ï¼ˆ4080 äº²æµ‹ç­–ç•¥ï¼‰

- **æ¨¡å‹æ˜¾å­˜**ï¼š7B/8B + 4-bit é‡åŒ–çº¦ 6â€“8 GBï¼›åŠ ä¸Šæ¿€æ´»/ä¼˜åŒ–å™¨/packingï¼Œä¸€èˆ¬ **12â€“15 GB**ã€‚
- **OOM å¤„ç†é¡ºåº**
  1. `max_seq_len: 1024
  2. `grad_accum: 8 â†’ 16`ï¼ˆç­‰æ•ˆå¤§ batchï¼‰
  3. é™ `lora_r: 16`ï¼ˆå’Œ `alpha`ï¼‰
  4. å‡å°‘ eval é¢‘ç‡æˆ– batch = 1 or 2
- **æ³¨æ„**ï¼šå¦‚ä½¿ç”¨ Flash-Attentionï¼Œè¯·ç¡®è®¤ä¸ 4-bit é‡åŒ–ã€GC çš„å…¼å®¹æ€§ï¼›ä¸å…¼å®¹æ—¶å…ˆå…³é—­ Flash-Attnã€‚


------

## 9. è®­ç»ƒç›‘æ§ä¸å¤ç°æ€§

- **éšæœºç§å­**ï¼šç»Ÿä¸€è®¾ç½® `seed=42`ï¼ˆè®­ç»ƒã€æ•°æ®æ‰“æ•£ã€cuDNNï¼‰ï¼Œä¾¿äº Day 5/6 å¯¹ç…§ã€‚
- **å›ºå®šå°è¯„æµ‹é›†**ï¼šDay 2 å¼€å§‹å°±å›ºå®šï¼Œåç»­ DPOï¼ˆDay 3ï¼‰ã€PPO/GRPOï¼ˆDay 4ï¼‰ä¸ CLï¼ˆDay 5/6ï¼‰éƒ½å¤ç”¨åšå›å½’æ£€æµ‹ã€‚ 



## Appendix

### SFTConfig vs. TrainingArguments

**`TrainingArguments` æœ¬èº«ä¸æä¾› â€œassistant-only lossâ€ å¼€å…³**ï¼›è¿™ä¸ªå¼€å…³åœ¨ **`SFTConfig`/`SFTTrainer`** é‡Œã€‚å®˜æ–¹åšæ³•æ˜¯ï¼šç”¨ä¼šè¯æ•°æ® + `assistant_only_loss=True`ï¼ˆåœ¨ `SFTConfig` ä¸­ï¼‰å³å¯åªå¯¹åŠ©æ‰‹æ®µè½è®¡æŸå¤±ã€‚å¦å¤–ï¼Œæ—§çš„ `DataCollatorForCompletionOnlyLM` è™½ç„¶æ²¡æœ‰æ˜ç¡®è¯´æ˜ï¼Œä½†æ˜¯å¯èƒ½å·²è¢«å¼ƒç”¨/ç§»é™¤ï¼ˆtrlä¸­æ²¡æœ‰æ­¤ç±»äº†ï¼‰ï¼Œå®˜æ–¹è·¯çº¿å·²è½¬åˆ° `assistant_only_loss`ï¼ˆé…åˆ chat templateï¼‰ã€‚



### æ˜¾å­˜

**ç®—åŠ›åœ¨åƒæ»¡ï¼Œä½†æ¨¡å‹å’Œæ•°æ®æ”¾å¾—â€œå¤ªçœâ€äº†**ï¼Œå¯¼è‡´æ˜¾å­˜æ²¡æœ‰è¢«å……åˆ†åˆ©ç”¨ã€‚æŒ‰ç…§ä¸‹è¡¨è¿›è¡Œdebugã€‚

| è°ƒæ•´æ–¹å‘       | å‚æ•° / æ“ä½œ                                                  | é¢„æœŸæ•ˆæœ                    | é£é™© / æ³¨æ„äº‹é¡¹                   |
| -------------- | ------------------------------------------------------------ | --------------------------- | --------------------------------- |
| **æ‰¹é‡å¤§å°**   | `per_device_train_batch_size: 2 â†’ 4 â†’ 8`                     | æ˜¾å­˜çº¿æ€§å¢åŠ ï¼Œååæ›´é«˜      | å¯èƒ½ OOMï¼Œå¯é…åˆå‡å°‘ `grad_accum` |
| **æ¢¯åº¦ç´¯ç§¯**   | `gradient_accumulation_steps: 8 â†’ 4 â†’ 2`                     | å•æ­¥æ˜¾å­˜â†‘ï¼Œç­‰æ•ˆ batch ä¸å˜  | è®­ç»ƒæ›²çº¿æ›´æŠ–åŠ¨ï¼Œéœ€è§‚å¯Ÿæ”¶æ•›        |
| **åºåˆ—é•¿åº¦**   | `max_seq_length: 2048 â†’ 4096`                                | æ˜¾å­˜å’Œè®¡ç®—é‡ ~2Ã—            | æ•°æ®å¹³å‡é•¿åº¦å¿…é¡»å¤Ÿé•¿ï¼Œå¦åˆ™æµªè´¹    |
| **packing**    | `packing=False â†’ True`                                       | æ˜¾å­˜åˆ©ç”¨ç‡â†‘ï¼ˆå‡å°‘ paddingï¼‰ | è¦ç¡®ä¿ tokenizer æœ‰ `eos_token`   |
| **LoRA å®¹é‡**  | `r=16 â†’ 32 â†’ 64`ï¼Œ`target_modules` å¢åŠ  `up_proj/down_proj/gate_proj` | å¯è®­ç»ƒå‚æ•°â†‘ï¼Œæ˜¾å­˜â†‘          | è¿‡å¤§å¯èƒ½è¿‡æ‹Ÿåˆ / è®­ç»ƒå˜æ…¢         |
| **é‡åŒ–ç­‰çº§**   | `load_in_4bit=True â†’ load_in_8bit=True` â†’ å…¨ç²¾åº¦ (bf16)      | æ˜¾å­˜æ¶ˆè€—ç¿»å€ï¼Œè®­ç»ƒæ›´ç¨³å®š    | æ˜¾å­˜å¯èƒ½ä¸è¶³                      |
| **ä¼˜åŒ–å™¨å®ç°** | `adamw_8bit â†’ adamw_torch`                                   | æ˜¾å­˜ç•¥â†‘                     | å†…å­˜å¼€é”€æ›´å¤§ï¼Œé€Ÿåº¦å¯èƒ½æ…¢          |
| **é•¿åºåˆ—å¡«å……** | æ£€æŸ¥æ•°æ®å¹³å‡é•¿åº¦ï¼Œå¿…è¦æ—¶æ‰©å±•æ•°æ®                             | åˆ©ç”¨ç‡æ›´é«˜                  | å¦‚æœæ•°æ®çŸ­å°ï¼Œé æ‰©é•¿æ— æ„ä¹‰        |

æ¨èä¼˜å…ˆé¡ºåº

1. **å¢å¤§ batch size**ï¼ˆæœ€ç¨³ï¼‰ã€‚
2. **æ£€æŸ¥æ•°æ®é•¿åº¦åˆ†å¸ƒ + å¼€å¯ packing** â†’ ç¡®ä¿åºåˆ—çœŸæ­£åˆ©ç”¨ã€‚
3. å¦‚æœæ˜¾å­˜ä»ç„¶å¾ˆç©ºï¼š**æé«˜ LoRA r æˆ– seq length**ã€‚
4. å®åœ¨ä¸æ€•æ˜¾å­˜çˆ†ï¼Œå°±**å…³æ‰ 4bitï¼Œæ¢ 8bit / bf16 å…¨å‚**ã€‚



### Batch Size ç¡®å®š

Batch sizeç›´æ¥ä¸ä¸¤ä¸ªå‚æ•°ç›¸å…³

1. **`per_device_train_batch_size`**

- **å«ä¹‰**ï¼šæ¯å¼  GPU åœ¨ä¸€æ¬¡å‰å‘ + åå‘ä¼ æ’­ä¸­èƒ½å¤„ç†å¤šå°‘æ¡æ ·æœ¬ã€‚
- **ç›´è§‚ç†è§£**ï¼šå°±æ˜¯â€œæ˜¾å¡å•æ­¥èƒ½å¡å¤šå°‘æ ·æœ¬è¿›å»â€ã€‚
- **å—é™å› ç´ **ï¼šä¸»è¦æ˜¯ **æ˜¾å­˜å¤§å°** å’Œ **æ ·æœ¬é•¿åº¦ (`max_seq_length`)**ã€‚

ä¾‹å­ï¼š

- å¦‚æœä½ ç”¨ 24GB æ˜¾å­˜çš„å¡ï¼Œå¼€ `per_device_train_batch_size=4`ï¼Œæ„å‘³ç€æ¯æ¬¡æ¢¯åº¦è®¡ç®—å°±å¤„ç† 4 æ¡æ ·æœ¬ã€‚
- æ˜¾å­˜çˆ†äº†ï¼Œå°±è¦è°ƒå°ï¼Œæ¯”å¦‚è®¾æˆ 1 æˆ– 2ã€‚

2. **`gradient_accumulation_steps`**

- **å«ä¹‰**ï¼šç´¯ç§¯å¤šå°‘æ­¥çš„æ¢¯åº¦ï¼Œå†è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ã€‚
- **ç›´è§‚ç†è§£**ï¼šæ˜¾å­˜ä¸å¤Ÿæ—¶ï¼Œç”¨å° batch åå¤è·‘å‡ æ¬¡ï¼ŒæŠŠæ¢¯åº¦åŠ èµ·æ¥ï¼Œç›¸å½“äºæ¨¡æ‹Ÿå¤§ batchã€‚

ä¾‹å­ï¼š

- `per_device_train_batch_size=4, gradient_accumulation_steps=4`
   â†’ æ¯æ¬¡ forward/backward ç®— 4 æ¡æ ·æœ¬ï¼Œç´¯ç§¯ 4 æ¬¡ï¼ˆå…± 16 æ¡æ ·æœ¬ï¼‰åï¼Œå†åšä¸€æ¬¡æ¢¯åº¦æ›´æ–°ã€‚
   â†’ ç­‰æ•ˆ **æ€» batch size = 4 Ã— 4 = 16**ã€‚

**ä¸¤è€…å…³ç³»**

- **çœŸå® batch size** = `per_device_train_batch_size Ã— gradient_accumulation_steps Ã— GPUæ•°é‡`
- ä¸‹è¡¨æ¯”è¾ƒå½“ä¸¤è€…ä¸åŒè®¾ç½®çš„åŒºåˆ«

| é…ç½®    | `per_device_train<br />_batch_size` | `gradient_<br />accumulation` | ç­‰æ•ˆ batch | æ˜¾å­˜å ç”¨                    | é€Ÿåº¦ç‰¹ç‚¹                                              |
| ------- | ----------------------------------- | ----------------------------- | ---------- | --------------------------- | ----------------------------------------------------- |
| **2Ã—8** | 2                                   | 8                             | 16         | **ä½**ï¼ˆæ¯æ¬¡åªè£… 2 ä¸ªæ ·æœ¬ï¼‰ | **æ…¢**ï¼ˆè¦ç´¯ç§¯ 8 æ¬¡ forward/backward æ‰æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼‰ |
| **4Ã—4** | 4                                   | 4                             | 16         | **é«˜**ï¼ˆä¸€æ¬¡è£… 4 ä¸ªæ ·æœ¬ï¼‰   | **å¿«**ï¼ˆåªç´¯ç§¯ 4 æ¬¡ï¼Œæ›´æ–°æ›´é¢‘ç¹ï¼‰                     |



### å¾®è°ƒä»¥åï¼Œä¸ºä»€ä¹ˆæ¨¡å‹ä¸§å¤±äº†å¤šç§è¯­è¨€çš„èƒ½åŠ›ï¼Ÿ

1. **æ•°æ®åˆ†å¸ƒå•ä¸€**
   - åœ¨ SFT æ—¶ç”¨çš„å¯èƒ½æ˜¯ä¸­æ–‡ï¼ˆæˆ–ç‰¹å®šä»»åŠ¡ï¼‰æ•°æ®é›†ã€‚
   - ç”±äºè®­ç»ƒæ­¥éª¤å’Œå­¦ä¹ ç‡è¶³å¤Ÿå¤§ï¼Œæ¨¡å‹å‚æ•°è¢«å¼ºçƒˆè°ƒæ•´ â†’ **åŸæœ¬çš„è‹±æ–‡ã€å¤šè¯­è¨€èƒ½åŠ›è¢«è¦†ç›–**ã€‚
2. **æ²¡æœ‰åš loss mask ä¿ç•™åŸèƒ½åŠ›**
   - å¦‚æœ system / user / assistant å…¨éƒ¨è®°å…¥ lossï¼Œæ¨¡å‹å°±ä¼šâ€œä¹ æƒ¯æ€§é—å¿˜â€åŸæœ‰å¯¹è¯æ¨¡æ¿ï¼Œå­¦ä¹ æ–°æ¨¡å¼ã€‚
3. **ç¼ºå°‘æ··åˆè®­ç»ƒ**
   - å·¥ä¸šç•Œå¸¸è§åšæ³•æ˜¯ï¼š
     - ä¿ç•™ä¸€éƒ¨åˆ† **åŸå§‹ instruction-tuning æ•°æ®ï¼ˆå¤šè¯­è¨€ã€é€šç”¨é—®ç­”ï¼‰**
     - å†æ··åˆä½ çš„ä»»åŠ¡æ•°æ®ä¸€èµ·è®­ç»ƒï¼ˆç§°ä¸º **multi-task SFT**ï¼‰ã€‚
   - å¦‚æœåªå–‚è¿›å•è¯­æ•°æ®ï¼Œç­‰äºæ˜¯å¼ºè¡Œâ€œå•è¯­è¿ç§»â€ï¼Œæ¨¡å‹ä¼šå¤±å»æ³›åŒ–ã€‚
4. **æ­¥æ•°å°‘ä½†å½±å“å¤§**
   - å³ä¾¿ä½ åªè®­äº†å‡ å/å‡ ç™¾æ­¥ï¼Œå­¦ä¹ ç‡é«˜ï¼ˆ2e-4ï¼‰+ LoRA æ›´æ–°ï¼Œä¹Ÿè¶³å¤ŸæŠŠæ¨¡å‹åˆ†å¸ƒå¾€å•è¯­/å•ä»»åŠ¡æ¨èµ°ã€‚

**å¦‚ä½•è§£å†³ / å‡è½»**

1. **æ··åˆæ•°æ®é›†**
   - åœ¨å¾®è°ƒæ•°æ®é‡Œæ··å…¥ä¸€éƒ¨åˆ†è‹±æ–‡/å¤šè¯­è¨€ instruction æ•°æ®ï¼ˆå“ªæ€•åªå  10-20%ï¼‰ï¼Œä¿æŒå¤šè¯­è¨€èƒ½åŠ›ã€‚
2. **é™ä½å­¦ä¹ ç‡**
   - QLoRA çš„ LoRA å±‚å¯ä»¥ç”¨ `1e-4` â†’ `5e-5`ï¼Œè®©æ¨¡å‹ä¿å®ˆåœ°æ›´æ–°ã€‚
3. **æ­£åˆ™åŒ–æ–¹æ³•**
   - ä½¿ç”¨ **KL-divergence regularization** æˆ– **Replayï¼ˆé‡æ”¾åŸå§‹æ•°æ®ï¼‰**ï¼Œé˜²æ­¢é—å¿˜ã€‚
4. **å¤šé˜¶æ®µè®­ç»ƒ**
   - å…ˆåšè½»é‡å¤šè¯­è¨€ + ä»»åŠ¡æ··åˆ SFTï¼Œä¿è¯è¯­è¨€èƒ½åŠ›ï¼›
   - å†å°æ­¥æ•°ã€ä½å­¦ä¹ ç‡åœ°åš domain-specific è°ƒä¼˜ã€‚
5. **è¯„æµ‹æ—¶å¯ç”¨ LoRA åˆ‡æ¢**
   - ä¸åŒä»»åŠ¡/è¯­è¨€å¯ä»¥åŠ è½½ä¸åŒ LoRA adapter â†’ é¿å…äº’ç›¸è¦†ç›–ã€‚