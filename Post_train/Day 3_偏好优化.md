# Day 3  åå¥½ä¼˜åŒ–

[TOC]



## 1. åå¥½å­¦ä¹ åˆ°åº•åœ¨å­¦ä»€ä¹ˆï¼Ÿ

ç»™å®šè¾“å…¥ $x$ å’Œä¸€å¯¹è¾“å‡º $(y^+, y^-)$ï¼Œå…¶ä¸­ $y^+$ ä¼˜äº $y^-$ã€‚åå¥½å­¦ä¹ çš„ç›®æ ‡æ˜¯å¾®è°ƒç­–ç•¥ï¼Œä½¿å…¶ç”Ÿæˆçš„åˆ†å¸ƒæ›´ç¬¦åˆè¿™ç§äººå·¥åˆæˆçš„åå¥½æ’åºã€‚æˆ‘ä»¬å¯ä»¥ä»ä¸¤ä¸ªç­‰ä»·çš„æ•°å­¦è§†è§’æ¥ç†è§£è¿™ä¸€è¿‡ç¨‹ [1] [2]ï¼š

### 1.1 æ‰“åˆ†å™¨è§†è§’ï¼ˆDiscriminative Viewï¼‰ï¼šæœ€å¤§åŒ–å¾—åˆ†å·®

è¿™æ˜¯æœ€ç›´è§‚çš„æ“ä½œè§†è§’ã€‚æˆ‘ä»¬è®­ç»ƒä¸€ä¸ªæ‰“åˆ†å‡½æ•°ï¼ˆå§‘ä¸”å«åš Reward Modelï¼‰$s_\theta(x, y)$ï¼Œç›®æ ‡æ˜¯è®©å¥½ç»“æœçš„å¾—åˆ†æ˜¾è‘—é«˜äºå·®ç»“æœã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨ *Pairwise Loss* æœ€å°åŒ–æ’åºé”™è¯¯çš„é£é™©ï¼š
$$
\mathcal L = -\log \sigma\big(s_\theta(x,y^+) - s_\theta(x,y^-)\big)
$$


å…¶ä¸­ $\sigma(z) = \frac{1}{1 + e^{-z}}$ æ˜¯ Sigmoid å‡½æ•°ã€‚

- ç›´è§‚çœ‹æ¥ï¼Œè¿™å®é™…ä¸Šæ˜¯åœ¨åšä¸€ä¸ªäºŒåˆ†ç±»ä»»åŠ¡ã€‚æ¨¡å‹è¯•å›¾æœ€å¤§åŒ– $y^+$ å’Œ $y^-$ ä¹‹é—´çš„Marginï¼Œä½¿å¾— $s(y^+) - s(y^-)$ è¶Šå¤§è¶Šå¥½ã€‚

### 1.2 é€‰æ‹©æ¦‚ç‡è§†è§’ï¼ˆProbabilistic Viewï¼‰ï¼šæ½œå˜é‡ä¸å™ªå£°å‡è®¾

è¿™æ˜¯ä¸Šè¿°æŸå¤±å‡½æ•°çš„ç†è®ºæ¥æºã€‚å‡è®¾ï¼Œæˆ‘ä»¬åœ¨åšé€‰æ‹©æ—¶éµå¾ªä¸€ä¸ªå¸¦æœ‰å™ªå£°çš„â€œæ•ˆç”¨æ¨¡å‹â€ï¼ˆUtility Modelï¼‰ï¼š
$$
\text{Utility}(x, y) = s_\theta(x, y) + \epsilon
$$


å…¶ä¸­ $s_\theta$ æ˜¯å¯è§‚æµ‹çš„å¾—åˆ†ï¼Œ$\epsilon$â€‹ æ˜¯ä¸å¯è§‚æµ‹çš„éšæœºå™ªå£°ã€‚åœ¨æˆ‘ä»¬è®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹ä¸­ï¼Œè‚¯å®šå€¾å‘äºè®©æ¨¡å‹é€‰æ‹©æ•ˆç”¨æ›´é«˜çš„é‚£ä¸ªé€‰é¡¹ã€‚è€Œæ­¤æ—¶ï¼Œå¯¹äºä¸¤ä¸ªé€‰é¡¹Aå’ŒBï¼Œ**â€œA æ¯” B å¥½â€çš„æ¦‚ç‡ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯â€œA çš„æ•ˆç”¨ > B çš„æ•ˆç”¨â€çš„æ¦‚ç‡**ï¼š
$$
\Pr(y^+ \succ y^-) = \Pr\big(\text{Utility}(y^+) > \text{Utility}(y^-)\big)
$$


å°†æ•ˆç”¨å…¬å¼ä»£å…¥å¹¶ç§»é¡¹æ•´ç†ï¼Œæˆ‘ä»¬å¾—åˆ°æ ¸å¿ƒç­‰å¼ï¼š
$$
\Pr(y^+ \succ y^-) = \Pr(\underbrace{\epsilon^- - \epsilon^+}_{\text{å™ªå£°å·®}} < \underbrace{s(y^+) - s(y^-)}_{\text{å¾—åˆ†å·®}})
$$


é‚£ä¹ˆï¼Œæˆå¯¹æ¦‚ç‡æ¨¡å‹$\Pr(y^+ \succ y^-)$ åˆ°åº•ç­‰äºä»€ä¹ˆï¼Ÿè¿™å–å†³äºæˆ‘ä»¬å‡è®¾**å™ªå£° $\epsilon$ æœä»ä»€ä¹ˆåˆ†å¸ƒ**ï¼š

- **Bradley-Terry æ¨¡å‹**ï¼ˆä¸»æµï¼‰ï¼šå¦‚æœæˆ‘ä»¬å‡è®¾å™ªå£° $\epsilon$ æœä» Gumbel åˆ†å¸ƒï¼ˆæå€¼åˆ†å¸ƒï¼‰ï¼Œé‚£ä¹ˆä¸¤ä¸ª Gumbel å˜é‡ä¹‹å·®å°±æœä» Logistic åˆ†å¸ƒã€‚æ­¤æ—¶ï¼Œåå¥½æ¦‚ç‡æ°å¥½æ¨å¯¼ä¸º Sigmoid å½¢å¼ï¼š
  $$
  \Pr(y^+ \succ y^-) = \frac{1}{1 + e^{-(s(y^+) - s(y^-))}} = \sigma(\Delta s)
  $$
  

  è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨ RLHF ä¸­æ™®éä½¿ç”¨ Log-Sigmoid æŸå¤±çš„åŸå› ã€‚

- **Thurstone-Mosteller** æ¨¡å‹ï¼šå¦‚æœæˆ‘ä»¬å‡è®¾å™ªå£° $\epsilon$ æœä» é«˜æ–¯åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ï¼Œé‚£ä¹ˆåå¥½æ¦‚ç‡å°†ç”±æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDF, å³ Probitï¼‰ç»™å‡ºã€‚

æˆ‘ä»¬å†æ¥çœ‹ä¸¤ä¸ªç›´è§‚çš„ä¾‹å­ï¼š

1. **ä¾‹å­ Aï¼šç”µå½±æ¨èï¼ˆé LLMï¼‰**ï¼šè¾“å…¥ $x$=ç”¨æˆ·ç”»åƒï¼Œå€™é€‰ $y^+$=ã€Šæ˜Ÿé™…ç©¿è¶Šã€‹ï¼Œ$y^-$=ã€Šç”µå½±Bã€‹ï¼ˆå…è´£å£°æ˜ï¼‰ã€‚å½“å‰æ‰“åˆ† $s(y^+)=2.0, s(y^-)=1.2$ï¼Œé‚£ä¹ˆ
    $\Pr(y^+\succ y^-)=\sigma(0.8)\approx 0.69$ã€‚è®­ç»ƒå°±è®©è¿™ä¸ªæ¦‚ç‡æ›´æ¥è¿‘ 1ï¼šè¦ä¹ˆæ‹‰é«˜ $s(y^+)$ï¼Œè¦ä¹ˆå‹ä½ $s(y^-)$ï¼Œæˆ–åšåˆ°ä¸¤è€…åŒæ—¶ã€‚
2. **ä¾‹å­ Bï¼šLLM å›ç­”åå¥½**ï¼š è¾“å…¥ $x$=â€œè¯·ç»™ä¸€ä¸ªèƒ½ç›´æ¥è¿è¡Œçš„ Python ä¾‹å­å¹¶è§£é‡Šå¤æ‚åº¦â€ã€‚ $y^+$=ç»“æ„æ¸…æ™°ã€å¯è¿è¡Œã€è§£é‡Šåˆ°ä½ï¼›$y^-$=è§£é‡Šå«ç³Šã€ä»£ç è·‘ä¸é€šã€‚æˆ‘ä»¬å¸Œæœ›ç­–ç•¥ $\pi_\theta$ç”Ÿæˆ $y^+$ çš„å¯¹æ•°å‡ ç‡æ›´å¤§, å³ï¼š

$$
\log \pi_\theta(y^+|x) - \log \pi_\theta(y^-|x) \uparrow
$$

â€‹        

 	è¿™å°±æ˜¯ DPO çš„æ ¸å¿ƒæ¨åŠ¨åŠ›ã€‚è¿™ç§æ–¹æ³•åŸæ¥ä¹Ÿè¢«æ¨èç³»ç»Ÿå¼•ç”¨ã€‚

---

## 2. é’ˆå¯¹äºLLM çš„åå¥½å’Œå¯¹é½æ–¹æ³•

- **RMï¼ˆReward Modelï¼‰pairwise è®­ç»ƒ**ï¼šè¿™ç§ä¸»è¦ç”¨äºå£°ç§°æ›¾RLHFä½¿ç”¨çš„Reward Model[7],å³ BT logistic æ‹Ÿåˆâ€œ$y^+$ ä¼˜äº $y^-$â€å†åˆ©ç”¨è¯¥è®­ç»ƒçš„RMè¿›è¡ŒPPOç­‰å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒã€‚
- **DPO / IPO / ORPO / KTOï¼ˆåå¥½åˆ°ç›‘ç£ï¼‰**ï¼šç»•å¼€æ˜¾å¼ RM è®­ç»ƒï¼Œç›´æ¥ç”¨ pairwise åå¥½æ›´æ–°æ¨¡å‹çš„ç­–ç•¥åˆ†å¸ƒï¼›
  - **DPO**ï¼šæœ€å¸¸ç”¨ï¼Œç¨³å®šã€å®ç°ç®€å•ï¼›
  - **IPO/KTO/ORPO**ï¼šå¯¹æŸå¤±å½¢å¼ã€é²æ£’æ€§ã€å™ªå£°å»ºæ¨¡ç­‰åšä¸åŒçš„å–èˆã€‚

---

## 3. ã€é‡ç‚¹ã€‘ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰

### 3.1 DPOç®€ä»‹

åœ¨æ¨¡å‹çš„å¯¹é½ï¼ˆAlignment, å°±æ˜¯è®©æ¨¡å‹è¾“å‡ºæ›´åŠ ç¬¦åˆæˆ‘ä»¬çš„åå¥½ï¼‰è®­ç»ƒé‡Œï¼Œæˆ‘ä»¬ä¹‹å‰æåˆ°ï¼Œæˆ‘ä»¬å¸¸æœ‰â€œæˆå¯¹åå¥½â€æ•°æ®å‚ä¸è®­ç»ƒï¼šåŒä¸€è¾“å…¥ $x$ ä¸‹ï¼Œæœ‰ä¸¤æ®µå›ç­” $y^+$ï¼ˆæ›´å¥½ï¼‰å’Œ $y^-$ï¼ˆæ›´å·®ï¼‰ã€‚å®ƒçš„**ç›®æ ‡**æ˜¯ï¼šè®©ç­–ç•¥ $\pi_\theta$ æ›´å€¾å‘äº§ç”Ÿ $y^+$ è€Œä¸æ˜¯ $y^-$ï¼Œä¸”åˆä¸è¦**åç¦»å‚è€ƒç­–ç•¥**ï¼ˆé€šå¸¸æ˜¯SFTæ¨¡å‹ï¼‰å¤ªè¿œã€‚ä¼ ç»Ÿ RLHFçš„åšæ³•æ˜¯ï¼Œå…ˆè®­å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰æ‹Ÿåˆåå¥½ï¼Œ å†ç”¨ PPO ç­‰ RL æ–¹æ³•æ¥ä¼˜åŒ–ç­–ç•¥ + KL æ­£åˆ™é˜²æ­¢å‚æ•°å¤ªè¿‡åç¦»å‚è€ƒæ¨¡å‹ã€‚

**DPO**ï¼ˆDirect Preference Optimizationï¼‰åˆ™è·³è¿‡æ˜¾å¼ RM çš„è®­ç»ƒï¼Œç›´æ¥æŠŠåå¥½çº¦æŸå†™æˆä¸€ä¸ª**ç›‘ç£å¼ï¼ç›®æ ‡**æ¥ä¼˜åŒ–ç­–ç•¥ï¼Œè®­ç»ƒæ›´ç¨³å®šã€ä¾¿å®œï¼ˆç›¸å¯¹äºPPOè¿™ä¸€ç±»çš„ç®—æ³•æ¥è¯´ï¼‰ã€æ˜“è½åœ°ã€‚æ¢ä¸ªè¯´æ³•å°±æ˜¯ï¼ŒDPO å‚è€ƒäº†è®­ç»ƒRMçš„èŒƒå¼ï¼Œé€šè¿‡æ•°å­¦å˜æ¢æŠŠâ€œ**å¥–åŠ±å·®**â€ç”¨**å‚è€ƒç­–ç•¥çš„å¯¹æ•°å‡ ç‡å·®**æ›¿ä»£ï¼Œå¾—åˆ°ä¸€ä¸ªå¯¹**åå¥½å¯¹**çš„**äºŒå…ƒé€»è¾‘å›å½’**æŸå¤±ã€‚äºæ˜¯æˆ‘ä»¬æ— éœ€æ˜¾å¼çš„RMï¼Œå¯ä»¥ç›´æ¥å¯¹ç­–ç•¥åš***pairwise***ç›‘ç£ä¼˜åŒ–ã€‚é’ˆå¯¹ä¸€ä¸ªæ ·æœ¬å¯¹ $(x, y^+, y^-)$ï¼ŒDPO çš„ loss å¸¸å†™ä½œï¼š
$$
\mathcal{L}_{\text{DPO}}(\theta) = -\log \sigma\!\big(
\beta\,[\,\underbrace{\log \pi_\theta(y^+|x)-\log \pi_\theta(y^-|x)}_{\text{ç­–ç•¥å¯¹æ•°å‡ ç‡å·®}}
-\underbrace{\log \pi_{\text{ref}}(y^+|x)+\log \pi_{\text{ref}}(y^-|x)}_{\text{å‚è€ƒæ ¡æ­£/éšå¼KL}}]\big).
$$

- $\sigma(\cdot)$ æ˜¯ sigmoid å‡½æ•°ï¼ŒæŠŠå®æ•°æ˜ å°„ä¸ºâ€œ$y^+$ èƒœå‡ºâ€çš„æ¦‚ç‡ã€‚
- $\pi_{\text{ref}}$ï¼šå‚è€ƒç­–ç•¥ï¼ˆé€šå¸¸æ˜¯ Day 2 çš„ SFT æ¨¡å‹ï¼Œ**å†»ç»“ä¸å‚ä¸è®­ç»ƒ**ï¼‰ã€‚
- $\beta>0$ï¼š**KL æ•£åº¦**çš„æ¸©åº¦ç³»æ•°ã€‚è™½ç„¶DPOé‡Œæ²¡æœ‰RMï¼Œä½†æ˜¯è¿™ä¸ªæ¸©æ§ç³»æ•°å…¶å®èµ·åˆ°äº†ä¸€å®šæ§åˆ¶éšå¼RMçš„ä½œç”¨ï¼ˆæˆ‘ä»¬ä¹‹å‰ä¹Ÿè¯´è¿‡ï¼ŒDPOæ˜¯å€Ÿé‰´äº†RMçš„è®­ç»ƒèŒƒå¼çš„ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼š
  - **$\beta$ å¤§**ï¼šå¼ºçº¦æŸ $\to$ æ¨¡å‹ç´§è´´ SFT åˆ†å¸ƒ $\to$ æŠ‘åˆ¶ Reward Hackingï¼ˆå¦‚å•°å—¦ã€å¥‡æ€ªçš„æ ¼å¼ï¼‰ã€‚
  - **$\beta$ å°**ï¼šå¼±çº¦æŸ $\to$ æ¨¡å‹è¿½æ±‚é«˜ Reward ï¼ˆé«˜åå¥½ï¼‰$\to$ æ¨¡å‹å¯èƒ½ä¼šé’»æ¼æ´ï¼ˆå¯èƒ½å¯¼è‡´æ¨¡å¼åå¡Œã€è¾“å‡ºå•è°ƒå’ŒæåŒ–ã€æˆ–åˆ©ç”¨ RM æ¼æ´ï¼‰ã€‚


ç›´è§‚ç†è§£ï¼š

- å¦‚æœç­–ç•¥å¯¹ $y^+$ çš„å¯¹æ•°æ¦‚ç‡è¿œé«˜äº $y^-$ï¼ˆä¸”ä¸åç¦»å‚è€ƒå¤ªå¤šï¼‰ï¼Œloss ä¼šå¾ˆå°ï¼›
- è‹¥ç­–ç•¥å¯¹ $y^-$ æ›´â€œåçˆ±â€ï¼Œæˆ–ç›¸å¯¹å‚è€ƒåå·®è¿‡å¤§ï¼Œloss ä¼šå¢å¤§ï¼Œæ¨åŠ¨æ›´æ–°ã€‚



### 3.2 DPO æ·±åº¦ç†è§£ï¼Œä»¥åŠå…¬å¼æ¨å¯¼

#### 3.2.1 å‰ç½®æ¦‚å¿µï¼šKLæ•£åº¦

**KL æ•£åº¦ï¼ˆKullbackâ€“Leibler divergenceï¼‰** ç”¨æ¥è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒ $p,q$ çš„å·®å¼‚ï¼š
$$
D_{\mathrm{KL}}(p\Vert q)=\sum_{y} p(y)\,\log\frac{p(y)}{q(y)} \quad(\text{æˆ– } \mathbb{E}_{y\sim p}[\log p(y)-\log q(y)])
$$



ä»–è¡¨ç¤º $p$ åˆ†å¸ƒå¯¹äº $q$ åˆ†å¸ƒçš„ç›¸ä¼¼ç¨‹åº¦:

- $D_{\mathrm{KL}}\ge 0$ï¼ˆæ ¹æ® Jensen ä¸ç­‰å¼ï¼‰ï¼Œä¸”å½“ä¸”ä»…å½“ $p=q$ æ—¶ä¸º 0ï¼Œ è¶Šæ¥è¿‘0ï¼Œè¡¨æ˜ä»–ä»¬è¶Šç›¸ä¼¼ã€‚
- å®ƒæ˜¯**æœ‰æ–¹å‘çš„**ï¼Œä¸å¯¹ç§°çš„ï¼Œå³ï¼š$D_{\mathrm{KL}}(p\|q)\neq D_{\mathrm{KL}}(q\|p)$ã€‚
- KL å¸¸ç”¨æ¥æŠŠæ¨¡å‹**â€œæ‹½å›å»â€**ï¼šä¸è®©æ–°ç­–ç•¥ $\pi$ åç¦» â€œå‚è€ƒç­–ç•¥â€ $\pi_{\text{ref}}$ å¤ªè¿œï¼ˆæ¯”å¦‚ SFT æ¨¡å‹ï¼‰ï¼Œé¿å…è¾“å‡ºé£æ ¼å·®å¼‚å¤ªå¤§ã€‚

åœ¨RLHFï¼ˆä¸‹ä¸€èŠ‚å¼€å§‹ä»‹ç»ï¼‰ä¸­ï¼ŒKLæ•£åº¦å¸¸è¢«ç”¨äºæ­£åˆ™åŒ–ç­–ç•¥ä¸Šã€‚ä¾‹å¦‚ï¼Œåœ¨æ¯ä¸ªè¾“å…¥ $x$ ä¸Šï¼Œæˆ‘ä»¬å¸Œæœ›ç­–ç•¥ $\pi(\cdot|x)$ **æ—¢èƒ½æ‹¿åˆ°é«˜â€œå¥–åŠ±â€ $r(x,y)$ï¼Œåˆåˆ«ç¦»å‚è€ƒ $\pi_{\text{ref}}(\cdot|x)$ å¤ªè¿œ**ã€‚ä¸€ä¸ªç»å…¸çš„ä¸€æ­¥å¼ç›®æ ‡æ˜¯ï¼ˆç±»ä¼¼PPO-KLèŒƒå¼ï¼‰ï¼š

$$
\max_{\pi(\cdot|x)}\;\; \mathbb{E}_{y\sim \pi(\cdot|x)}[\,r(x,y)\,]\;-\;\beta\;D_{\mathrm{KL}}\!\left(\pi(\cdot|x)\,\big\Vert\,\pi_{\text{ref}}(\cdot|x)\right) = \min_{\pi(\cdot|x)}\;\; \beta\;D_{\mathrm{KL}}\!\left(\pi(\cdot|x)\,\big\Vert\,\pi_{\text{ref}}(\cdot|x)-\mathbb{E}_{y\sim \pi(\cdot|x)}[\,r(x,y)\,]\;\;)\right.
\tag{1}
$$



>DPO è®ºæ–‡ä¸­å†™é“ [3]ï¼š During the RL phase, the learned reward function is used to provide feedback to the language model. Following prior works [17, 18], the optimization is formulated as (1)

è¿™é‡Œ $\beta>0$ çš„ä½œç”¨ä¸ä¹‹å‰ä»‹ç»çš„ DPO loss ä¸­çš„æ¸©åº¦ç³»æ•°ä½œç”¨ä¸€è‡´ã€‚æœ€åï¼Œå¯¹äºLLMä¸­ä½¿ç”¨KLæ•£åº¦ä½¿è®­ç»ƒæ¨¡å‹ä¸åç¦»å‚è€ƒæ¨¡å‹ï¼Œæˆ‘ä»¬ç›´æ¥ç»™å‡ºå…¬å¼ã€‚å¯¹ç»™å®šçš„ $x$ï¼Œ

$$
D_{\mathrm{KL}}\!\big(\pi(\cdot|x)\,\|\,\pi_{\rm ref}(\cdot|x)\big)
=\mathbb{E}_{y\sim \pi(\cdot|x)}
\left[\log \frac{\pi(y|x)}{\pi_{\rm ref}(y|x)}\right].
$$

#### 3.3.2 å‰ç½®æ¦‚å¿µï¼šBTåå¥½æ•°æ®æ¨¡å‹

ç„¶è€Œåœ¨ç°å®é‡Œï¼Œæˆ‘ä»¬å¾ˆéš¾æœ‰æ˜¾å¼ä¸”ç›´è§‚çš„å¥–åŠ±åˆ†æ•° $r(x,y)$ ï¼ˆæ‰€è°“æ–‡æ— ç¬¬ä¸€ï¼Œæ­¦æ— ç¬¬äºŒï¼Ÿï¼‰ï¼Œä½†ç¡®å®æœ‰å¯¹äºæŸä¸ªè¾“å…¥å›åº”çš„**åå¥½å¯¹æ•°æ®** $(x,y^+,y^-)$ï¼Œå³å¯¹åŒä¸€ $x$ï¼Œæˆ‘ä»¬è®¤ä¸ºâ€œ$y^+$ èƒœè¿‡ $y^-$â€ã€‚è¿™æ—¶å€™å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæˆ‘ä»¬ä¹‹å‰æåˆ°è¿‡çš„æ¨¡å‹â€”â€”***Bradleyâ€“Terry Model*** [1]ï¼Œè¯¥å…¬å¼åœ¨ DPO åŸæ–‡æœ‰è¯¦ç»†è®²è§£ [3]:

$$
\Pr(y^+ \succ y^- \mid x)\;=\;\sigma\!\big(r(x,y^+)-r(x,y^-)\big),
$$

è¿™æ˜¯è¯´ï¼Œæˆ‘ä»¬è§„å®šï¼š

- æ¯ä¸ªå€™é€‰è¾“å‡º $y$ æœ‰ä¸€ä¸ªâ€œæ•ˆç”¨/å¾—åˆ†â€ $r(x,y)$ã€‚
- å¦‚æœä¸¤ä¸ªå€™é€‰ $y^+, y^-$ æ‹¿æ¥æ¯”è¾ƒï¼Œæˆ‘ä»¬å‡è®¾å®ƒä»¬çš„æ•ˆç”¨å·® $r(x,y^+) - r(x,y^-)$ å†³å®šäº†å“ªä¸€ä¸ªæ›´å¥½ã€‚
- ä¸ºäº†æŠŠå·®å€¼æ˜ å°„åˆ° $[0,1]$ ä¹‹é—´çš„æ¦‚ç‡ï¼Œç”¨ sigmoid å‡½æ•°æŠŠä»–ä»¬æ•ˆç”¨å·®å˜ä¸ºæ¦‚ç‡å€¼ã€‚

å¦‚æœæˆ‘ä»¬å°†å…¶ä¸ SFT ä¸­ç†Ÿæ‚‰çš„â€œäº¤å‰ç†µâ€è”ç³»èµ·æ¥ï¼Œåå¥½å­¦ä¹ çš„æŸå¤±å‡½æ•°å¯ä»¥è¢«èµ‹äºˆæ›´ä¸¥è°¨çš„ç»Ÿè®¡å­¦è§£é‡Šï¼š

1. æœ€å¤§ä¼¼ç„¶ç›®æ ‡ (MLE Objective)

â€‹	æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–è§‚æµ‹æ•°æ®çš„ä¼¼ç„¶æ¦‚ç‡ã€‚ç»™å®šæ•°æ®é›†ä¸­çš„åå¥½å¯¹ $(x, y^+, y^-)$ï¼Œæˆ‘ä»¬å¸Œæœ›å‚æ•° $\theta$ èƒ½å¤Ÿæœ€å¤§åŒ–â€œ$y^+$ èƒœè¿‡ $y^-$â€è¿™ä¸€äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚åŸºäº Bradley-Terry æ¨¡å‹å‡è®¾ï¼Œè¯¥ç›®æ ‡å‡½æ•°ä¸ºï¼š
$$
\max_\theta \mathbb{E}_{(x, y^+, y^-) \sim \mathcal{D}} \Big[ \log \Pr_\theta(y^+ \succ y^- \mid x) \Big] = \max_\theta \mathbb{E} \Big[ \log \sigma\big(r_\theta(x,y^+) - r_\theta(x,y^-)\big) \Big]
$$


2. è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (NLL Loss)

â€‹	åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†â€œæœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶â€è½¬åŒ–ä¸ºâ€œæœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNegative Log-Likelihoodï¼‰â€ã€‚è¿™ç›´æ¥å¯¼å‡ºäº†æ ‡å‡†çš„ Pairwise Ranking Lossï¼š
$$
\mathcal{L}(\theta) = -\log \sigma\big(r_\theta(x,y^+) - r_\theta(x,y^-)\big)
$$


3. ä¸äºŒå…ƒäº¤å‰ç†µçš„ç­‰ä»·æ€§ (Equivalence to BCE)

â€‹	è¿™æ˜¯æˆ‘ä»¬å°†è§†è§’ç»Ÿä¸€ã€‚ä¸Šè¿°æŸå¤±å‡½æ•°å®é™…ä¸Šå°±æ˜¯ **äºŒå…ƒäº¤å‰ç†µï¼ˆBinary Cross-Entropy, BCEï¼‰** çš„ä¸€ç§ç‰¹ä¾‹ã€‚æˆ‘ä»¬å¯ä»¥å°†åå¥½é¢„æµ‹è§†ä¸ºä¸€ä¸ª**äºŒåˆ†ç±»é—®é¢˜**ï¼š

- **è¾“å…¥**ï¼šä¸€å¯¹æ ·æœ¬ $(y^+, y^-)$ çš„åˆ†å·® $\Delta r$ã€‚
- **é¢„æµ‹**ï¼šæ¨¡å‹ç»™å‡ºçš„èƒœç‡æ¦‚ç‡ $\hat{p} = \sigma(\Delta r)$ã€‚
- **æ ‡ç­¾**ï¼šçœŸå®è§‚æµ‹ç»“æœ $y_{\text{label}} = 1$ï¼ˆå³ $y^+$ ç¡®å®èµ¢äº†ï¼‰ã€‚

â€‹	å›é¡¾é€šç”¨çš„ BCE å…¬å¼ï¼š
$$
\mathcal{L}_{\text{BCE}}(y, p) = -\big[ y \cdot \log p + (1-y) \cdot \log(1-p) \big]
$$


å½“æˆ‘ä»¬å°†æ ‡ç­¾ $y_{\text{label}} = 1$ ä»£å…¥æ—¶ï¼Œå³è¾¹çš„é¡¹ $(1-1)$ æ¶ˆå¤±ï¼Œå…¬å¼åç¼©ä¸ºï¼š
$$
\mathcal{L}_{\text{BCE}}(1, \hat{p}) = -1 \cdot \log \hat{p} = -\log \sigma(\Delta r)
$$


#### 3.3.3 DPO loss å…¬å¼æ¨å¯¼

æˆ‘ä»¬æ¥ä¸‹æ¥æ­£å¼è¿›è¡Œæ¨å¯¼ã€‚ä»åŸæ–‡ä¸­çœ‹ï¼Œä½œè€…ä»å¼ºåŒ–å­¦ä¹ å¾®è°ƒé˜¶æ®µçš„ç›®æ ‡å‡½æ•°ï¼ˆ1ï¼‰å¼ä¸­ï¼Œåˆ†æå¾—å‡ºåœ¨ KL çº¦æŸæ¡ä»¶ä¸‹ï¼Œæœ€å¤§åŒ–å¥–åŠ±çš„**æœ€ä¼˜ç­–ç•¥å½¢çŠ¶**ï¼ˆå¯¹æ¯ä¸ªå›ºå®šçš„ $x$ï¼‰ï¼š

$$
\pi_\tau(y\mid x)=\frac{1}{Z(x)}\,\pi_{\text{ref}}(y\mid x)\,\exp\!\Big(\tfrac{1}{\beta} \, r(x,y)\Big),
\tag{2}
$$

å…¶ä¸­ $Z(x)=\sum_y \pi_{\text{ref}}(y\mid x)\exp(\tfrac{1}{\beta}r(x,y))$ æ˜¯é…åˆ†å‡½æ•°(partition function)ï¼Œ è¯¥é¡¹åªéš $x$ å˜ï¼Œä¸éš $y$ å˜ï¼Œæ˜¯å½’ä¸€åŒ–å‡½æ•°ã€‚ä¸ºäº†è®©ä¹˜æ³•å˜æˆåŠ æ³•ï¼Œä½¿å¾—å®¹æ˜“åš**å·®åˆ†**æ¶ˆå»é…åˆ†å‡½æ•°åŒæ—¶æ›´åŠ ç¨³å®šï¼Œä¸¤è¾¹å–å¯¹æ•°å¹¶ç§»é¡¹ï¼Œå¾—åˆ°ï¼š
$$
r(x,y) = \beta \log \frac{\pi_\tau(y\mid x)}{\pi_{\text{ref}}(y\mid x)} + \beta \log Z(x).
\tag{3}
$$
æ³¨æ„ $\log Z(x)$ **åªä¾èµ–äº $x$**ï¼Œä¸ $y$â€‹ æ— å…³ã€‚ç„¶åï¼ŒBradleyâ€“Terryï¼ˆBTï¼‰å‡è®¾å¯¹åŒä¸€ $x$ çš„ä¸¤æ®µè¾“å‡º $y_1,y_2$ï¼š
$$
p^*(y_1 \succ y_2 \mid x) \;=\; \sigma\!\big(r^*(x,y_1) - r^*(x,y_2)\big).
\tag{4}
$$



æŠŠ (3) ä»£å…¥åˆ°â€œå¥–åŠ±å·®â€é‡Œï¼ˆ$r^*$ ç”¨æœ€ä¼˜ç­–ç•¥ $\pi^*$ çš„è¡¨è¾¾ï¼‰ï¼Œ$\beta\log Z(x)$ åœ¨åšå·®æ—¶**æŠµæ¶ˆ**ï¼Œå¾—åˆ°ï¼ˆç”±æ­¤å¯çŸ¥é…åˆ†å‡½æ•°è¢«æ¶ˆé™¤åå¯¹æ¢¯åº¦ä¹Ÿæ²¡æœ‰å½±å“ï¼‰ï¼š
$$
p^*(y_1 \succ y_2 \mid x) = \sigma\!\Big(
\beta \log \frac{\pi^*(y_1\mid x)}{\pi_{\text{ref}}(y_1\mid x)}
-\beta \log \frac{\pi^*(y_2\mid x)}{\pi_{\text{ref}}(y_2\mid x)}
\Big).
\tag{5}
$$

æŠŠ $\sigma(z)=1/(1+e^{-z})$ å±•å¼€ï¼Œå°±å¾—åˆ°åŸæ–‡ç­‰ä»·å†™æ³•ï¼š

$$
p^*(y_1 \succ y_2 \mid x)=
\frac{1}{1+\exp\!\Big(\beta \log \frac{\pi^*(y_2\mid x)}{\pi_{\text{ref}}(y_2\mid x)}
-\beta \log \frac{\pi^*(y_1\mid x)}{\pi_{\text{ref}}(y_1\mid x)}\Big)}.
\tag{6}
$$

ç”¨ $\pi_\theta$ è¿‘ä¼¼ $\pi^*$ï¼Œæå–$\beta$ï¼Œå†å¯¹æ¯ä¸ªåå¥½å¯¹ $(x,y_w,y_l)$ çš„â€œæ ‡ç­¾=1ï¼ˆ$y_w$ èµ¢ï¼‰â€åšä¼¯åŠªåˆ©æå¤§ä¼¼ç„¶ï¼š

$$
\max_\theta\;\;\log\sigma\!\Big(\beta\big[\underbrace{\log\tfrac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)}
-\log\tfrac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}}_{\text{è¾¹é™…}}\big]\Big).
$$

å–è´Ÿå·ï¼Œå˜ä¸ºæ±‚minçš„æœ€å°å€¼ï¼Œå°±æ˜¯è®­ç»ƒæ—¶ç”¨çš„ **DPO æŸå¤±**ï¼š

$$
\mathcal L_{\text{DPO}}(\theta;\pi_{\text{ref}})
= -\,
\Big[\log \sigma\!\Big(
\beta \big(\underbrace{\log \pi_\theta(y_w\mid x)-\log \pi_\theta(y_l\mid x)}_{\text{ç­–ç•¥å¯¹æ•°å‡ ç‡å·®}}
-\underbrace{\log \pi_{\text{ref}}(y_w\mid x)+\log \pi_{\text{ref}}(y_l\mid x)}_{\text{å‚è€ƒæ ¡æ­£}}\big)
\Big)\Big].
$$

Againï¼Œ**$\beta$ å¤§ï¼šæ›´è´´è¿‘å‚è€ƒï¼›å°ï¼šæ›´æ•¢æ”¹å˜ï¼Œè¿œç¦»å‚è€ƒ**ã€‚æ€»ç»“ï¼ŒDPOçš„æ¢¯åº¦æ›´æ–°æ—¨åœ¨**å¢åŠ **ä¼˜è´¨å›ç­”çš„æ¦‚ç‡ï¼ŒåŒæ—¶**å‡å°‘**åŠ£è´¨å›ç­”çš„æ¦‚ç‡ã€‚

#### 3.3.4 å’ŒSFT/RLHFçš„å·®å¼‚

| æ–¹æ³•           | æ•°æ®éœ€æ±‚                       | æ˜¯å¦æ˜¾å¼RM | æ˜¯å¦ç¯å¢ƒäº¤äº’ | ä¼˜ç‚¹                             | é£é™©/å±€é™                         |
| -------------- | ------------------------------ | ---------- | ------------ | -------------------------------- | --------------------------------- |
| **SFT**        | å¸¦â€œå¥½ç¤ºä¾‹â€çš„ç›‘ç£æ•°æ®           | å¦         | å¦           | ç¨³å®šã€ä¾¿å®œ                       | åªèƒ½â€œå­¦åƒâ€ï¼Œä¸èƒ½â€œå­¦åå¥½â€          |
| **DPO**        | æˆå¯¹åå¥½ï¼ˆchosen vs rejectedï¼‰ | å¦         | å¦           | ç›´æ¥ä¼˜åŒ–åå¥½ï¼Œè®­ç»ƒç®€å•ç¨³å®š       | ä¾èµ–åå¥½è¦†ç›–é¢ï¼›æ˜“å‡ºç°â€œå˜é•¿/å•°å—¦â€ |
| **PPO (RLHF)** | åå¥½â†’RMâ†’å¥–åŠ±                   | æ˜¯         | æ˜¯           | èƒ½ç›´æ¥æœ€å¤§åŒ–å¥–åŠ±ï¼ˆå¯è‡ªå®šä¹‰æŒ‡æ ‡ï¼‰ | å·¥ç¨‹å¤æ‚ã€æ˜‚è´µã€æ˜“ä¸ç¨³            |
| **GRPO/GSPO**  | ç»„å†…å¤šæ ·æœ¬ç›¸å¯¹åå¥½             | å¦         | å¯æ—          | æŠ˜ä¸­ç¨³å®šï¼Œå¼±åŒ–å¯¹RMä¾èµ–           | éœ€å¤šæ ·æœ¬é‡‡æ ·ï¼Œä»æœ‰å®ç°ç»†èŠ‚        |

---

## 4. DPO è®­ç»ƒ

æˆ‘ä»¬ä½¿ç”¨`torch`å’Œ`transformer`ä½œäº†æœ€å°å¯æ‰§è¡Œçš„DPOç®—æ³•ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬åˆ—å‡ºlossè®¡ç®—çš„æµç¨‹ã€‚å¯¹äºæµ‹è¯•é˜¶æ®µï¼Œæˆ‘ä»¬çš„ä¿„æ•°æ®é›†éœ€è¦åŒ…å«æ­£å‘åå¥½å’Œè´Ÿå‘åå¥½ï¼Œç±»ä¼¼äºï¼š

```python
{"prompt": "What is the capital of France?", 
 "chosen": "The capital of France is Paris. Paris is located in the north-central part of             the country and is known for its rich history, culture, and landmarks like the             Eiffel Tower.", 
 "rejected": "I don't know the capital of France."}
```

ç®€æ´åœ°ï¼Œdpoçš„losså¯ä»¥è¢«å†™æˆï¼š

```python
"""
Compute DPO loss.

Args:
    pol_pos_lp: Policy model log prob for positive examples
    pol_neg_lp: Policy model log prob for negative examples
    ref_pos_lp: Reference model log prob for positive examples
    ref_neg_lp: Reference model log prob for negative examples
    beta: DPO temperature parameter
"""
# Î” = (logÏ€Î¸(y+)-logÏ€Î¸(y-)) - (logÏ€ref(y+)-logÏ€ref(y-))
delta = (pol_pos_lp - pol_neg_lp) - (ref_pos_lp - ref_neg_lp)
# -log Ïƒ(Î²Î”) = softplus(-Î²Î”)
return F.softplus(-beta * delta).mean()
```

è¿™é‡Œä½¿ç”¨äº†[softplus](https://docs.pytorch.org/docs/stable/generated/torch.nn.Softplus.html)å‡½æ•°ï¼Œå› ä¸ºï¼š

$$
-\log \sigma(z) = \log(1+e^{-z}) = \text{softplus}(-z).\\
$$

è¿™ç§å†™æ³•åœ¨æ•°å€¼ä¸Šæ¯” `-torch.logsigmoid(z)` æ›´ç¨³å®šï¼Œå°¤å…¶æ˜¯å½“ $\beta\Delta$ å–æå¤§/æå°å€¼æ—¶ï¼Œèƒ½é¿å…æº¢å‡ºã€‚ç„¶åï¼Œè®­ç»ƒæ—¶æˆ‘ä»¬é€šå¸¸ä¸€ä¸ª batch æœ‰å¤šæ¡æ ·æœ¬ï¼š$\mathcal L = \frac{1}{N} \sum_{i=1}^N -\log \sigma(\beta \Delta_i).$ æ‰€ä»¥ `mean()` å°±æ˜¯åš **batch å¹³å‡**ã€‚å¦‚æœä¸å– meanï¼Œ ä¼šåªè¿”å›ä¸€ä¸ªå‘é‡ï¼Œæ¢¯åº¦å°±ä¼šæŒ‰æ ·æœ¬æ•°æ”¾å¤§ï¼Œè®­ç»ƒä¸ç¨³å®šã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬æ¥walkthroughè®­ç»ƒè¿‡ç¨‹ï¼Œè¿™è½®è®­ç»ƒçš„è¾“å…¥ä¸ç›®æ ‡ï¼š

- æ¯ä¸ª batch å«æˆå¯¹æ ·æœ¬ï¼š`(x, y+ , y-)`ã€‚
- æˆ‘ä»¬è¦è®©**ç­–ç•¥**ç›¸å¯¹**å‚è€ƒ**æ›´åçˆ± `y+`ï¼ˆchosenï¼‰è€Œä¸æ˜¯ `y-`ï¼ˆrejectedï¼‰ã€‚
- æŸå¤±ï¼š$\mathcal L_{\text{DPO}}=-\log \sigma(\beta\Delta)$

**1ï¼‰å– batch **

```python
pos_ids = batch.pos_input_ids.to(device) # prompt + response(target)åœ¨è¯è¡¨ä¸­çš„token id 
neg_ids = batch.neg_input_ids.to(device)
pos_mask_tok = batch.pos_mask.to(device) # label mask
neg_mask_tok = batch.neg_mask.to(device)
```

- `pos_ids/neg_ids`ï¼šæŠŠ **prompt+response** æ‹¼å¥½çš„ token åºåˆ—ï¼ˆè¦å’ŒSFTçš„tokenizerä¸€è‡´ï¼‰ã€‚
- `pos_mask_tok/neg_mask_tok`ï¼š**æ ‡ç­¾æ©ç ï¼ˆlabel maskï¼‰**ï¼Œåœ¨ **response æ®µ**ä¸º 1ï¼Œprompt æ®µå’Œ padding ä¸º 0ã€‚
  - ä½œç”¨ï¼šåé¢ç®—åºåˆ—å¯¹æ•°ä¼¼ç„¶æ—¶ï¼Œåªç´¯è®¡ **response token** çš„ logprobï¼ˆè¿™ä¸ SFT çš„ â€œassistant maskâ€ ä¸€è‡´ï¼‰ã€‚
  - è¿™æ˜¯ **DPO çš„å…³é”®**ï¼šæˆ‘ä»¬æ¯”è¾ƒçš„æ˜¯â€œåŒä¸€ prompt ä¸‹ï¼Œä¸¤æ®µå®Œæ•´å›ç­”çš„åºåˆ—å¯¹æ•°ä¼¼ç„¶å·®â€ï¼Œå› æ­¤è¦æŠŠ prompt çš„ token æ’é™¤åœ¨å¤–ã€‚åœ¨å…·ä½“åšæ³•å¯ä»¥è§ä»£ç ï¼Œæˆ–è¿™çœ‹æ¥ä¸‹æ¥çš„å®æˆ˜ã€‚

**2) attention maskï¼ˆç»™ Transformer ç”¨ï¼‰**

```python
pos_attn = (pos_ids != tokenizer.pad_token_id).long()
neg_attn = (neg_ids != tokenizer.pad_token_id).long()
```

- **attention mask** å‘Šè¯‰æ¨¡å‹å“ªäº›ä½ç½®æ˜¯ **æœ‰æ•ˆ token**ï¼ˆ1ï¼‰ï¼Œå“ªäº›æ˜¯ **pad**ï¼ˆ0ï¼‰ã€‚å’Œä¸Šé¢çš„ **label mask** ä¸åŒï¼Œè¿™æ˜¯ç»™æ¨¡å‹æ³¨æ„åŠ›ç”¨ï¼Œå±è”½ paddingï¼›**prompt+response éƒ½æ˜¯ 1**ã€‚

**3) autocastï¼ˆæ··åˆç²¾åº¦ï¼‰ä¸ä¸¤å¥—å‰å‘**

```python
# autocastï¼šå¼€å¯ bf16/fp16 æ··åˆç²¾åº¦ï¼Œçœæ˜¾å­˜æé€Ÿã€‚
with torch.amp.autocast(device_type="cuda", dtype=config["dtype"], enabled=True):
    pol_pos_logits = forward_policy(policy, pos_ids, pos_attn)
    pol_neg_logits = forward_policy(policy, neg_ids, neg_attn)

    with torch.no_grad():# å‚è€ƒç­–ç•¥ä¸å‚ä¸åä¼ 
        ref_pos_logits = forward_ref(ref, pos_ids, pos_attn)
        ref_neg_logits = forward_ref(ref, neg_ids, neg_attn)
```

- **policy**ï¼šå¯è®­ç»ƒçš„ç­–ç•¥ï¼›**ref**ï¼š**å†»ç»“**çš„å‚è€ƒç­–ç•¥ï¼ˆä¸€èˆ¬æ˜¯ Day2 çš„ SFT æ¨¡å‹ï¼‰ã€‚
- å¯¹æ¯ä¸ª `(x, y+)` å’Œ `(x, y-)` åˆ†åˆ«èµ°ä¸€æ¬¡å‰å‘ï¼Œå¾—åˆ° `[B, T, V]` çš„ `logits`ã€‚ï¼ˆB-Batch size, T-seq lenï¼ˆæ¯ä¸ªæ ·æœ¬ token åºåˆ—çš„é•¿åº¦ï¼‰ï¼Œ V-vocab sizeï¼ˆæ¨¡å‹è¯è¡¨çš„å¤§å°ï¼‰ï¼‰ã€‚
- æ³¨æ„ï¼ï¼šè™½ç„¶è¿™é‡Œçš„ `pos_ids`/`neg_ids` éƒ½æ˜¯ **[prompt || response]** çš„æ‹¼æ¥ï¼Œä½†è¿™ä¸æ˜¯â€œè®©æ¨¡å‹ç”Ÿæˆâ€ï¼Œè€Œæ˜¯è®©æ¨¡å‹**æ‰“åˆ†**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬éœ€è¦æ¨¡å‹åœ¨**å·²çŸ¥æ•´æ®µç›®æ ‡å“åº”**çš„å‰æä¸‹ï¼Œé€ token è®¡ç®—â€œç”Ÿæˆè¿™ä¸€ token çš„æ¦‚ç‡â€ã€‚è¿™å°±æ˜¯**teacher forcing / è¯„åˆ†æ¨¡å¼**ï¼Œä¸æ˜¯è‡ªå›å½’é‡‡æ ·ã€‚
  - ä¸ºäº†å¾—åˆ°è¿™æ¡åºåˆ—çš„ logprobï¼Œæˆ‘ä»¬å¿…é¡»æŠŠ **prompt ä¸ï¼ˆä½œä¸ºç›‘ç£ç›®æ ‡çš„ï¼‰response** ä¸€èµ·è¾“å…¥ï¼›éšåæ¨¡å‹å‰å‘å¾—åˆ°çš„æ˜¯ `logits`ï¼ˆæœªå½’ä¸€åŒ–åˆ†æ•°ï¼‰ï¼›ï¼ˆè§ä¸‹é¢å‡ æ­¥ï¼‰æˆ‘ä»¬å† `log_softmax` + `gather` + èšåˆï¼Œæ‰å¾—åˆ°â€œè¿™æ¡ response çš„ï¼ˆå¯¹æ•°ï¼‰æ¦‚ç‡â€ã€‚

**4) ä» logits å¾—åˆ°â€œåºåˆ—å¯¹æ•°ä¼¼ç„¶â€**

```python
pol_pos_lp = seq_logprob_from_logits(
    pol_pos_logits[:, :-1], pos_ids[:, 1:], pos_mask_tok[:, 1:], config["length_norm"]
)
...
# seq_logprob_from_logits
def seq_logprob_from_logits(logits: torch.Tensor, labels: torch.Tensor,mask: torch.Tensor, 
    length_norm: bool
) -> torch.Tensor:
    """
    Compute sequence log probabilities from logits.
    
    Args:
        logits: [B, T, V] - model logits
        labels: [B, T] - target token ids
        mask: [B, T] - mask for response tokens only
        length_norm: whether to normalize by length
        
    Returns:
        [B] - sequence log probabilities
    """
    # åœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆè¯è¡¨ç»´åº¦ï¼‰åš softmaxï¼Œå†å–å¯¹æ•° â†’ å¾—åˆ°æ¯ä¸ªä½ç½®å¯¹æ‰€æœ‰ token çš„ log æ¦‚ç‡ã€‚
    log_prob = F.log_softmax(logits, dim=-1)
    # ä»å®Œæ•´çš„ vocab æ¦‚ç‡åˆ†å¸ƒé‡Œï¼Œåªä¿ç•™â€œé¢„æµ‹çœŸå® tokenâ€é‚£ä¸€é¡¹ã€‚
    tok_log_prob = log_prob.gather(-1, labels.unsqueeze(-1)).squeeze(-1)  # [B, T]
    tok_log_prob = tok_log_prob * mask # ç›¸ä¹˜åï¼Œprompt å’Œ pad çš„ log æ¦‚ç‡ä¼šè¢«ç½®é›¶ï¼Œåªä¿ç•™ response éƒ¨åˆ†ã€‚
    
    if length_norm: # æŠŠæ€» logprob é™¤ä»¥é•¿åº¦ âˆ£ğ‘¦âˆ£ï¼Œ å³å–å¹³å‡ï¼š
        denom = mask.sum(-1).clamp_min(1.0)
        return tok_log_prob.sum(-1) / denom
    else:
        return tok_log_prob.sum(-1)
```

è¿™é‡Œæœ‰ä¸‰ä¸ªå…³é”®ç‚¹ï¼š

**(a) â€œshiftâ€ ä¸€æ ¼ï¼ˆnext-token é¢„æµ‹ï¼‰**ï¼Œè¯­è¨€æ¨¡å‹æ˜¯ **é¢„æµ‹ä¸‹ä¸€ä¸ª token**ï¼š

- ç”¨ `logits[:, :-1]` é¢„æµ‹ `labels = input_ids[:, 1:]`ã€‚
- è¿™å°±æ˜¯å¸¸è¯´çš„ **teacher forcing**ï¼ˆå’Œ HuggingFace `labels` ç§»ä½ä¸€è‡´ï¼‰ã€‚

**(b) label mask åªæ•° response token**ï¼Œ`pos_mask_tok[:, 1:]`ï¼š prompt æ®µä¸ pad éƒ½æ˜¯ 0ï¼Œ**response æ®µæ˜¯ 1**ã€‚

**(c) policy ä¸ ref ä¸¤å¥—éƒ½è¦ç®—**

- å¾—åˆ°å››ä¸ªæ ‡é‡å‘é‡ï¼ˆæŒ‰ batchï¼‰ï¼š`pol_pos_lp, pol_neg_lp, ref_pos_lp, ref_neg_lp`ã€‚
- è¿™äº›å°±æ˜¯ $\log\pi_\theta(y^+|x), \log\pi_\theta(y^-|x)$ åŠå¯¹åº”å‚è€ƒé¡¹ã€‚

è¿™é‡Œé¢å¤–è¯´ä¸€ä¸‹ä»¥ä¸‹ä»£ç ï¼š

```python
#                              dim         index
tok_log_prob = log_prob.gather(-1, labels.unsqueeze(-1)).squeeze(-1) 
```

è¿™é‡Œgatherçš„ä½œç”¨æ˜¯æ²¿ç€log_probçš„dimï¼ŒæŒ‰ç…§indexæŠ½å–å‘é‡ã€‚ç»“æœè¿”å›å’Œ `index` çš„å½¢çŠ¶ä¸€æ ·ï¼Œæ¯ä¸ªä½ç½®å­˜çš„æ˜¯ `log_prob` å¯¹åº”ä¸‹æ ‡çš„å€¼ã€‚åœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œlogp` çš„å½¢çŠ¶ `[B, T, V]ï¼Œ labels` çš„å½¢çŠ¶ `[B, T]ï¼Œé‡Œé¢æ˜¯**çœŸå® token çš„ç´¢å¼•**ï¼ˆæ¯ä¸ªä½ç½®ä¸€ä¸ªæ•´æ•°ï¼‰ã€‚æˆ‘ä»¬å›åˆ°å‰é¢çš„æå–çš„éƒ¨åˆ†ï¼Œæå–çš„æ—¶å€™

```python
pol_pos_logits[:, :-1]	 # shape [B, T-1, V]ï¼Œæ‰”æ‰æœ€åä¸€ä¸ªä½ç½®çš„é¢„æµ‹ï¼ˆå› ä¸ºæ²¡æœ‰â€œä¸‹ä¸€ä¸ªtokenâ€å¯ä»¥å¯¹é½ï¼‰ã€‚
pos_ids[:, 1:]           # shape [B, T-1]ï¼Œæ‰”æ‰ç¬¬ä¸€ä¸ªtokenï¼ˆå› ä¸ºç¬¬0ä¸ªlogité¢„æµ‹çš„å°±æ˜¯token1ï¼‰ã€‚
pos_mask_tok[:, 1:]      # shape [B, T-1]ï¼ŒåŒæ ·æ‰”æ‰ç¬¬ä¸€ä¸ªtokençš„maskï¼Œä¿è¯å’Œlabelså¯¹é½ã€‚
```

**5ï¼‰è®¡ç®— DPO çš„ margin ä¸ loss**

```python
# Compute DPO loss
loss = dpo_loss(pol_pos_lp, pol_neg_lp, ref_pos_lp, ref_neg_lp, config["beta"]) / config["grad_accum"]
```

**6ï¼‰ åä¼ ä¸ä¼˜åŒ–**

```python
scaler.scale(loss).backward()
if (batch_idx + 1) % grad_accum == 0:
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad(set_to_none=True)
    scheduler.step()
```

- `GradScaler`ï¼šé…åˆ autocast çš„ **æ··åˆç²¾åº¦**ï¼Œé¿å… fp16 æ¢¯åº¦ä¸‹æº¢ã€‚
- **æ¢¯åº¦ç´¯ç§¯**ï¼šæ¯ `grad_accum` ä¸ªå°æ­¥åšä¸€æ¬¡ä¼˜åŒ–å™¨ stepï¼Œç›¸å½“äºæ›´å¤§çš„æœ‰æ•ˆ batchï¼ˆçœæ˜¾å­˜ï¼‰ã€‚
- `scheduler.step()`ï¼šå­¦ä¹ ç‡è°ƒåº¦ï¼ˆçº¿æ€§ warmup + çº¿æ€§ decay ç­‰ï¼‰ã€‚

åˆ°æ­¤ä¸ºæ­¢ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œç®€å•çš„DPOéƒ¨ç½²äº†ã€‚åœ¨æ¥ä¸‹æ¥çš„å®æˆ˜ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šç”¨åˆ°unslothå’Œtrlçš„å®˜æ–¹åº“ï¼Œè€Œä¸ä¼šä½¿ç”¨è¯¥è‡ªå®šä¹‰çš„DPOã€‚

---

## 5. DPO å®æˆ˜

æœ‰äº†ä¸Šé¢çš„ç»éªŒï¼Œæˆ‘ä»¬é’ˆå¯¹SFTä½¿ç”¨è¿‡çš„Qwen3 4bæ¨¡å‹ç»§ç»­ä½¿ç”¨DPOè¿›è¡Œä¼˜åŒ–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„DPOå’Œå·¥ä¸šåº“è·‘ä¸€éç›¸åŒçš„æ•°æ®é›†ï¼Œä»¥å¯¹æ¯”ä»–ä»¬çš„æƒ…å†µã€‚

### 5.1 æ•°æ®é›†

ç›®æ ‡ç»Ÿä¸€æˆä¸‰åˆ—æ ¼å¼ï¼š

```
prompt | chosen | rejected
```

è¿™é‡Œæ¨èçš„èµ·æ­¥çš„datasetï¼ŒåŒ…å«ä¸­è‹±æ–‡ï¼Œé˜²æ­¢åœ¨SFTä¸­å‡ºç°çš„æ¨¡å‹å¤±è¯­çš„èƒ½åŠ›ï¼š

- UltraFeedback/HelpSteer2ï¼ˆåå¥½æˆ–æ‰“åˆ†å¯è½¬ pairï¼›åªé€‰é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼‰[4]

  - å…¶è¯„ä»·æŒ‡æ ‡å¤šå…ƒï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ç»™å‡ºçš„è¯„ä»·æŒ‡æ ‡**å…ˆåšå½’ä¸€åŒ–**å†**åŠ æƒæ±‚å’Œ**ï¼Œæ¨èæƒé‡ï¼š`correctness 0.4, helpfulness 0.3, coherence 0.2, å‰©ä½™å¤æ‚åº¦å’Œå†—é•¿åº¦å„ä½0.5`ï¼ˆç†ç”±ï¼šå…ˆç¡®ä¿â€œå¯¹â€ï¼Œå†è€ƒè™‘â€œæœ‰ç”¨â€ï¼Œæœ€åâ€œè¿è´¯â€ï¼‰ã€‚

- opencsg/UltraFeedback-chinese [5]ï¼ˆä¸­æ–‡åå¥½æ‰“åˆ†æ•°æ®é›†ï¼Œå«æœ‰Deepseekè¿›è¡Œçš„å¤šå…ƒæ‰“åˆ†ï¼Œå¯ç”Ÿæˆåå¥½å¯¹ï¼‰

  - æœ‰ç›¸ä¼¼é—®é¢˜ã€‚æˆ‘ä»¬å…ˆ**æŒ‰ç›¸åŒè¯„åˆ†è§„åˆ™(æƒé‡åˆ†é…)åšå½’ä¸€åŒ–**ã€å‰”é™¤å¼‚å¸¸çŸ­/æ¨¡æ¿åŒ–å›ç­”ï¼Œå¹¶ç¡®ä¿**chosenâ‰ rejected**ã€‚

    ```python
    annotation_mapping = {
    "helpfulness": "helpfulness",
    "honesty": "correctness",  # Map honesty to correctness
    "instruction_following": "coherence",  # Map instruction to coherence
    "truthfulness": "correctness"  # Map truthfulness to correctness
    }
    ```

> å®æˆ˜å»ºè®®ï¼šå– 30kâ€“50k å¯¹ï¼Œè¦†ç›– 70% è‹±æ–‡ + 30% ä¸­æ–‡ï¼Œå¹¶è·å¾—ç¨³å®šçš„ win-rate ä¿¡å·ï¼ˆéšåå†æ‰©å®¹ä¸æ¸…æ´—è¿­ä»£ï¼‰ã€‚

æˆ‘ä»¬æŒ‰ç…§ä»¥ä¸‹è§„åˆ™sample æ•°æ®é›†çš„å¤§å°ï¼š

```python
    "sampling": {
        "total_samples": 7500,  # Target total samples
        "en_ratio": 0.7,        # English ratio (70%)
        "zh_ratio": 0.3,        # Chinese ratio (30%)
        "en_samples": 5250,     # English samples (70% of 7500)
        "zh_samples": 2250      # Chinese samples (30% of 7500)
    }
```

å…¶ä»–å¯¹æ•°æ®é›†çš„å¤„ç†ï¼Œè¯¦è§`dpo/dataset/data_process.py`ã€‚

### 5.2 ä¸»æµåº“é€‰å–

| æ¶æ„/å·¥å…·                           | ä¸Šæ‰‹éš¾åº¦ | ç®—æ³•è¦†ç›–                  | å·¥ç¨‹ç‰¹æ€§                                   | 4080 å‹å¥½åº¦              | é€‚åˆæœ¬è½®å—ï¼Ÿ               |
| ----------------------------------- | -------- | ------------------------- | ------------------------------------------ | ------------------------ | -------------------------- |
| **TRL (HuggingFace)**               | ä½       | DPO/IPO/ORPO/SimPO/KTO ç­‰ | ä¸ Transformers/PEFT/Accelerate åŸç”ŸååŒå¥½ | é«˜ï¼ˆå•å¡ã€QLoRA éå¸¸ç¨³ï¼‰ | âœ… é¦–é€‰è·‘é€šåŸºçº¿             |
| **LLaMA-Factory**                   | ä½-ä¸­    | DPO/ORPO/KTOâ€¦             | CLI é…ç½®å³è·‘ï¼Œå¤šæ•°æ®æ ¼å¼é€‚é…               | é«˜                       | ä½œä¸ºâ€œå‘½ä»¤è¡Œç‰ˆâ€å¿«é€Ÿå¤ç°     |
| **OpenRLHF / VE**ï¼ˆå·¥ç¨‹åŒ– RL æ¡†æ¶ï¼‰ | ä¸­-é«˜    | DPO/GRPO/PPOâ€¦             | å¤šæœºå¤šå¡ã€æµæ°´çº¿ã€ååå¼º                   | ä¸­ï¼ˆå•å¡ç•¥é‡ï¼‰           | äº†è§£å·¥ç¨‹å½¢æ€ï¼Œç”¨äºæ‰©å±•     |
| **è‡ªç ”è½»æ¡†æ¶ï¼ˆTRL ä¸ŠåŒ…ä¸€å±‚ï¼‰**      | ä¸­       | çµæ´»                      | å®Œå…¨å¯æ§ã€ä¾¿äºåš CL å¾ªç¯æ¥å£               | é«˜                       | âœ… ä¾¿äºåç»­ Day5/6 å¾ªç¯å®éªŒ |

æˆ‘ä»¬é€‰ç”¨TRLæ¡†æ¶æ­é…unsloth(`train_dpo.py`)ï¼Œä¸è‡ªå·±å†™çš„å®¢åˆ¶åŒ–dpoé¡¹ç›®(`mini_dpo.py`), åŒæ—¶è¿›è¡Œæœ¬æ¬¡å®éªŒï¼Œä»¥å¯¹æ¯”æ€§èƒ½ã€‚

### 5.3 æ ¸å¿ƒå‚æ•°

**è®­ç»ƒé…ç½®ï¼ˆä¸ SFT å¯¹é½ï¼‰**

- **batch_size = 1**ï¼šå•å¡æ˜¾å­˜ç´§å¼ æ—¶ç”¨æœ€å°æ‰¹ï¼›é…åˆæ¢¯åº¦ç´¯ç§¯ä¿æŒæœ‰æ•ˆæ‰¹é‡ã€‚
- **grad_accum = 24**ï¼šç´¯ç§¯ 24 æ¬¡å†æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œç›¸å½“äºæœ‰æ•ˆæ‰¹é‡ = 1Ã—24ï¼Œæ›²çº¿æ›´ç¨³ã€‚
- **learning_rate = 1e-5**ï¼šDPO+QLoRA çš„ä¿å®ˆå­¦ä¹ ç‡ï¼Œé™ä½æŠ–åŠ¨ä¸è¿‡æ‹Ÿåˆé£é™©ã€‚
- **epochs = 1**ï¼šå…ˆè·‘ 1 ä¸ª epoch è§‚å¯ŸæŒ‡æ ‡ï¼ˆwin-rate/åˆæ³•ç‡ï¼‰ï¼Œå†å†³å®šæ˜¯å¦ç»§ç»­ã€‚
- **max_length = 1024**ï¼šé™åˆ¶æ‹¼æ¥åçš„åºåˆ—é•¿åº¦ï¼ŒèŠ‚çœæ˜¾å­˜ä¸è®­ç»ƒæ—¶é—´ã€‚
- **beta = 0.1**ï¼šDPO çš„åå¥½å¼ºåº¦ï¼›0.1 æ˜¯ç¨³å¥èµ·ç‚¹ï¼Œè¿‡å¤§æ˜“æŠ–ã€è¿‡å°æ”¹è¿›æ…¢ã€‚
- **length_norm = True**ï¼šæŒ‰é•¿åº¦å½’ä¸€åŒ–å¯¹æ•°ä¼¼ç„¶ï¼Œæ˜¾è‘—å‡å°‘å› é•¿çŸ­å·®å¼‚å¼•èµ·çš„ loss æ³¢åŠ¨ã€‚
- **use_lora = PEFT_AVAILABLE**ï¼šè‹¥ç¯å¢ƒè£…æœ‰ PEFT å°±å¯ç”¨ LoRAï¼Œä»…è®­ç»ƒå°‘é‡å¢é‡å‚æ•°ï¼Œçœæ˜¾å­˜ã€‚

**é‡åŒ–ï¼ˆBitsAndBytesï¼ŒQLoRAï¼‰**

- **load_in_4bit = True**ï¼šä»¥ 4-bit æƒé‡é‡åŒ–åŠ è½½åº•åº§æ¨¡å‹ï¼Œå¤§å¹…é™ä½æ˜¾å­˜å ç”¨ã€‚
- **bnb_4bit_quant_type = "nf4"**ï¼šNF4 é‡åŒ–æ ¼å¼ï¼Œç›¸æ¯” q4_0 é€šå¸¸ç²¾åº¦æ›´å¥½ã€‚
- **bnb_4bit_compute_dtype = config["dtype"]**ï¼šè®¡ç®—ç²¾åº¦ï¼ˆå»ºè®® bf16ï¼‰ï¼›æƒé‡æ˜¯ 4-bitï¼Œè®¡ç®—ç”¨ bf16/FP16ã€‚
- **bnb_4bit_use_double_quant = True**ï¼šåŒé‡é‡åŒ–è¿›ä¸€æ­¥å‹ç¼©ç»Ÿè®¡é‡ï¼Œç»§ç»­çœæ˜¾å­˜ã€å¯¹æ•ˆæœå½±å“å°ã€‚

**LoRA é…ç½®ï¼ˆPEFTï¼‰**

- **r = 32**ï¼šLoRA ç§©ï¼›è¡¨ç¤ºå¢é‡çŸ©é˜µå®¹é‡ã€‚r è¶Šå¤§å®¹é‡è¶Šå¼ºä½†æ˜¾å­˜å¼€é”€â†‘ã€‚
- **lora_alpha = 64**ï¼šç¼©æ”¾ç³»æ•°ï¼Œå¸¸å– 2Ã—rï¼›æ§åˆ¶ LoRA æ›´æ–°çš„æœ‰æ•ˆå¼ºåº¦ã€‚
- **lora_dropout = 0.01**ï¼šè½»å¾®æ­£åˆ™ï¼Œå‡è½»è¿‡æ‹Ÿåˆï¼›è®¾å¤ªå¤§ä¼šé™æ”¶æ•›é€Ÿåº¦ã€‚
- **bias = "none"**ï¼šä¸è®­ç»ƒ biasï¼Œè¿›ä¸€æ­¥èŠ‚çœå‚æ•°ä¸æ˜¾å­˜ã€‚
- **task_type = "CAUSAL_LM"**ï¼šè‡ªå›å½’è¯­è¨€å»ºæ¨¡ä»»åŠ¡ç±»å‹ï¼Œç¡®ä¿æ³¨å…¥ç‚¹ä¸è°ƒåº¦æ­£ç¡®ã€‚
- **target_modules = ["q_proj","k_proj","v_proj","o_proj","up_proj","down_proj","gate_proj"]**ï¼šåœ¨æ³¨æ„åŠ›ä¸ MLP å…³é”®æŠ•å½±å±‚æŒ‚ LoRAï¼Œè¦†ç›–é¢å¹¿ã€æ•ˆæœç¨³å®šã€‚

### 5.4 æ¨¡å‹é€‰å–

æˆ‘ä»¬ä½¿ç”¨`unsloth/Qwen3-1.7B-unsloth-bnb-4bit`æ¥åšè¿™æ¬¡å®éªŒï¼ˆ4bæ˜¾å­˜å’Œå‚¨å­˜ç©ºé—´ä¸å¤Ÿäº†lolï¼Œ base policy modelå’Œref modelå‡é€‰æ‹©è¯¥æ¨¡å‹ã€‚

## 6.å…¶å®ƒåå¥½å­¦ä¹ ç­–ç•¥

### 6.1 LiPOï¼ˆListwise Preference Optimizationï¼‰[6]

**è¦ç‚¹**ï¼šæŠŠå¯¹é½è§†ä¸º**å­¦ä¹ æ’åºï¼ˆLearning-to-Rankï¼‰\**é—®é¢˜ï¼Œä¸å†æŠŠ K ä¸ªå€™é€‰æ‹†æˆæˆå¯¹æ¯”è¾ƒï¼Œè€Œæ˜¯\**ç›´æ¥åˆ©ç”¨æ•´ç»„æ’åº**åšç›‘ç£ã€‚ç»™å®šåŒä¸€æç¤º $x$ çš„ K ä¸ªå€™é€‰ $\{y_1,\dots,y_K\}$ åŠå…¶æ’åºï¼ˆæˆ–æ‰“åˆ†ï¼‰ï¼Œä»¤ç­–ç•¥ $\pi_\theta$ äº§å‡ºåºåˆ—å¯¹æ•°ä¼¼ç„¶ï¼ŒæŒ‰ LTR çš„ **listwise** ç›®æ ‡ä¼˜åŒ–ç­–ç•¥å‚æ•° $\theta$ï¼ˆæœ¬è´¨æ˜¯â€œç›‘ç£å¼åå¥½ä¼˜åŒ–â€ï¼Œé RLï¼‰ã€‚

**åå¥½å»ºæ¨¡**ï¼šLiPO åœ¨ç»Ÿä¸€æ¡†æ¶ä¸‹ç»™å‡º pointwise / pairwise / listwise å¤šç§ç›®æ ‡ï¼Œå…¶ä¸­**listwise** ç›´æ¥æœ€å¤§åŒ–â€œç»™å®šæ’åºçš„æ¦‚ç‡â€ï¼Œå¯ç”¨ ListMLE / softmax / Lambda-loss ç­‰å®ç°ã€‚ä¸€ä¸ªå¸¸è§çš„ **ListMLE** å†™æ³•ï¼ˆå¯¹å•ä¸ª $(x,\{y\},\text{perm})$ï¼‰æ˜¯ï¼š

$$
\mathcal{L}_{\text{ListMLE}}(\theta)
= -\sum_{t=1}^{K}\Big[
s_\theta(x,y_{(t)}) - \log\sum_{j=t}^{K} \exp\big(s_\theta(x,y_{(j)})\big)
\Big],
$$

å…¶ä¸­ $y_{(t)}$ è¡¨ç¤ºç¬¬ $t$ åå€™é€‰ï¼Œ$s_\theta$ å¯å–åºåˆ—çš„å½’ä¸€åŒ– log-prob æˆ–æŠŠç­–ç•¥-æ‰“åˆ†ä¸ RM/è¯„åˆ†ç»“åˆçš„å¯å¾®åˆ†æ•°ã€‚LiPO è®ºæ–‡å®ä½œä¸­åŒæ—¶æ¯”è¾ƒäº† **pair-logisticã€pair-hingeã€list-MLEã€lambda-loss** ç­‰å¤šç§æŸå¤±ï¼Œå¹¶æŠ¥å‘Š listwise ç±»ç›®æ ‡åœ¨**çœŸå® rankwise æ•°æ®**ä¸Šå¯¹ DPO ç­‰åŸºçº¿æœ‰å¢ç›Šï¼ˆå¦‚ LiPO-$\lambda$ï¼‰ã€‚



## Reference

[1] Bradley, R. A., & Terry, M. E. (1952). *Rank analysis of incomplete block designs: I. The method of paired comparisons*. Biometrika, 39(3/4), 324â€“345.

[2] Thurstone, L. L. (1927). *A law of comparative judgment*. Psychological Review, 34(4), 273.

[3] Rafailov, R., et al. (2023). *Direct Preference Optimization: Your Language Model is Secretly a Reward Model*. arXiv:2305.18290.

[4] Wang, Zhilin, et al. "Helpsteer 2: Open-source dataset for training top-performing reward models." *Advances in Neural Information Processing Systems* 37 (2024): 1474-1501.

[5] https://huggingface.co/datasets/opencsg/UltraFeedback-chinese

[6] Liu, T., Qin, Z., Wu, J., Shen, J., Khalman, M., Joshi, R., Zhao, Y., Saleh, M., Baumgartner, S., Liu, J., Liu, P.J., & Wang, X. (2024). LiPO: Listwise Preference Optimization through Learning-to-Rank. *ArXiv, abs/2402.01878*.

[7] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback. *Advances in neural information processing systems*, *35*, 27730-27744.


## Appendix

### BT æ¨¡å‹ä¾‹å­

ä¸‹é¢ç”¨ä¸€ä¸ª**Bradleyâ€“Terryï¼ˆBTï¼‰æˆå¯¹æ¯”è¾ƒæ¨¡å‹**çš„ä¾‹å­ï¼ŒæŠŠå…¬å¼ä¸è®¡ç®—èµ°ä¸€éã€‚

1. **åœºæ™¯**

æœ‰ä¸‰ä½é€‰æ‰‹ï¼šAã€Bã€Cã€‚æˆ‘ä»¬åšäº†å‡ æ¬¡ä¸¤ä¸¤å¯¹å†³ï¼Œè®°å½•åˆ°ï¼š

- A å¯¹ Bï¼šA èµ¢ 7/10ï¼ˆp(Aâ‰»B)=0.7ï¼‰
- A å¯¹ Cï¼šA èµ¢ 6/10ï¼ˆp(Aâ‰»C)=0.6ï¼‰
- C å¯¹ Bï¼šC èµ¢ 6/10ï¼ˆp(Câ‰»B)=0.6ï¼‰

2. **BT æ¨¡å‹è®¾å®š**

ç»™æ¯ä¸ªé€‰æ‰‹ä¸€ä¸ªâ€œå¼ºåº¦/åå¥½å‚æ•°â€ $w_i$ã€‚A èƒœè¿‡ B çš„æ¦‚ç‡å»ºæ¨¡ä¸ºï¼š

$$
\Pr(A \succ B) \;=\; \sigma(w_A - w_B) \;=\; \frac{1}{1+e^{-(w_A-w_B)}}
$$

åŒç†å¯¹å…¶å®ƒç»„åˆã€‚æ³¨æ„ï¼š**åªè¯†åˆ«å¼ºåº¦å·®** $w_i-w_j$ï¼Œæ‰€ä»¥æ•´ä½“åŠ å¸¸æ•°ä¸å˜ï¼ˆæˆ‘ä»¬ç”šè‡³å¯æŠŠä¸€ä¸ªäººçš„å¼ºåº¦è®¾ä¸º 0 ä½œåŸºå‡†ï¼‰ã€‚

**ç”±æ•°æ®â€œåæ¨â€å¼ºåº¦å·®**

æŠŠ**è§‚æµ‹èƒœç‡å½“ä½œçœŸæ¦‚ç‡**çš„è¿‘ä¼¼ï¼Œå°±æœ‰ï¼ˆç”¨å¯¹æ•°æœ€å¤§ä¼¼ç„¶ä¼°è®¡ logit($p$)=$\ln\frac{p}{1-p}$ï¼‰ï¼š

$$
\begin{aligned}
w_A - w_B &\approx \text{logit}(0.7) = \ln\!\frac{0.7}{0.3} = \ln\!\frac{7}{3} \approx 0.8473 \\
w_A - w_C &\approx \text{logit}(0.6) = \ln\!\frac{0.6}{0.4} = \ln(1.5) \approx 0.4055
\end{aligned}
$$

è®¾å®šåŸºå‡† $w_A=0$ï¼Œåˆ™

$$
w_B \approx -0.8473,\qquad w_C \approx -0.4055.
$$

**ç”¨è¿™ä¸ªå¼ºåº¦å»â€œé¢„æµ‹â€å¦ä¸€å¯¹**

ç°åœ¨é¢„æµ‹ **B å¯¹ C** çš„èƒœç‡ï¼ˆé¡ºä¾¿æ£€æŸ¥ä¸€è‡´æ€§ï¼‰ï¼š

$$
\Pr(B \succ C)=\sigma(w_B - w_C)=\sigma(-0.8473 - (-0.4055))=\sigma(-0.4418).
$$

è®¡ç®— $\sigma(-0.4418)=\frac{1}{1+e^{0.4418}}\approx \frac{1}{1+1.556}\approx 0.391$ã€‚

ä¹Ÿå°±æ˜¯ **B èƒœ C çº¦ 39.1%**ï¼Œç­‰ä»·åœ° **C èƒœ B çº¦ 60.9%**ï¼Œå’Œæˆ‘ä»¬è§‚æµ‹çš„ 0.6 å¾ˆæ¥è¿‘ã€‚

