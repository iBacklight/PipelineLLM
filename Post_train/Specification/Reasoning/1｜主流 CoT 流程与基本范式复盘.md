# 1｜主流 CoT 流程与基本范式复盘

[TOC]

## Introduction

Chain-of-Thought (**CoT**) 是一种让大模型在回答前**显式写出中间推理步骤**的提示与推理范式。它把原本隐式的多步思考“外显化”为一条可读的“思维链”（步骤 → 中间结论 → 最终答案）。

**为什么有效**

- **过程约束**：把复杂任务拆成小步，降低“跳步/偷换概念”。
- **纠错与复用**：中间步骤可被检查、投票、重写或复用为新数据。
- **不依赖外部知识**：主要靠**结构化思考**提升稳定性与可解释性。



## 1. 快速总览（30–45 分钟）

- **Reasoning LLMs 2025 综述**：先扫“系统1↔系统2、显式推理轨迹、搜索/验证器的角色”这类总框图，给今天的 CoT 放坐标。[arXiv](https://arxiv.org/pdf/2502.17419?utm_source=chatgpt.com)
- **Test-Time Compute 2025 综述**：只看“what/how/where/how-well to scale”的框架图——理解 **Self-Consistency**、多样采样与“何时触发长思考”的位置。[arXiv](https://arxiv.org/abs/2503.24235?utm_source=chatgpt.com)
- **Long-CoT 2025 综述**：浏览“长 CoT 的利弊与压缩”部分，为后续日程埋个伏笔（知道有“别想太长”的证据与策略）。[arXiv](https://arxiv.org/abs/2503.09567?utm_source=chatgpt.com)

------

## 2. 主流 CoT 流程（概念 + 提示骨架）

> 在传统的思维链模式中，研究者倾向于使用提示词直接改变获取答案的质量，这本质上是一种提示工程 (Prompt Enigeering)

### 1) 经典 CoT（few-shot / zero-shot）

- **要点**：在输入中加入“思考轨迹”示例（few-shot），或用触发语（如 “Let’s think step by step.”）诱导模型显式推理（zero-shot）。 [arXiv](https://arxiv.org/abs/2201.11903)

- **Zero-shot CoT**：不给示例，仅用触发语诱导“逐步思考”。

  - 适合：题型未知、快速尝试。

  - 极简模板：

    ```
    请逐步推理并单独给出“最终答案：<…>”。让我们一步步思考。
    题目：<题干>
    ```

- **Few-shot CoT**：提供1–3个示例（题目→分步→答案），让模型模仿**拆分粒度与格式**。

  - 适合：需要稳定的输出风格/步骤模板。

  - 极简模板：

    ```
    【示例】……（含“步骤/中间结论/最终答案”）
    【请按相同格式解答】题目：<新题>
    ```

> 经验法则：**要强格式 → few-shot**；**要快速泛化 → zero-shot**。很多场景用 **hybrid**：一个很短的示例 + 触发语。

- **何时用**：题目步骤清晰、计算/逻辑中等复杂度、无需外部工具的场景。

### 2) CoT + Self-Consistency（自一致投票）

- **要点**：对同一题目进行 **多样采样**（改变温度/随机种子），再对“最终答案”做 **多数表决**。题目可被多种有效路径解出（数学、逻辑、多跳问答）。arXiv](https://arxiv.org/abs/2203.11171)

- **投票方式**

  - **多数表决（Majority Vote）**
    - 做法：同题采样 N 条思维链，统计“最终答案”出现次数，取频次最高者。
    - 何时用：入门默认；N 较小（5–10）。
    - 细节：并列时用“**最短链优先**”或“**置信度最高优先**”。
  - **置信度加权投票（Confidence-Weighted）**
    - 做法：要求每条链**自报置信度**（1–5分）或用 log-prob/困惑度近似置信；按权重计票。
    - 何时用：模型自评稳定、或能拿到 token-level 概率时。
  - **长度惩罚投票（Length-Penalty / Shortest-Consistent）**
    - 做法：对相同答案，优先选择**平均链更短**（或惩罚过长链）的一组。
    - 何时用：控制成本、防止“越想越错”。
  - **软投票（Soft Vote by Probabilities）**
    - 做法：把每条链的“最终答案”对应概率相加/取均值，选最大者（可做温度缩放）。
    - 何时用：需要更细粒度的置信聚合。

- **骨架**

  ```
  （同“经典 CoT”骨架）
  ```

- **实操提示**：以 N=5/10 路径起步；若一致率低，增大 N 或改用分解式提示。[arXiv](https://arxiv.org/abs/2203.11171?utm_source=chatgpt.com)

### 3) Least-to-Most（先分解后求解）

- **要点**：把复杂题拆成序列子问题，从易到难逐步解决；对需要“组合/构造”的题特别有效。[arXiv](https://arxiv.org/abs/2205.10625?utm_source=chatgpt.com)

- **骨架**

  ```
  目标：用“先分解，后求解”的方式完成题目。
  第一步：列出子问题清单（从最简单到最关键）。
  第二步：逐个求解子问题，并在每步后给出“中间结论”。
  最后：整合中间结论，给出“最终答案：<…>”。
  题目：<题干>
  ```

### 4) Plan-and-Solve（先拟计划再执行）

- **要点**：显式生成“计划（步骤纲要）”，再按计划填充细节，可减少“漏步/跳步”。[arXiv](https://arxiv.org/abs/2305.04091)

- **骨架**

  ```
  请先给出“计划（仅要点）”，随后逐步执行该计划，每步只做一件事并给出理由。
  题目：<题干>
  输出格式：
  计划：
  - 步骤1：…
  - 步骤2：…
  执行：
  步骤1 → 中间结论：…
  步骤2 → 中间结论：…
  最终答案：<…>
  ```

### 5) Self-Ask（自提子问；可接检索/工具）

- **要点**：先让模型**提问**自己需要的关键信息（可选地检索/计算），再汇总作答；擅长多跳（Multi-hop）问答。[arXiv](https://arxiv.org/abs/2210.03350)

- **骨架**

  ```
  先列出必须回答的子问题（原子化），标注是否需要查证/计算；
  逐一回答，并在最后整合结论给出“最终答案：<…>”。
  题目：<题干>
  ```

### 6) Verifier / PRM（裁判 / 过程奖励）

- 需要**稳定与可控**的推理质量，想把“生成-验证-再生成”做成闭环。
- 想减少长链错误累积，或在**短链**下保持正确率。
- “裁判/Verifier（含 PRM）”既可以是**更强更大的模型**，也可以是**与被评模型同级甚至更小、专门训练过的判分器**，还可以是**规则+模型**的组合。关键是：能否**稳定、低偏置**地判断“这条思维链/这一步是否合理”。

- 最小路径

  1. 选择裁判类型：
     - **规则裁判**：单位/范围/格式/回代检查；
     - **模型裁判（PRM/Verifier）**：对每步/整链打分；

  1. 生成阶段：产出 M 条候选链；

  1. 验证阶段：逐步/整链评分，**淘汰低分**或**要求重写出错步骤**；

  1. 聚合阶段：在通过验证的候选中选最优（可再投票）。

- **骨架（裁判提示示意）**

```
你现在是“裁判”。请审查下列思考链，逐步判断每一步是否合理并给出理由。
若发现错误，请指出具体步号与原因；若可修复，给出最小修改建议。
请给出最终判定：通过/不通过 + 分数(0-1)。
[思考链文本]
```

- 指标：裁判-答案一致率、通过率、最终正确率提升、额外 token 成本。
- Ablation：单裁判 vs 多裁判；步级 vs 整链；重写次数上限。



## 3. CoT趋势

- **系统2/“先思考再作答”家族**（如 OpenAI **o1**、DeepSeek **R1**）：不是提示工程层面的 CoT，而是**在训练或推理预算上显式为“思考”买单**；你可以把上面的 CoT/分解/投票视作“轻量系统2”，与它们在理念上是一条线。
- **TTS（测试时算力）**视角：把“是否触发 CoT、采样几条、是否投票”理解为 **推理时的预算分配策略**； Self-Consistency 就是 TTS 里“**扩多样性→再聚合**”的基本招。[arXiv](https://arxiv.org/abs/2503.24235)

------

## 4. 典型失效模式 & 立即可用的对策

- **漏步/跳步** → 用 **Plan-and-Solve** 或 **LtM** 显式强制步骤、加“中间结论”锚点。[arXiv](https://arxiv.org/abs/2305.04091)
- **算术/符号错误** → 在模板中加入“单位/格式校验 + 最终只给一个数字/选项”的约束；必要时结合少量多样采样（N=5）。[arXiv](https://arxiv.org/abs/2203.11171)
- **思路漂移/冗长** → 给出**固定输出槽位**（计划/步骤/中间结论/最终答案）；若仍冗长，改为 LtM 的“最小充分链”。[arXiv](https://arxiv.org/abs/2205.10625)
- **答案不稳定** → 尝试 **Self-Consistency 投票**；若成本过高，之后会学习“短 CoT/压缩”策略。[arXiv+1](https://arxiv.org/abs/2203.11171)

------

## 5. 论文

1. **CoT 原始论文**（看引言+方法+图表）：理解“显式推理”的核心直觉。[arXiv](https://arxiv.org/abs/2201.11903)
2. **Self-Consistency**（看方法+实验）：为什么“多路径→投票”显著提分。[arXiv](https://arxiv.org/abs/2203.11171)
3. **Least-to-Most / Plan-and-Solve / Self-Ask**（看方法示例）：掌握三种“先分解”的典型提示结构。[arXiv](https://arxiv.org/abs/2205.10625)
4. **2025 综述扫读**（挑图表与框架）：把上述套路映射到“系统2/TTS/长-短 CoT”的更大图景。[arXiv](https://arxiv.org/pdf/2502.17419)
5. **o1 / R1 博文/论文摘要**：理解新一代“会思考”的定位与与 CoT 的关系。[openai.com+](https://openai.com/index/learning-to-reason-with-llms)

