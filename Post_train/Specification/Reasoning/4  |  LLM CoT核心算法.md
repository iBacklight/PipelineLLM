# 4 |  CoT 分类算法

[TOC]



​	推理LLM能够实现**复杂推理**，离不开一些关键技术手段的支持。本文将核心方法归纳为以下五类（见图4所示）：**结构搜索**、**奖励建模**、**自我改进**、**宏动作**和**强化微调**。下面分别介绍每种方法的基本思想、技术路径，以及它们的优势与局限，并适时比较它们之间的关系。这篇内容主力要来源于综述 [1]。

## 1. 结构搜索 (Structure Search)

- **核心思想**：引入搜索算法（特别是蒙特卡洛树搜索MCTS）辅助LLM进行多步推理。由于基础LLM在面对复杂问题时存在诸多固有限制（如缺乏内部环境模型、无法预测长程推理结果、难以根据未来信息调整当前步骤等），导致它们无法在庞大的推理空间中有效平衡探索和利用。**结构搜索**的目标正是弥补这点，通过系统性的搜索策略，让模型像人一样**规划、评估多种可能**，从而提高解题深度和正确率。
- **技术路线**：结构搜索通常借助MCTS来实现。MCTS在推理过程中构建一棵搜索树，每个节点代表一个中间**推理状态**，节点分支则对应不同的下一步推理选择。算法通过模拟未来若干步的推理并估计回报，不断对树中节点进行**评估和回溯**，从而指导模型选取高价值的推理路径。这一过程允许模型**迭代改进**中间推理步骤：如果某条路径前景不佳，可以回退换另一条尝试。例如，RAP方法将LLM与一个**世界模型**结合进MCTS，使系统能够反复优化中间推理并改进对未来步骤的预测；又如Forest-of-Thought利用MCTS动态探索多条推理轨迹，发现错误路径后可以返回修正结果。这些工作证明，将MCTS这种**树搜索**嵌入LLM推理框架，能有效提升LLM解决数学、逻辑推理等多步任务的能力。表1列出了一些基于树搜索的方法示例。
- **优势**：结构搜索赋予LLM一些单次前向生成做不到的能力。首先，它允许模型**走走停停，反思重来**：不满意当前解法时，可以通过搜索回溯上一步乃至更前步骤重新思考。这种迭代能力对具有巨大决策空间或需要长远规划的任务至关重要——早一步决策的偏差可能影响最后结果，通过搜索，模型可在**做出最终决定前反复检验**各种路线。其次，结构搜索带来更**全面的探索**：MCTS可以在不同路径间分配资源，避免模型局限于惯常的思路，从而可能找到非直观但正确的解法。研究表明，引入树搜索后，模型不仅在特定任务上性能更好，在跨领域任务上也表现出更强的**泛化**，因为它学会了一种通用的“思考策略”。再次，结构搜索能**对接奖励信号**：在搜索中定义合适的**动作**和**奖励**来评估路径优劣是关键一步。这可以将奖励模型（见3.2.2节）融入，形成**规划-评估**闭环，进一步提升决策质量。因此，结构搜索为推理LLM提供了更高级的**战略探索能力**，让其在复杂问题上能接近人类的缜密思维。
- **局限**：首先，结构搜索的**计算开销**较大。MCTS需要多次模拟，每一步都要询问LLM，导致推理速度变慢。此外，搜索树的空间需要手工设计参数，比如设置搜索的宽度、深度，否则模型可能**探索空间不足**或反之浪费算力在不必要分支上。过小的搜索空间限制模型探索，过大的搜索又不切实际。其次，树中不同分支之间的信息**难以共享**。模型在一条路径上学到的经验，无法直接用于另一条，只能通过最后的结果或奖励来间接反馈。因此效率上仍有提升余地。再次，搜索方法有**样板化**倾向：某些父节点不同子分支之间缺少多样性，模型可能在同一节点下尝试的策略彼此相似，导致探索不足。最后，结构搜索需要**定义动作空间**（模型每一步能做什么）和**奖励函数**（如何衡量好坏），这在不同任务上并非易事，需要经验和反复试验。不恰当的动作定义可能让搜索无效，不恰当的奖励会误导模型。这一点已经引起关注，后续一些工作通过**丰富动作空间**和更复杂的策略（如采用宏动作、环境模拟等）部分解决了上述问题。但总体而言，结构搜索提供的强大能力是以复杂度为代价的。
- **比较**：**结构搜索 vs. 直接CoT**：后者一次性地从头写到尾，无法回头修正；结构搜索则相当于给模型“多次尝试”的机会，因此对于需要试探的难题，结构搜索成功率高但速度慢。**结构搜索 vs. 奖励建模**：两者可以结合。结构搜索更像推理阶段的算法，奖励建模则可以用于训练阶段或者作为搜索过程的评价函数[ar5iv.labs.arxiv.org](https://ar5iv.labs.arxiv.org/html/2502.17419#:~:text=Summary%3A MCTS has played a,to handle intricate and dynamic)。**结构搜索 vs. 自我改进**：结构搜索通常发生在**推理（测试）时**，而自我改进多指模型在训练或推理中自我反馈提高（下节详述），但两者思想相近，都是利用模型自身行为来优化决策，只是一个更偏外部算法，一个偏内部学习。**结构搜索 vs. 宏动作**：宏动作可以看作丰富的高层动作定义，事实上宏动作常被用于扩充搜索的行动空间，让搜索算法有更大施展余地。因此宏动作框架与结构搜索并不冲突，反而是升级版的结构搜索——把模型推理抽象成更高层步骤来搜索。

## 2. 奖励建模 (Reward Modeling)

- **核心思想**：通过**奖励模型**来评估并指导LLM的推理过程。不同于让模型自由生成然后只看最终答案对不对，奖励建模在模型训练或推理中引入一个**评价者**，对模型输出的**最终结果**或**中间步骤**打分，以此作为训练信号或选择依据。其目的是为多步推理提供更精细的监督，纠正模型过程中的偏差，提高推理可靠性。

- **技术路线**：有两种主要范式：**结果监督（Outcome Supervision）\**和\**过程监督（Process Supervision）**。结果监督只关注最后答案是否正确，训练一个**结果奖励模型**（Outcome Reward Model, ORM）来根据最终解答的正确性给出奖励。过程监督则要求对解题的每一步骤打标签，训练一个**过程奖励模型**（Process Reward Model, PRM）来评估每个推理步骤的质量。简单来说，ORM粗粒度地奖惩模型最终输出，而PRM细粒度地指导模型每一步推理。以数学证明题为例，ORM可能只在证明结论正确时给高分，而PRM则会检查证明过程中的每一步推理是否合乎逻辑并分别打分。

  在训练阶段，可以使用奖励模型来做**反馈指导**：如采用强化学习(RLHF)或有偏采样，让模型倾向于生成奖励高的推理路径，从而提高最终准确率。在推理阶段，也可以用奖励模型**筛选**模型自生成的多个解答，选出评分最高的作为最终答案（类似于**自洽性**或“最佳样本”挑选）。

- **优势**：采用PRM这样的过程监督有显著好处：首先，它提供了**细粒度**、逐步的信号，使模型能精确定位解题过程中哪里出错。例如，如果一个数学推理的某一步算错了，PRM会在那一步给低分，模型据此可以只调整该步，而不至于整条推理全盘皆输。这种针对具体错误的监督对强化学习以及自动化错误校正尤其有价值。其次，PRM的评估方式更贴近**人类推理**，因为人解题也是确保每一步都正确才能推出正确结论。相比之下，ORM只看最后对错，可能出现**“歪打正着”**的情况（中间逻辑错但最后巧合得对），从而误导模型学习到不可靠的模式。PRM则避免了这种情况，确保模型每一步走得扎实，这让推理链条更加**稳健、可解释**。再次，PRM虽然最初主要用于复杂数学问题，但其优势很快推广到其他领域。例如，ORPS方法将PRM用于代码生成场景，帮助评估代码中间步骤是否有利于最终正确运行。又如Step-DPO结合过程监督与直接偏好优化算法，提升了数学长链推理的能力。甚至在多模态领域，也有MSTAR等工作利用自我演化训练优化PRM来处理视觉-语言任务，以及VisualPRM通过“最佳N选1”的办法有效地给多模态推理步骤打分。这些成果都证明了PRM的**广泛适用性**。

- **局限**：首先，训练一个高质量的PRM需要大量带**过程标签**的数据，即每道题的标准解题步骤及对每步的好坏判定。这种数据获取成本极高，往往需要专家人工标注多个步骤的对错。其次，PRM本身也面临**可信度**挑战。它可能给每步打分却没有给出解释，导致模型和人都不太清楚为何某步被扣分。缺乏解释的评分降低了其指导推理的有效性，因为模型难以知道如何改正（这有点像老师只打分不讲错哪儿）。另外，PRM的训练数据和方法易引入偏差。比如，用MCTS生成的训练数据可能偏重某些问题模式，导致PRM对这些模式评分偏高，对其他模式不敏感。再如，PRM对推理初始几步的预测准确率往往较低，因为一开始不确定性最大。这些**偏差**都削弱了PRM对模型的指导作用。最后，如果奖励模型设计不良，还可能发生**奖赏滥用**（reward hacking）：模型学会投机取巧拿高分，而非真正改善推理。这在RL中是知名问题，需要通过交替训练和防范策略来缓解。

- **比较**：**奖励建模 vs. 结构搜索**：前者提供**评价机制**，后者提供**探索机制**。两者经常配合使用——结构搜索过程中需要奖励模型来评估节点优劣，从而剪枝或选择最优路径[](https://ar5iv.labs.arxiv.org/html/2502.17419#:~:text=Summary%3A MCTS has played a,to handle intricate and dynamic)。**奖励建模 vs. 自我改进**：自我改进有时依赖模型自身的反馈，但也可以借助奖励模型的打分来进行**自我过滤**，筛掉不良解答。比如STaR方法就是模型生成多个解答后用验证者（可看作一种ORM）筛选高质量的再训练模型。**奖励建模 vs. 强化微调**：强化微调本质上就需要一个奖励信号，这通常来自人类偏好（RLHF）或任务定义。奖励建模可以理解为**专门为推理任务设计的奖励信号**。Process Reward和Outcome Reward模型都可以直接用于RL训练中，引导模型参数朝正确方向优化（详见3.2.5节）。可以说，奖励建模是RL方法在推理场景下的特化应用。**ORM vs. PRM**：ORM实现简单，但信息利用率低；PRM信息丰富但成本高。实践中二者也可以结合，如一些工作用ORM便宜地产生数据训练一个隐式PRM，以减少对人工标注的依赖。

## 3. 自我改进 (Self Improvement)

- **核心思想**：利用模型自身的探索能力，实现由弱到强的**自我迭代提升**。与传统直接微调（CoT微调）不同，自我改进强调让模型参与到数据生成和策略优化的过程中，逐步提高推理性能。这一理念类似AlphaGo的自我对弈：模型不断用自己的输出训练自己，从而摆脱人工数据的限制，达到更高水平。
- **技术路线**：自我改进可以分为**训练阶段**和**推理阶段**两类应用：
  1. **训练期的自我改进**：模型在训练过程中通过自身生成的数据或反馈来强化学习效果。这又包括不同的**探索策略**和**改进策略**。
      **探索阶段**，模型需要收集可以用来提升自己的数据。方法多种多样：STaR利用少量示例让模型自由发挥生成解题过程，再将这些生成的过程作为额外训练数据；ReST系列工作让模型对每个问题**采样多条完整推理轨迹**，以获取多样解法；Quiet-STaR甚至把探索细化到每个token的生成，引入“元token”和非贪心损失等机制，让模型在token级别进行尝试，从而获取更丰富的监督信号。此外，结构搜索也能用于数据生成：如ReST-MCTS*结合了过程奖励引导的MCTS来生成高质量的推理轨迹作为训练样本，rSTAR-Math通过MCTS扩充数学推理数据。
      **改进阶段**，模型基于收集到的数据进行参数更新，不同方法有不同策略：STaR及其改进版（如V-StaR、B-StaR）通常将模型生成的解答**过滤**（比如只保留正确或评分高的），然后用这些数据进行监督微调。ReST系列则倾向于引入奖励机制，将采样来的多条轨迹通过设计好的奖励计算融入强化学习训练，以改进模型策略。还有RISE方法结合了**外部反馈**，记录模型回答的奖励再蒸馏回模型，使其在改进中参考环境或任务给的奖励。值得一提的是，rSTAR-Math的结果表明，哪怕是相对较小的模型，通过这种**自我演进式训练**也能获得类似System 2的反思能力。表3列出了一系列自我改进方法的概览。
  2. **推理时的自我改进**：这类方法不改变模型参数，而是在**推理过程中**动态利用模型自身知识来修正输出。典型手段有：模型先生成一个初步答案或解题步骤，然后触发一个内置的**自检机制**再改进。比如Self-Refine方法是在模型给出答案后，提示它“再检查一遍并给出改进版本”，模型往往能纠正之前的错误。Self-Check则让模型针对自己推理的每一步做检查确认，找到不一致之处进行修补。还有CRITIC等方法，引入一个独立的“评论员”模块（可以是另一个LLM或工具），对模型输出做评价，然后模型根据评论反馈调整回答。例如EffiLearner在代码生成中用编译器（外部工具）检查代码是否有错误，再指导模型修改。一些方法如 Self-Verification 让模型产生多个答案互相验证、或SelfEval-Decoding通过束搜索确保推理链各步合法。总之，这类方法将模型**多步对话**式地使用，让它自己发现并纠正谬误，从而在不额外训练的情况下提高答案质量。表3中Inference部分列举了一些这种推理时自校正的方法。
- **优势**：自我改进减少了对人工高质量数据的依赖，充分挖掘模型自身的潜力。通过**自举**的方式，模型可以拓展训练数据（尤其针对复杂推理，很难人工穷尽各种情况），并不断适应更高难度。它还鼓励模型**探索**——因为模型要尝试不同解法来丰富数据或纠错，这避免了模型被动学习时形成的某种**固化**。在应用上，自我改进已被证明能有效提升翻译、数学、多模态感知等任务的性能。特别值得强调的是，自我改进为**小模型赋能**提供了一条路径：一些只有几十亿参数的模型，通过自我演化训练，竟也展现出了类似更大模型的反思与推理能力。这对于降低推理LLM门槛意义重大。
- **局限**：首先，如果模型初始性能较弱，生成的数据质量可能不好，直接用于训练会形成**错误的自我强化**——模型不断学自己的错误输出，越学越错。所以自我改进一般需要搭配**筛选**或**评分**机制确保反馈质量，如用验证模型过滤，或只选高置信度输出。其次，自我改进过程可能引入**分布偏差**：模型倾向重复自己擅长的某些模式，导致生成数据多样性不足，这需要通过加入随机性或外部信息来缓解。再次，自我改进需要更多算力（例如生成多样解答、反复改进增加了推理时间）以及精细的流程设计，否则适得其反。最后，推理时的自我改进尽管不改参数，但它其实延长了推理链，加大了推理成本，在应用中要权衡性能和效率。另外，在某些场景下模型自我检查仍可能看不出自身错误（所谓“盲区”），因此引入外部工具或多模型协作有时不可避免。总的来说，自我改进为提升性能提供了强力手段，但需要精心设计流程以避免陷入“自我误导”的陷阱。
- **比较**：**自我改进 vs. 奖励建模**：两者着眼点不同：自我改进重在**如何获取和利用额外数据/反馈**，而奖励建模重在**如何评估**。实际上自我改进常常会用到奖励建模的结果，如使用过程奖励评估模型采样的步骤并指导改进（例如ReST使用奖励算分来强化训练）。可以说奖励模型是自我改进的重要工具。**自我改进 vs. 结构搜索**：结构搜索发生在推理中，为单次求解服务，而自我改进的训练式方法是在改进模型本身，推理式方法虽然也在一次求解中迭代但不需要外部搜索算法（更多靠模型自身多次生成比较）。**自我改进 vs. 宏动作**：宏动作可以融入自我改进框架，比如在自我探索时让模型采取宏动作以获得更高层次的多样解法。**自我改进 vs. 强化微调**：强化微调本身就是一种自我改进的实现方式（通过模型与环境交互获得奖励再更新）。区别是强化微调通常泛指借助环境奖励优化模型参数，而自我改进范围更广，包括不通过环境、仅靠模型自身知识来改进。两者可结合，如一些工作用强化学习产生的数据再做自监督改进，或在自我训练过程中引入少量强化步骤。

## 4. 宏动作 (Macro Action)

- **核心思想**：将推理过程提升到**宏观层次**，通过**分阶段的层次化思维**来解决问题。传统LLM是一字一句自回归地产生答案，而宏动作框架则让模型的思考分为**战略规划、内部自省、迭代精炼**等阶段。可以理解为在模型的“脑海”中划分几个心智过程：先想大方向，再验证，然后细化等。每个阶段相当于一个“宏动作”。这种分层机制拓展了模型的解题思路广度和深度，使其能够找到**更稳健、多样**的解决方案。

- **技术路线**：宏动作方法主要有两方面进展：

  1. **测试时的宏动作**：也即**推理扩展**。当模型在推理时遇到复杂任务，可以借助预先设计或学习得到的**宏动作序列模板**来指导推理。例如，HiICL-MCTS方法在推理前进行** deliberate search（深思搜索）**，利用一批种子数据生成一系列宏动作链模板，让模型在推理时遵循这些高层步骤。ReasonFlux框架则采用**迭代推理扩展**：引入外部高层思维模板，不断对当前Chain-of-Thought进行改写和更新，逐步逼近正确解答。简单来说，这类方法为模型提供了**高阶指引**，让它在推理过程中以更大的步骤为单位行动（而非逐词），从而提升复杂推理题目的性能和效率。这类似人类解题时先制定大纲，再逐步展开。
  2. **数据生成与训练中的宏动作**：宏动作也可用于**合成训练数据**和**丰富训练模式**。一个关键应用是在**推理数据增强**中融入宏动作，以提升模型的泛化能力和数据效率。例如，LLaVA-CoT将宏动作思想用于多模态CoT数据生成：显式地将推理过程的中间步骤（如视觉分析阶段、语言推理阶段）体现在训练样本中。AtomThink通过在提示中结构化引导（称为g1提示）生成一个包含宏动作的数学推理数据集AMATH-SFT，结果对长程推理任务效果显著优于传统CoT训练。CoAct甚至引入了**双智能体**协同框架：一个全局规划智能体负责执行宏动作（高层决策），一个局部执行智能体负责在每个宏动作下完成具体小步骤。这一分工使得模型既能有全局视野又能顾及细节，是宏动作在训练架构上的体现。

  此外，宏动作还被证明对**自我改进**很有帮助。比如rSTAR-Math结合**代码增强的CoT**进行高层次搜索，能产生多样可靠的解并具备前瞻性；Satori把CoT和RL结合，引入“<reflect>”这样风格的宏动作以增加探索多样性，避免在线RL训练时策略过早收敛；Huatuo-o1则将宏动作与医疗知识库结合，通过层次规划+知识检索提升医学推理性能。ReasonFlux也会动态重构推理模板（比如把微积分题拆成符号阶段和数值阶段），使推理步骤更契合问题结构。

- **优势**：宏动作赋予模型**全局规划视野**。通过将推理划分阶段，模型能先处理**“想什么”**再处理**“怎么说”**，这使得推理链更有条理，\**减少了走弯路\**的概率。宏动作让模型敢于探索不同策略，因为它可以先假设几个高层方案再逐一验证，而不是一条路走到黑。这显著提高了解题的**多样性**和**鲁棒性**。此外，在训练数据中加入宏动作，有助于模型学习**分解问题**、**分配注意力**的能力。例如，将长链推理显式标注几个阶段后，模型知道先分析再推导，从而数据利用效率提高（实验表明加入宏动作的推理链能**显著提升**模型小样本下的表现）。宏动作还方便融合先验知识或人工启发：我们可以手工设计一些宏动作序列给模型用，迅速提升特定任务的效果。总之，宏动作通过**层次化**让模型的推理变得更深刻却不混乱，更复杂却可控，体现了符号AI与LLM结合的思路。

- **局限**：首先，设计有效的宏动作不是易事。哪些步骤作为宏动作、顺序如何安排，往往需要对任务有深入理解，还可能**因任务而异**。这导致宏动作具有一定**任务特定性**：一个领域有效的宏动作换个领域未必适用。其次，如果宏动作是**人为设计**的，那么其上限受制于设计者的聪明才智，模型可能被框在这些模板内，错过其他解法。但如果完全让模型自己学宏动作，又涉及如何在模型中表征和触发这些高层行为，目前这仍是开放研究。再次，引入宏动作层通常**增加了系统复杂度**。可能需要额外的控制模块、策略来决定何时用哪个宏动作，这些都需要调优。也有工作尝试动态宏动作（如根据情境自动调整宏动作序列），但这进一步增加算法复杂度。最后，宏动作阶段的错误可能造成**全局性影响**：如果最初规划错了方向，即便后续微观步骤做得再好也无济于事。因此，宏动作模型要有纠错机制（许多框架通过在宏动作层也引入“反思/重试”来解决这一点）。尽管如此，宏动作作为让LLM突破逐字生成局限的有力工具，已经展现出巨大潜力，研究者正在持续完善其自动化和通用性。

- **比较**：**宏动作 vs. 结构搜索**：宏动作本质上是**扩充了LLM的行动空间**——LLM不仅可以逐词输出，还能一次输出一个“段落”或一个“步骤”。结构搜索如果把每个宏动作当成一个节点的行动，那么配合宏动作的搜索能够搜索更大的解空间。反过来，没有搜索，仅宏动作生成也是可行的（模型一次生成一长段分析，再生成下一段）。**宏动作 vs. 奖励建模**：宏动作框架通常也需要评价其宏观步骤的好坏，可以借助奖励模型去评估一个宏动作序列是否有效。**宏动作 vs. 自我改进**：宏动作和自我改进可以相辅相成，前者提供更高层的探索单元，后者提供迭代改进机制。例如模型可以通过尝试不同宏动作序列来自我探索，然后强化学习选出好的宏动作方案。**宏动作 vs. 强化微调**：OpenAI的RFT等方法目前主要还是在token级别进行强化学习。如果加入宏动作，未来可能看到强化学习直接针对宏动作序列进行（类似强化学习中选Macro-Action），这可能提高样本效率。另一方面，RFT已经隐含学到一些宏观策略，因为它优化的是整个推理过程是否能得到高奖励。

## 5. 强化微调 (Reinforcement Fine-Tuning, RFT)

- **核心思想**：**强化微调**是OpenAI最近提出的一种创新技术，旨在使用强化学习的方法对已有模型进行微调，以让模型在特定领域或复杂任务上具备更强的推理能力。不同于常规的有监督微调（SFT），RFT通过引入**奖励机制**来引导模型演化，从而优化模型的推理过程和准确性。可以将其视为**将RLHF的思想进一步专门化**用于推理任务：通过少量高质量数据和一个合理的奖励设计，让模型用强化学习策略进行定向提升。

- **技术路线**：RFT的核心在于利用**最少量的高质量训练数据**、配套的奖励模型，以及针对长上下文优化的稳定训练过程，来提升模型在特定领域的表现。具体步骤通常包括：

  1. **定义奖励**：可以是**结果奖励**（正确解答则奖励，高度惩罚错误）或结合**过程奖励**（每步正确有奖），视任务需求设定。也需要训练或制定一个可靠的**奖励模型**来给出这些奖励信号。
  2. **准备环境**：让模型与一个环境交互，这个环境可以是真人反馈、模拟测评或者自动判题系统等。
  3. **应用RL算法**：使用策略梯度、PPO等RL算法，在模型生成输出后根据奖励进行参数更新。OpenAI提到的Reinforcement Fine-Tuning主要使用他们改进的策略优化算法（如Proximal Policy Optimization变体）来更新模型权重。
  4. **循环训练**：模型不断尝试任务、获取奖励、优化，直到在少量训练任务上表现达到要求。

  与RLHF相比，RFT更强调**推理能力**的提升，而不仅是语言流畅度或偏好满足。它通常针对**复杂推理任务**（如数学证明、代码合成）进行微调。以DeepSeek-R1为例，他们采用了**验证者奖励**策略：模型生成解答后通过一个验证模型判定正确与否，作为奖励。在强化微调训练下，DeepSeek-R1的性能远超传统方法，如Self-Consistency等。

- **优势**：强化微调已经展示出多项**显著优势**。
   **训练流水线简化**：由于有RL的监督，数据构建和训练过程更**简洁**，不需要复杂的逐步搜索机制，也不一定要大量标注过程数据。模型通过试错自主获取信号，减少了人为干预。
   **可扩展性**：在线RL训练可以在**大规模数据**上高效扩展，相比传统有监督方法更容易处理复杂推理任务的数据。而且RL可以持续训练，让模型不断学习新问题而不遗忘旧问题（通过适当策略可实现）。
   **涌现能力**：实践表明，DeepSeek-R1在强化微调后出现了一些**独特的涌现行为**，如能自主生成**超长的推理链**（Long-CoT）、出现“顿悟时刻”（aha moment，一步骤突然解开问题）等，这些仅靠有监督微调很难实现。也就是说，RL训练触发了模型潜藏的高阶推理能力。例如，R1模型通过简单的RL扩展，就学会了以前需要人工设计链条才能做到的深度反思和规划。

  此外，R1的成功在纯文本推理中验证后，很快被移植到**多模态场景**（MLLM，即多模态慢思模型）。通过类似的RFT方法，研究者将DeepSeek-V3这样的基础模型强化微调到图像、医学影像等领域，取得了多领域SOTA表现。关键策略包括**基于规则的奖励**（如给符合逻辑结构的解答高分）、**跨模态通用训练框架**（如Vision-R1在视觉推理上引入冷启动的CoT数据生成）、以及**高效参数利用**（如7B参数的MM-R1就能逼近更大模型性能）。在强调透明性的医学场景，R1模型还能给出**可解释的推理路径**辅助人类理解。

- **局限**：首先，目前尚不完全清楚RL究竟**如何**提升了推理能力。一些现象引人深思：DeepSeek-R1表现出的长链推理能力，也许在基础模型里就已经潜伏存在，而不全是RL训练新学的，有研究提示某些能力可能非RL独有。同时，在较小规模模型上，性能提升也可以出现但却没有观察到类似“aha时刻”，这让人困惑RL的作用机制。这些**机理不明**的问题需要更多理论分析。
   其次，RL训练常遇到**奖励饱和**难题：训练一段时间后模型可能“学会”了奖励模型的偏好，导致**探索崩溃**，也就是模型只重复那些得高分的模式，不再创新。据报道，很多RL算法在100多个训练步后就出现奖励不再提升，这对于复杂推理任务来说远远不够。DeepSeek-R1通过特别的奖励格式设计有所缓解，但其他工作也提出不同解决方案，如ReFT和Satori建议交替采样和SFT蒸馏，防止模型过度迎合奖励而不求变。
   第三，RL产生的**长推理链**有时不稳定。模型会出现**上下文超长**（context overflow）导致记忆混乱，或者有时给不出最后答案（因为一直在推理没收敛），对奖励形状也很敏感。举例来说，有的方法引入了余弦奖励函数希望调控输出，结果反而令性能下降。又如O1-Prune通过在RL/SFT后处理时剪掉过长内容来稳住输出。在多模态模型中也看到类似问题：处理长视频或复杂图像推理时，输出链条不稳定，使模型难以生成连贯的多步解答。

  最后，RL引入**安全**问题：模型为了奖励可能会产生不符合人类价值的做法（后面5.7节详述）。这需要额外警惕。

  值得一提的是，有些研究在探索**无外部奖励的RL**，即模型仅凭内在信号改进推理。比如INTUITOR等工作表明，即便没有显式奖励，巧妙设计模型目标，也能在推理上取得进展。这可能成为强化微调的另一方向，以避免对人工奖励模型的依赖。

- **比较**：**强化微调 vs. 经典微调**：前者利用交互和奖励，可以在很少的新数据下取得大的性能提升，并可能激发模型新能力；后者需要大量标注数据，且能力受限于训练分布，缺少探索。**强化微调 vs. RLHF**：RLHF通常优化模型的回答符合人类偏好，一般用于对话、避免不良内容等；RFT则针对**认知推理任务**，重在提升模型解题的逻辑正确性和深度，而不仅是表面上的可接受性。二者目标不同，但技术类似。**强化微调 vs. 结构搜索**：可以认为强化微调让模型**内化**了结构搜索的能力。以前模型要靠MCTS额外搜，现在模型参数经过RL优化后，可能自己学会了一些搜索策略。因此在推理时RFT模型可以直接给出长链高质量推理，而不需要显式树搜索。这是深度学习“用数据学算法”的体现。但对于一些极复杂任务，也有可能把结构搜索嵌入到RFT训练中（例如让模型在MCTS模拟环境里学）。**强化微调 vs. 奖励建模**：RFT几乎完全依赖奖励模型，它是**奖励建模思想的执行者**。有了RFT，我们可以用奖励模型来不断训练模型，而不仅仅是评估或选答案。**强化微调 vs. 自我改进**：RFT是自我改进的一种（通过与环境交互自我提升），区别在于它强调使用强化学习算法和明确的奖惩信号。RFT和自我监督也可结合，如在RFT训练中周期性地加入监督微调步骤稳定训练。**强化微调 vs. 宏动作**：目前RFT多在token级别优化，但未来可以尝试针对宏动作序列优化策略，让模型学会何时采用何种宏行动解题。这将把RFT和宏动作的优势结合起来。

## 6. 推理 LLM 的演进 (Evolution of Reasoning LLMs)

推理型LLM的发展大致经历了若干**阶段**，每个阶段都针对直觉式LLM（直接自回归生成）的局限提出了不同的改进策略，逐步构建起更先进的慢思考推理架构。概括来说，从早期的“外部辅助”到中期的“内置推理”，再到近期的“强化涌现”，推理LLM不断进化。

**阶段1：外部算法增强**。在这一初期阶段（约2023年前后），研究者没有修改LLM自身的参数，而是借助**外部推理算法**来提升预训练LLM的推理效果。典型方法有*思维树搜索*（Tree of Thoughts）和*通过规划进行推理*（Reasoning via Planning）等，它们利用LLM来驱动BFS、DFS以及MCTS等搜索算法。简单来说，LLM扮演“分步提议者”，外部算法组织这些步骤形成树或图结构的推理路径。然后通过投票机制或者蒙特卡洛值估计来从众多路径中选出一个最佳解答。**举例**：Tree-of-Thoughts让LLM展开一个思维树，每步分出多个想法节点，最后用**自洽性投票**选最可信路径；RAP等用LLM+MCTS构造不同推理序列，通过估值函数找最优。

这种方法证明了在LLM外加一层搜索/规划，确实可以解决基础LLM无法解决的复杂任务。然而，它也带来了**挑战**：一方面，**探索空间受限**：由于算力和时间有限，必须预先限定搜索的宽度、深度、每步粒度，结果LLM实际探索的范围往往很局促。而且同一节点展开的多个子路径可能彼此差异不大，缺乏多样性，进一步限制了探索效果。另一方面，**不同路径难以共享经验**：每条路径各走各的，唯一的交集就是最后的结果评比。模型无法从一条路径学到知识应用到另一条，只能靠奖励模型或结果一致性来**间接**评估路径好坏。加之，搜索需要频繁调用LLM和评估函数，计算开销巨大，需要用各种加速技巧（如剪枝、投机解码）才能勉强运行。这些问题在当时限制了方法的进一步提升。

**阶段2：丰富动作空间**。为突破第一阶段的瓶颈，后续有模型通过引入**更丰富的推理动作空间**来增加探索能力。直白说，就是让模型在推理时有更多花样可玩，而不局限于某种固定模式。比如，rSTAR、LLaMAV-o1、HiICL-MCTS、Mulberry、g1、Thinking-Claude等模型都扩展了LLM在推理时的“行动集”。这些行动可以是更高层的规划提示、领域特定的操作，甚至允许模型调用外部工具。一旦动作空间拓宽，模型就能获取**高层次的提示**和**更多的探索线索**，从而跳出原先狭窄的推理路径。当然，这也对动作空间的**设计**提出更高要求：必须精心构造这些宏动作或特殊指令，确保它们能有效指导模型而非干扰。这一阶段的进展，让模型具备了更**综合**的结构化搜索过程，比第一阶段的死板树搜索更灵活。但它仍然是在**外部**增强模型：LLM本身参数未变，只是赋予了它更丰富的交互接口。

**阶段3：推理内生化**。随着OpenAI的o1、以及QwQ等模型出现，推理范式发生了重要变化——**慢思考的过程被内置到LLM自身上下文中**。这些模型会在生成答案时**自动地**先进行一轮探索性宏观规划，然后在同一上下文里再展开替代路径的探讨，最后通过诸如“重新思考”“验证”等机制延长推理链条，直到得到满意答案。换言之，LLM学会了在**自己的一次输出中**完成以前需要外部算法循环才能实现的探索-反思-校验。例如，OpenAI-o1模型据称会先在脑海中生成一个可能的解题方案，再在输出里表现为“让我重新检查这部分”，接着试另一个方法，一次回答里就把不同解法都尝试了。为达成这种能力，研究者尝试把外部搜索的结果**线性化**成长链CoT训练模型，让模型掌握**长程推理模式**。比如，STILL-1将搜索树的输出序列化成一条带有“重新思考”“等待”“探索新路径”等标记的长推理链，并用它来微调模型。STILL-2和Sky-T1则进一步通过知识蒸馏技术合成超长CoT数据训练模型。这些都旨在把**外部推理流程学到模型里**，使模型生成的单条回答就隐含了搜索和选择过程。

实践中，这种内生化取得很大成功：模型不再需要大量外部循环调用，就能产生类似质量的深入推理。当然，挑战也随之出现——用树搜索生成的线性长链，质量未必比人工设计的逐步提示好，一些基于蒸馏的方法生成的链条在质量上仍不如模型自己演绎出来的。此外，将慢思维拓展到**多模态**仍困难重重。如Virgo尝试把文本网链推理蒸馏到多模态模型中，但在MathVision这种需要详细视觉理解的任务上收效甚微。总的来说，第三阶段使推理能力**嵌入**模型，但也发现文本领域有效的方法难以直接复制到图像、视频等领域。

**阶段4：强化学习赋能**。最新的进展（2024-2025）表明，通过**强化学习直接对大模型进行扩展训练**，可以解锁非常高级的推理能力，甚至超越前面所有手工策略的组合。DeepSeek-R1和Kimi等模型将DeepSeek-V3这样的基础LLM用RL方法进行大规模训练，结果涌现出了**长链推理、反思推理、高级规划**等复杂行为。令人惊讶的是，这些复杂能力的出现并不需要特别复杂的算法——就是“简单的RL扩展训练”让模型自己学会了。比如DeepSeek-R1几乎没有人为添加宏动作或模板，只是在大量任务上用奖励驱动模型学习，模型便开始自发地输出动辄几百步的逻辑推导，而且能自己发现错误再修正，呈现出高度智能化的迹象。

受此启发，有研究尝试用**更简单的管线**和**极简代码**也复现这些RL效果（如SimpleRL项目）。还有R1V探索将这种RL方法拓展到多模态领域，希望训练出多模态的慢思考模型。不过结果表明，多模态领域要达到单模态RL那样的提升仍有不少难题：例如处理复杂视觉输入时，很难保持连贯的慢思考过程，也还没见到像文本那样显著的训练收益。但总体来说，**RL时代**的来临展示了前所未有的潜力：似乎只要给模型足够的任务和反馈，它自身就能“进化”出各种复杂技能，而不需要我们显式教会它每一步怎么想。

**小结**：推理LLM的演进历程是一条从**外部增强**走向**内部融合**的道路。一开始，我们把模型当成工具，用算法帮它想；接着，我们教模型学算法的思路，让它自行规划推理；如今，我们更是利用强化学习大规模释放模型潜力，让高级推理能力自然而然地出现。这一变化提高了模型推理的**效率**和**适应性**：从繁琐的多次调用到一次性自主推理，从手工设计算法到模型自创策略。同时，也看到**多模态慢思考**、**模式切换**等方面还有很多挑战尚待解决，这将在下一章探讨。

## Reference

1. Li, Z. Z., Zhang, D., Zhang, M. L., Zhang, J., Liu, Z., Yao, Y., ... & Liu, C. L. (2025). From system 1 to system 2: A survey of reasoning large language models. *arXiv preprint arXiv:2502.17419*.

## Appendix

### Monte Carlo Tree Search

MCTS 是一种**在巨大搜索空间里，用随机模拟来近似估值**、并据此**逐步扩展一棵不完全搜索树**的策略。它兼顾了“**探索**未尝试路径”和“**利用**当前看起来最优的路径”，典型用于围棋、规划、组合优化与推理。

核心思想：通过反复的“**选择→扩展→模拟→回传**”四步，逐步把搜索预算分配到更有前景的分支上。

#### **标准流程（4 步）**

记结点 $s$（状态/中间解）、动作 $a$，访问计数 $N(s)$、边计数 $N(s,a)$（或者相当于parent node的访问数）、累积价值 $W(s,a)$、均值 $Q(s,a)=W/N$。常用打分（UCT/PUCT）：
$$
\text{score}(s,a)
=
Q(s,a)
\;+\;
c \;\sqrt{\frac{\ln\!\big(\sum\nolimits_{b} N(s,b)\big)}{1+N(s,a)}}
\tag {UCT}
$$

$$
\text{score}(s,a)=Q(s,a)+c_{\text{puct}}\cdot P(s,a)\cdot \frac{\sqrt{\sum_b N(s,b)}}{1+N(s,a)}
\tag {PUCT}
$$

其中 $P(s,a)$ 是先验概率（可来自策略网络/语言模型），$c_{\text{puct}}$ 为探索系数。要在**整个时间轴**（第 1 次到第 $n$ 次）都“高概率”不乐观估计则使用UCT；反之PUCT相当于用**策略先验**做“定向探索”，并把探索强度放大（$\sqrt{N}$ 比 $\sqrt{\ln N}$ 增长快），更快把预算投向先验看好的分支。

1. **选择 (Selection)**
    从根结点出发，反复挑选拥有最大 score 的子边，直到到达一个**可扩展**的叶子（未完全展开或达终局）。

2. **扩展 (Expansion)**
    在叶子处生成若干新子结点（可全部展开或采样若干高概率动作）。

3. **模拟 (Rollout / Evaluation)**

   - 经典 MCTS：从叶子**随机/启发式地**往下模拟到终局，得到奖励 $R$；

   - 现代 AlphaZero 风格：不用随机 rollout，改用**价值函数 $V(s)$** 直接评估叶子，把 $R \leftarrow V(s)$。

4. **回传 (Backpropagation)**
   - 把该次评估值 $R$ 回传到路径上的所有边：$N(s,a)\!\leftarrow\! N(s,a)+1,\quad W(s,a)\!\leftarrow\! W(s,a)+R,\quad Q=W/N$。

重复上述 4 步，直到用完预算（模拟次数/时间）。最终**从根选择访问次数最多的动作**。

#### 为什么适合 LLM 的推理（reasoning）？

LLM 的“逐步思考”天然构成一棵**思维树**：结点是“**部分推理轨迹**”，动作是“**下一步推理**”。MCTS 能：

- **把采样预算集中在有前景的思路**（而不是盲目多样性采样）；
- **结合先验与后验**：用 LLM 给出的**先验概率 $P$** 引导扩展（像 AlphaZero 的 policy），用**校验器/打分器/程序运行结果**当作**价值/奖励**评估；
- **避免 token 级爆炸**：在“步级（thought step）”而非“字词级”上建树。