#!/usr/bin/env python3
"""
Objective Model Testing Script
=============================

This script provides objective evaluation metrics for fine-tuned models including:
- Perplexity (PPL) calculation
- Usability rate assessment
- JSON validity rate
- EvalScope integration for comprehensive evaluation

Usage:
    python obj_test_model.py
"""

import os
import sys
import math
import json
import random
import torch
from pathlib import Path
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM

# Add project root to path
current_path = Path(__file__).resolve()
project_root = current_path.parent.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

# Configuration
MODEL_DIR = "/home/awpc/studies/models/unsloth/Qwen3/FTTrained"  # Merged model or adapter+base combination
VAL_PATH = "dataset/processed_datasets/qwen3_sft_mixed/val.jsonl"  # Your validation dataset

# Alternative model paths (uncomment to use)
# MODEL_DIR = "/home/awpc/studies/models/unsloth/Qwen3/Qwen3-4B-Instruct-2507"  # Base model
# MODEL_DIR = "./models/Qwen3-4B-Instruct-2507-merged16"  # Custom merged model

# Load model and tokenizer
print(f"Loading model from: {MODEL_DIR}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_DIR, 
    torch_dtype=torch.bfloat16, 
    device_map="auto",
    trust_remote_code=True
)
model.eval()
print("Model loaded successfully!")

# ========== 1) Perplexity (PPL) Calculation ==========
def compute_ppl(jsonl_path, sample_n=200):
    """
    Compute perplexity on the validation dataset.
    
    Args:
        jsonl_path: Path to the validation dataset
        sample_n: Number of samples to evaluate (None for all)
        
    Returns:
        float: Perplexity score
    """
    print(f"Computing PPL on {jsonl_path}...")
    ds = load_dataset("json", data_files=jsonl_path)["train"]
    if sample_n and len(ds) > sample_n:
        ds = ds.select(range(sample_n))
    
    losses = []
    for i, ex in enumerate(ds):
        if i % 50 == 0:
            print(f"Processing sample {i}/{len(ds)}")
            
        # Support both message format and text format
        if "messages" in ex:
            prompt = tokenizer.apply_chat_template(ex["messages"], add_generation_prompt=False, tokenize=False)
            text = prompt  # Language modeling PPL: model the input itself
        else:
            text = ex["text"]
            
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=2048).to(model.device)
        with torch.no_grad():
            loss = model(**inputs, labels=inputs["input_ids"]).loss
        losses.append(loss.item())
    
    ppl = math.exp(sum(losses)/len(losses))
    print(f"PPL computed on {len(losses)} samples")
    return ppl

# ========== 2) Usability Rate Assessment ==========
def usability_rate(jsonl_path, sample_n=100):
    """
    Assess model usability by checking if it generates reasonable responses.
    
    Args:
        jsonl_path: Path to the validation dataset
        sample_n: Number of samples to evaluate
        
    Returns:
        float: Usability rate (0.0 to 1.0)
    """
    print(f"Computing usability rate on {jsonl_path}...")
    ds = load_dataset("json", data_files=jsonl_path)["train"]
    rows = random.sample(list(ds), min(sample_n, len(ds)))
    
    ok = 0
    for i, ex in enumerate(rows):
        if i % 20 == 0:
            print(f"Processing sample {i}/{len(rows)}")
            
        if "messages" in ex:
            prompt = tokenizer.apply_chat_template(ex["messages"], add_generation_prompt=True, tokenize=False)
        else:
            prompt = ex["text"]
            
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)
        ans = tokenizer.decode(out[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True).strip()
        
        # Simple usability criteria: response should be at least 5 characters
        # This can be customized based on your specific task requirements
        if len(ans) >= 5:
            ok += 1
    
    rate = ok / len(rows)
    print(f"Usability rate: {rate:.3f} ({ok}/{len(rows)})")
    return rate

# ========== 3) JSON Validity Rate ==========
def json_valid_rate(jsonl_path, sample_n=100):
    """
    Check the rate of valid JSON responses generated by the model.
    
    Args:
        jsonl_path: Path to the validation dataset
        sample_n: Number of samples to evaluate
        
    Returns:
        float: JSON validity rate (0.0 to 1.0)
    """
    print(f"Computing JSON validity rate on {jsonl_path}...")
    ds = load_dataset("json", data_files=jsonl_path)["train"]
    rows = random.sample(list(ds), min(sample_n, len(ds)))
    
    ok = 0
    for i, ex in enumerate(rows):
        if i % 20 == 0:
            print(f"Processing sample {i}/{len(rows)}")
            
        if "messages" in ex:
            prompt = tokenizer.apply_chat_template(ex["messages"], add_generation_prompt=True, tokenize=False)
        else:
            prompt = ex["text"]
            
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)
        ans = tokenizer.decode(out[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True).strip()
        
        try:
            json.loads(ans)
            ok += 1
        except Exception:
            pass
    
    rate = ok / len(rows)
    print(f"JSON validity rate: {rate:.3f} ({ok}/{len(rows)})")
    return rate

# ========== 4) EvalScope Integration ==========
def setup_evalscope_config():
    """
    Setup EvalScope configuration for comprehensive evaluation.
    
    Returns:
        TaskConfig: EvalScope task configuration
    """
    try:
        from evalscope import TaskConfig
        
        task_cfg = TaskConfig(
            model=MODEL_DIR,  # Model path for evaluation
            api_url="http://127.0.0.1:8000/v1/chat/completions",
            eval_type="service",

            # Specify datasets (auto-download)
            datasets=["data_collection"],
            dataset_args={
                "data_collection": {
                    "dataset_id": "modelscope/EvalScope-Qwen3-Test",
                    # Filter settings (remove chain-of-thought tags, etc.)
                    "filters": {"remove_until": "</think>"}
                }
            },

            # Evaluation batch size and generation config (adjust as needed)
            eval_batch_size=128,
            generation_config={
                "max_tokens": 3000,
                "temperature": 0.2,
                "top_p": 0.9,
                "stop": None,
            },

            # ‚ë¢ Metrics (EvalScope has built-in evaluations; you can also extend)
            metrics=["exact_match", "bleu", "rougeL"],  # Examples, adapt to your dataset
        )
        
        return task_cfg
    except ImportError:
        print("EvalScope not available. Install with: pip install evalscope")
        return None

def run_evalscope_evaluation():
    """Run EvalScope evaluation if available."""
    try:
        from evalscope import run_task
        
        task_cfg = setup_evalscope_config()
        if task_cfg:
            print("Running EvalScope evaluation...")
            run_task(task_cfg)  # Will: pull data -> call your service -> compute metrics -> output report
        else:
            print("EvalScope evaluation skipped.")
    except ImportError:
        print("EvalScope not available. Skipping comprehensive evaluation.")

def main():
    """Main evaluation function."""
    print("="*60)
    print("üîç OBJECTIVE MODEL EVALUATION")
    print("="*60)
    
    # Check if validation file exists
    if not os.path.exists(VAL_PATH):
        print(f"‚ùå Validation file not found: {VAL_PATH}")
        print("Please check the path and ensure the validation dataset exists.")
        return
    
    print(f"üìä Evaluating model: {MODEL_DIR}")
    print(f"üìÅ Validation data: {VAL_PATH}")
    print()
    
    # Run basic evaluations
    try:
        ppl = compute_ppl(VAL_PATH, sample_n=200)
        print(f"‚úÖ Perplexity: {ppl:.3f}")
    except Exception as e:
        print(f"‚ùå PPL calculation failed: {e}")
        ppl = None
    
    try:
        usability = usability_rate(VAL_PATH, sample_n=100)
        print(f"‚úÖ Usability Rate: {usability:.3f}")
    except Exception as e:
        print(f"‚ùå Usability calculation failed: {e}")
        usability = None
    
    try:
        json_valid = json_valid_rate(VAL_PATH, sample_n=100)
        print(f"‚úÖ JSON Validity Rate: {json_valid:.3f}")
    except Exception as e:
        print(f"‚ùå JSON validity calculation failed: {e}")
        json_valid = None
    
    print()
    print("="*60)
    print("üìã EVALUATION SUMMARY")
    print("="*60)
    if ppl is not None:
        print(f"Perplexity: {ppl:.3f}")
    if usability is not None:
        print(f"Usability Rate: {usability:.3f}")
    if json_valid is not None:
        print(f"JSON Validity Rate: {json_valid:.3f}")
    
    # print()
    # print("üöÄ Running EvalScope comprehensive evaluation...")
    # run_evalscope_evaluation()

if __name__ == "__main__":
    main()