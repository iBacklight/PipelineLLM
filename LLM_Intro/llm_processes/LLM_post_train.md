# LLM Post-Train

常见的 **post-train 组件**（可单独或组合使用）：

1. **继续预训练（CPT）**：在新增语料上继续 CLM（补知识/语言/代码/领域风格）。
2. **SFT**：高质量指令/工具/安全示范，教会“怎么做、怎么说、什么不能做”。
3. **偏好优化（Preference Optimization）**
   - **离线、非 RL**：DPO/IPO/KTO/RRHF… 用**比较/偏好对**直接优化“更喜欢的回答”，稳定、工程简单。
4. **强化学习（RL）/RLHF/RLAIF/PRM**
   - **RLHF**：人类偏好→奖励模型（RM），在线采样 + PPO 等优化最终奖励。
   - **RLAIF**：用强模型代替人类打分。
   - **PRM（过程奖励）/o1-style**：对“思维过程/草稿”打分，奖励可分配到中间步骤。
   - **在线自博弈/工具成功率奖励/成本奖励**：直接优化业务 KPI。
5. **持续学习（Continual Learning, CL）**
   - 在**时间轴上不断纳入新数据/新目标**，同时**不遗忘**旧能力；含评测、回放、参数管理与安全回归。

常见的流水线（从左到右复杂度递增）：

**CPT → SFT →（DPO/IPO…）→ RL（PPO/GRPO…）→ PRM/过程奖励 → 在线 CL（含回放与监控）**

- **对齐（Alignment）**：总称，让模型行为与人类价值/产品 KPI 一致；贯穿 DPO/RLHF/安全策略等。
- **偏好学习（Preference Learning）**：主要指 **DPO/IPO/KTO/RRHF** 这类**不需要在线 RL**的方法。
- **RLHF**：人评/AI 评→**奖励模型 RM**→在线采样→**PPO/变体**更新策略。
- **继续学习（Continual/Lifelong）**：把以上步骤做成**循环**：新数据进入→（可）CPT/SFT/DPO/RL→回放与回归测试→上线。

