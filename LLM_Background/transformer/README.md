# SimpleTokenizer

ä¸€ä¸ªç®€å•è€ŒåŠŸèƒ½å®Œæ•´çš„æ–‡æœ¬åˆ†è¯å™¨ï¼Œä¸“ä¸ºTransformeræ¨¡å‹è®¾è®¡ã€‚æ”¯æŒè¯æ±‡è¡¨æ„å»ºã€æ–‡æœ¬ç¼–ç /è§£ç ã€ä½ç½®ç¼–ç å’ŒåµŒå…¥ç”Ÿæˆã€‚

## ğŸš€ ç‰¹æ€§

- **ç®€å•åˆ†è¯**ï¼šåŸºäºç©ºæ ¼çš„åˆ†è¯ï¼Œæ”¯æŒæ–‡æœ¬æ¸…æ´—
- **è¯æ±‡è¡¨æ„å»º**ï¼šä»è¯­æ–™åº“è‡ªåŠ¨æ„å»ºè¯æ±‡è¡¨
- **ç‰¹æ®Šæ ‡è®°**ï¼šæ”¯æŒ `<pad>`, `<bos>`, `<eos>`, `<unk>` ç‰¹æ®Šæ ‡è®°
- **ä½ç½®ç¼–ç **ï¼šæ”¯æŒæ­£å¼¦ä½ç½®ç¼–ç å’Œå¯å­¦ä¹ ä½ç½®ç¼–ç 
- **è®¾å¤‡æ”¯æŒ**ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨CUDA/CPU
- **æ‰¹å¤„ç†**ï¼šæ”¯æŒæ‰¹é‡æ–‡æœ¬å¤„ç†å’Œå¡«å……
- **åµŒå…¥ç”Ÿæˆ**ï¼šç»“åˆè¯åµŒå…¥å’Œä½ç½®åµŒå…¥

## ğŸ“¦ å®‰è£…

```bash
# ç¡®ä¿å·²å®‰è£…PyTorch
pip install torch
```

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•

```python
from simple_tokenizer import SimpleTokenizer

# 1. å‡†å¤‡è¯­æ–™åº“
corpus = [
    "Transformer is almost the origin of large models",
    "All modern LMs are based on the transformer architecture",
    "Attention is all you need",
]

# 2. åˆ›å»ºåˆ†è¯å™¨
tokenizer = SimpleTokenizer(
    corpus=corpus,
    max_vocab=10000,
    d_model=256,
    max_pos_len=1024
)

# 3. ç¼–ç æ–‡æœ¬
text = "attention is all you need"
encoded = tokenizer.encode(text)
print(f"Encoded: {encoded}")

# 4. è§£ç 
decoded = tokenizer.decode(encoded)
print(f"Decoded: {decoded}")
```

### æ‰¹å¤„ç†

```python
# æ‰¹é‡å¤„ç†æ–‡æœ¬
batch_texts = [
    "Transformer is the origin",
    "attention is all you need"
]

# è·å–token IDså’Œæ³¨æ„åŠ›æ©ç 
input_ids, attention_mask = tokenizer.collate_fn(batch_texts, max_len=12)
print(f"Input IDs shape: {input_ids.shape}")      # [2, 12]
print(f"Attention mask shape: {attention_mask.shape}")  # [2, 12]

# è·å–åµŒå…¥
embeddings = tokenizer.get_embeddings(input_ids)
print(f"Embeddings shape: {embeddings.shape}")    # [2, 12, 256]
```

## ğŸ—ï¸ ç±»ç»“æ„

### SimpleTokenizer

ä¸»è¦çš„åˆ†è¯å™¨ç±»ï¼ŒåŒ…å«æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ã€‚

#### åˆå§‹åŒ–å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | æè¿° |
|------|------|--------|------|
| `corpus` | List[str] | None | ç”¨äºæ„å»ºè¯æ±‡è¡¨çš„æ–‡æœ¬è¯­æ–™åº“ |
| `max_vocab` | int | 10000 | æœ€å¤§è¯æ±‡è¡¨å¤§å° |
| `d_model` | int | 256 | åµŒå…¥ç»´åº¦ |
| `max_pos_len` | int | 1024 | æœ€å¤§ä½ç½®é•¿åº¦ |

#### ä¸»è¦æ–¹æ³•

##### `tokenize(text: str) -> List[str]`
å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚

```python
tokens = tokenizer.tokenize("Hello world!")
# è¿”å›: ["hello", "world"]
```

##### `encode(text: str, add_bos=True, add_eos=True) -> List[int]`
å°†æ–‡æœ¬ç¼–ç ä¸ºtoken IDåºåˆ—ã€‚

```python
ids = tokenizer.encode("hello world")
# è¿”å›: [1, 123, 456, 2]  # [<bos>, hello, world, <eos>]
```

##### `decode(ids: List[int]) -> str`
å°†token IDåºåˆ—è§£ç ä¸ºæ–‡æœ¬ã€‚

```python
text = tokenizer.decode([1, 123, 456, 2])
# è¿”å›: "hello world"
```

##### `collate_fn(texts: List[str], max_len=None) -> Tuple[Tensor, Tensor]`
æ‰¹é‡å¤„ç†æ–‡æœ¬ï¼Œè¿”å›å¡«å……åçš„token IDså’Œæ³¨æ„åŠ›æ©ç ã€‚

```python
input_ids, attention_mask = tokenizer.collate_fn(texts, max_len=10)
# input_ids: [batch_size, max_len]
# attention_mask: [batch_size, max_len]
```

##### `get_embeddings(input_ids: Tensor) -> Tensor`
è·å–æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤ºï¼ˆè¯åµŒå…¥ + ä½ç½®åµŒå…¥ï¼‰ã€‚

```python
embeddings = tokenizer.get_embeddings(input_ids)
# è¿”å›: [batch_size, seq_len, d_model]
```

## ğŸ” æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### 1. åˆ†è¯è¿‡ç¨‹

```python
def tokenize(self, text):
    text = text.lower()  # è½¬å°å†™
    text = re.sub(r"[^a-z\s]", "", text)  # åªä¿ç•™å­—æ¯å’Œç©ºæ ¼
    return text.split()  # æŒ‰ç©ºæ ¼åˆ†è¯
```

### 2. è¯æ±‡è¡¨æ„å»º

```python
# ç‰¹æ®Šæ ‡è®° + é«˜é¢‘è¯æ±‡
vocab = ["<pad>", "<bos>", "<eos>", "<unk>"] + most_common_tokens
```

### 3. ä½ç½®ç¼–ç 

æ”¯æŒä¸¤ç§ä½ç½®ç¼–ç æ–¹å¼ï¼š

#### æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆé»˜è®¤ï¼‰
```python
# é¢„è®¡ç®—æ­£å¼¦ä½ç½®ç¼–ç 
pos_embeddings = self._create_sinusoidal_embeddings()
```

#### å¯å­¦ä¹ ä½ç½®ç¼–ç 
```python
# å¯å­¦ä¹ çš„åµŒå…¥å±‚
self.pos_emb = nn.Embedding(max_pos_len, d_model)
```

### 4. è®¾å¤‡æ”¯æŒ

```python
# è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ‰€æœ‰å¼ é‡è‡ªåŠ¨ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
input_ids = torch.tensor(input_ids, device=self.device)
```

## ğŸ“Š æ•°æ®æµç¨‹

```mermaid
graph TD
    A[åŸå§‹æ–‡æœ¬] --> B[åˆ†è¯]
    B --> C[è¯æ±‡è¡¨æŸ¥æ‰¾]
    C --> D[æ·»åŠ ç‰¹æ®Šæ ‡è®°]
    D --> E[æ‰¹å¤„ç†å¡«å……]
    E --> F[è¯åµŒå…¥]
    F --> G[ä½ç½®åµŒå…¥]
    G --> H[æœ€ç»ˆåµŒå…¥]
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### 1. ä¸Transformeræ¨¡å‹ç»“åˆ

```python
# åˆ›å»ºåˆ†è¯å™¨
tokenizer = SimpleTokenizer(corpus=corpus, d_model=256)

# åˆ›å»ºTransformeræ¨¡å‹
from transformer_model import TransformerModel
model = TransformerModel(
    vocab_size=tokenizer.get_vocab_size(),
    d_model=256,
    nhead=8,
    num_encoder_layers=6,
    num_decoder_layers=6
)

# å¤„ç†æ–‡æœ¬
input_ids, attention_mask = tokenizer.collate_fn(texts)
output = model(input_ids, target_ids)
```

### 2. æ–‡æœ¬é¢„å¤„ç†

```python
# æ‰¹é‡æ–‡æœ¬é¢„å¤„ç†
def preprocess_texts(texts):
    input_ids, attention_mask = tokenizer.collate_fn(texts)
    embeddings = tokenizer.get_embeddings(input_ids)
    return embeddings, attention_mask
```

## âš™ï¸ é…ç½®é€‰é¡¹

### è¯æ±‡è¡¨è®¾ç½®
```python
tokenizer = SimpleTokenizer(
    max_vocab=50000,  # æ›´å¤§çš„è¯æ±‡è¡¨
    corpus=large_corpus
)
```

### åµŒå…¥ç»´åº¦
```python
tokenizer = SimpleTokenizer(
    d_model=512,  # æ›´å¤§çš„åµŒå…¥ç»´åº¦
    max_pos_len=2048  # æ›´é•¿çš„åºåˆ—æ”¯æŒ
)
```

### ä½ç½®ç¼–ç 
```python
# åœ¨_init_embeddingsä¸­ä¿®æ”¹
self.use_sinusoidal_pos = False  # ä½¿ç”¨å¯å­¦ä¹ ä½ç½®ç¼–ç 
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰åˆ†è¯è§„åˆ™

```python
class CustomTokenizer(SimpleTokenizer):
    def tokenize(self, text):
        # è‡ªå®šä¹‰åˆ†è¯é€»è¾‘
        text = text.lower()
        # æ·»åŠ è‡ªå®šä¹‰æ¸…æ´—è§„åˆ™
        text = re.sub(r"[^a-z0-9\s]", "", text)
        return text.split()
```

### å¤„ç†ç‰¹æ®Šæ ‡è®°

```python
# è·å–ç‰¹æ®Šæ ‡è®°ID
special_ids = tokenizer.get_special_token_ids()
print(f"PAD ID: {special_ids['pad_id']}")
print(f"BOS ID: {special_ids['bos_id']}")
print(f"EOS ID: {special_ids['eos_id']}")
print(f"UNK ID: {special_ids['unk_id']}")
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. é¢„è®¡ç®—ä½ç½®ç¼–ç 
```python
# ä½ç½®ç¼–ç åœ¨åˆå§‹åŒ–æ—¶é¢„è®¡ç®—ï¼Œé¿å…é‡å¤è®¡ç®—
pos_embeddings = self._create_sinusoidal_embeddings()
```

### 2. è®¾å¤‡ä¼˜åŒ–
```python
# æ‰€æœ‰å¼ é‡è‡ªåŠ¨åœ¨æ­£ç¡®è®¾å¤‡ä¸Šåˆ›å»º
input_ids = torch.tensor(input_ids, device=self.device)
```

### 3. æ‰¹å¤„ç†æ•ˆç‡
```python
# å‘é‡åŒ–æ‰¹å¤„ç†ï¼Œæé«˜æ•ˆç‡
input_ids, attention_mask = tokenizer.collate_fn(texts)
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: å¦‚ä½•å¤„ç†æœªçŸ¥è¯æ±‡ï¼Ÿ
A: æœªçŸ¥è¯æ±‡ä¼šè¢«æ˜ å°„åˆ° `<unk>` æ ‡è®°ã€‚

### Q: å¦‚ä½•è°ƒæ•´åºåˆ—é•¿åº¦ï¼Ÿ
A: ä½¿ç”¨ `max_len` å‚æ•°æ§åˆ¶æœ€å¤§åºåˆ—é•¿åº¦ï¼Œè¶…å‡ºéƒ¨åˆ†ä¼šè¢«æˆªæ–­ã€‚

### Q: å¦‚ä½•æ·»åŠ æ–°çš„ç‰¹æ®Šæ ‡è®°ï¼Ÿ
A: ä¿®æ”¹ `special_tokens` åˆ—è¡¨å¹¶é‡æ–°æ„å»ºè¯æ±‡è¡¨ã€‚

### Q: å†…å­˜ä½¿ç”¨è¿‡å¤šæ€ä¹ˆåŠï¼Ÿ
A: å‡å°‘ `max_vocab` å’Œ `max_pos_len` å‚æ•°ã€‚

## ğŸ“ ç¤ºä¾‹ä»£ç 

å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹è¯·å‚è€ƒ `simple_tokenizer.py` æ–‡ä»¶æœ«å°¾çš„ç¤ºä¾‹ä»£ç ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªåˆ†è¯å™¨ï¼

## ğŸ“„ è®¸å¯è¯

MIT License
