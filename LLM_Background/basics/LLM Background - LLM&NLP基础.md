# LLM Background - LLM/NLP基础

[TOC]



## 词向量

### 规则和静态词向量训练方法

1. 规则： **one-hot**，e.g. `[0,1,0,0,...]`
   - 维度灾难（维度等于词表长度）
   - 无法度量词之间相似度
2. 静态：**Word2Vec**, 向量与向量矩阵，词+高维表达
   - **CBOW**： 窗口内的两边词预测中间词 + softmax输出
   - **Skip-Gram**：窗口内中间词预测两边词 + softmax输出
   - **Negative Sampling 负采样**：滑窗内样本为正样本，滑窗以外为负样本，使用正样本和少量负样本训练词向量权重+sigmoid函数输出（>0.5 =1, 否则输出0）

以上静态词向量时代 (Static Embeddings)的一些方法，它们的参数量大概在几百万 ~ 几千万 (取决于词表大小)。采用这些方法的典型词向量模型有：

| **算法/方法** | **典型模型/产物**           | **架构核心**           | **训练简述**                                                 |
| ------------- | --------------------------- | ---------------------- | ------------------------------------------------------------ |
| Word2Vec      | Google News Vectors (300维) | 浅层神经网络 (1层隐层) | 如前所述，Skip-Gram + 负采样。                               |
| GloVe         | Stanford GloVe              | 矩阵分解               | 基于全局共现矩阵，统计词与词同时出现的概率。                 |
| FastText      | Facebook FastText           | 类似 Word2Vec          | 在 Word2Vec 基础上加入了**N-gram (子词)**。例如把 "Apple" 拆成 "Ap", "pp", "le" 训练，能解决生僻词（OOV）问题。 |



### 现代大模型词向量模型 (Contextual Embeddings)

现代LLM Embedding的最大特征是动态向量， 可以理解成——同一个词在不同语境下向量不同，可以用于生成整个句子或段落的向量。目前的主流架构底层大多是基于Transformer Encoder的 **BERT** 或者 **RoBERTa** 的变体，参数量大约在几亿 ~ 几千亿 (7B, 70B...)。

- **模型表达**：输入一句话，经过 12 层（或更多）Transformer Block，取最后一层的 `[CLS]` 符号向量或者所有 Token 的平均值 (Mean Pooling) 作为这句话的 Embedding。

对比学习 (Contrastive Learning) 是现代 Embedding 模型训练的核心，实际上它是 Word2Vec 负采样思想进阶版本。

- **逻辑**： Word2Vec 是拉近“词”与“上下文词”的距离，而现代 Embedding 是拉近“查询（Query）”与“正向文档（Positive）” 的距离，推开 “负向文档（Negative）” 的距离。

- **具体流程 (InfoNCE Loss)**：

  1. **Anchor (锚点)**：用户的问题，例如 "Python怎么学？"

  2. **Positive (正例)**：正确的答案，例如 "Python 学习路线图..."

  3. **Negative (负例)**：Hard Negatives (困难负例)。模型不再随机选不相关的词，而是特意选看起来像但其实不对的句子。

     - *E.g.*："Python是一种蛇类生物..." (字面重合度高，但语义无关)。

  4. **目标**：在高维空间里，把 Anchor 和 Positive 拉近，并把 Anchor 和 Negative 踢远。

  5. **损失函数**：**InfoNCE Loss** (Information Noise-Contrastive Estimation)。假设一个 Batch 里有 $N$ 个样本。 对于第 $i$ 个样本 $q_i$（Query），它对应的正例是 $p_i$（Positive）。 那么，Batch 里剩下的 $N-1$ 个样本的文档（甚至包括其他的 Query），对于 $q_i$​ 来说，统统都是负例 (Negatives)。InfoNCE Loss 公式：

     
     $$
     L_i = - \log \frac{e^{\text{sim}(q_i, p_i) / \tau}}{\sum_{j \in \text{Batch}} e^{\text{sim}(q_i, d_j) / \tau}}
     $$

以下是一些现代词向量模型的例子：

| **开源模型系列**                       | **机构**        | **核心架构**         | **训练核心**                                                 | **特点 & 适用场景**                                          |
| -------------------------------------- | --------------- | -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| BGE Series (bge-m3, bge-large-en-v1.5) | BAAI (北京智源) | BERT-like (RetroMAE) | 难负例挖掘 (Hard Negative) 多语言混合训练                    | **中文最强之一**。 BGE-M3 支持多粒度（稠密+稀疏）检索，支持 8k 长度。 |
| E5 Series (e5-mistral-7b, e5-large-v2) | Microsoft       | BERT or Mistral      | 指令微调 (Instruction) 强调用 `query:` 和 `passage:` 前缀区分输入。 | **通用性强**。 v2 版本是纯 Encoder，Mistral 版本是基于 LLM 的 Decoder。 |
| Jina Embeddings (jina-embeddings-v2)   | Jina AI         | BERT (with ALiBi)    | 超长上下文 (8k) 使用了 ALiBi 位置编码，并未简单的截断。      | **长文本处理**。 适合法律文档、长篇论文的 RAG 检索。         |
| Nomic Embed (nomic-embed-text-v1.5)    | Nomic           | BERT                 | Matryoshka (套娃学习) 动态维度，长上下文。                   | 高性能且灵活。 采用了和 OpenAI text-embedding-3 相似的技术   |

| **闭源模型名称**                  | **机构**             | **维度**           | **训练核心**                                                 | **特点 & 适用场景**                                          |
| --------------------------------- | -------------------- | ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| text-embedding-3 (small / large)  | OpenAI               | 1536 / 3072 (可变) | Matryoshka (俄罗斯套娃)：训练时强制让前 $N$ 维也能包含完整语义。 | **工业界默认标准**。 支持缩减维度（如从 3072 缩到 256）以节省数据库空间，性能损失极小。 |
| Voyage (voyage-large-2)           | Voyage AI (Stanford) | 1024+              | 领域特化优化：针对代码、金融、法律等特定领域做了深度对比学习。 | **RAG 专用**。 在很多 RAG 检索任务上优于 OpenAI，但价格较贵。 |
| Cohere Embed (embed-english-v3.0) | **Cohere**           | 1024               | Hard Negative + Rerank 与其 Rerank 模型配合使用效果极佳。    | **企业级搜索**。 特别针对“搜索查询”场景优化。                |

| **LLM基座词向量模型** | **机构** | **基座**       | **训练核心**                                                 | **特点 & 适用场景**                                          |
| --------------------- | -------- | -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| NV-Embed-v2           | NVIDIA   | Mistral / Qwen | 使用 LLM 的强大理解能力，加上双向注意力机制 (Bidirectional Attention)。 | **MTEB 霸榜模型**。 优点：理解力爆表；缺点：推理极慢，显存占用大 (7B起步)。 |
| Qwen-Embed            | Alibaba  | Qwen           | 同上，利用 Qwen 强大的多语言能力。                           | **中文理解力极强**。 适合资源充足、追求极致精度的场景。      |



## Token 和 Tokenizer

简单来讲，**Token** 是模型处理文本的最小单位。我们眼中的文本是‘字’或‘词’，然而模型无法直接理解这些字词组合，只能理解数字。因此，Token 充当了中间的桥梁：它值它不仅仅是简单的字词，而是被切分后的**文本片段**（可能是字、词或词根）。而 **Tokenizer** 是一个预处理工具，它负责两件事：先将文本**切分**成 Token，再查表将它们**转换**为模型能读懂的数字 ID。

### Token

Again, **Token** 是模型处理文本的最小单位。具体来说

**它不仅仅是单词**：

- 对于英文 `Apple`，它是一个 Token。
- 但对于 `Unbelievable`，它可能被切分成 `Un` + `believ` + `able` 三个 Token（词根词缀）。

**它也不仅仅是汉字**：

- 对于中文，通常一个字就是一个 Token（如 `我`、`爱`）。
- 但对于常用词，可能合并成一个 Token（如 `人工智能` 在某些字典里可能就是一个 Token）。

从经验来说，在英文中，1 个 Token $\approx$ 0.75 个单词（或者说 1000 个 Token 约等于 750 个英文单词）。在中文中，1 个 Token $\approx$ 1.5 ~ 2 个汉字（取决于分词器的粒度）。我们要具体结合不同算法进行分析。

那么为什么不直接用字或者词作为token的单位呢？这也是接下来的要讨论的 Tokenizer 技术进化的核心驱动力，也是为了解决 **“词表大小” vs “序列长度”** 的平衡问题。

| **方案**                       | **逻辑**                        | **词表大小 (Vocabulary)**        | **序列长** | **缺点**                                                     |
| ------------------------------ | ------------------------------- | -------------------------------- | ---------- | ------------------------------------------------------------ |
| **Character-level** (按字符切) | 把 `apple` 切成 `a, p, p, l, e` | 极小 (26字母+符号 $\approx$ 100) | 极长       | 语义稀疏，模型很难学到 `a` 和 `p` 组合起来的意义；计算量爆炸。 |
| **Word-level** (按单词切)      | 把 `apple` 切成 `apple`         | 极大 (百万级)                    | 最短       | **OOV (Out of Vocabulary)** 问题。遇到生僻字直接报错或变成 `<UNK>`。 |
| **Subword-level** (按子词切)   | 当前主流 (BPE)                  | 适中 (3万~10万)                  | 适中       | 平衡点。常见词保留完整，生僻词拆解成词根。                   |

### Tokenizer

Tokenizer实际上是一种把文本转换为token的工具或者算法。它的工作只有两步：

1. **切分 (Tokenize)**：把长长的文本切成一个个 Token。
2. **索引 (Index)**：查阅“词表 (Vocabulary)”，把 Token 变成一串数字 ID。

我们再来盘点一下主流的tokenizer算法：

#### 1. BPE (Byte Pair Encoding, 字节对编码)

这是目前最统治级地位的算法，几乎所有现代 Decoder-only 大模型都在使用它的变体。它突出的特点是合并后的token可以继续参与后续的合并。

- **核心逻辑**：**自底向上（Bottom-up）的统计合并**。

  1. **初始化**：将词表初始化为所有基础字符（或字节）。
  2. **统计**：在语料库中统计相邻字符对（Pair）出现的频率。
  3. **合并**：将频率最高的字符对合并成一个新的 Token（例如 `t` + `h` $\rightarrow$ `th`）。
  4. **迭代**：重复上述过程，直到词表大小达到预设值（如 32k, 50k, 128k），或者所有词的频率均相等。

  > - 实例：
  >
  > 假设我们的语料库中，“e” 和 “s” 经常挨着出现，“es” 和 “t” 也经常挨着出现。统计发现 est 是个极高频的后缀。
  >
  > **输入单词**：`"hood"和"wood"`
  >
  > **切分过程**：
  >
  > 1. **初始状态**：先把所有字母拆开。
  >
  >    - `h` `o` `o` `d`
  >    - `w` `o` `o` `d`
  >
  > 2.  **第一轮合并**：统计发现，`o` 和 `o` 总是粘在一起（出现了2次，频率最高）, BPE 把 `o` + `o` 组合，变成新模块 **`oo`**
  >
  > 3. **第二轮合并**：再次统计，发现新模块 `oo` 和后面的 `d` 总是粘在一起。 **操作**：把 `oo` + `d` 粘起来，变成更大的模块 **`ood`**。
  >
  >    - `h` `ood`
  >    - `w` `ood`
  >
  > 4. **最终状态**：
  >
  >    ```python
  >    ['h', 'w', 'o', 'd', 'oo', 'ood']
  >    ```
  >
  > **特点**：不做特殊标记，直接拼起来就是原词。如果遇到词表中完全没见过的字符（比如生僻Emoji），会直接变成 `<UNK>`（Unknown）。

- **关键变体：Byte-level BPE (BBPE)**

  - 为了彻底解决生僻字符和多语言问题，现代模型不再以 Unicode 字符为基础，而是以 **UTF-8 字节** 为基础进行 BPE。
  - BBPE初始化是 **256 个基础字节** (`0x00` 到 `0xFF`)而不是字符本身，这意味着任何文本（包括 Emoji、二进制代码）都能被编码，实现了真正的“无 OOV”。

  > - 实例：
  >
  > 中文字符 “我” (UTF-8编码: `0xE6`, `0x88`, `0x91`)。在 BBPE 眼里，它起初不是一个字，而是三个字节 Token：`<0xE6>`, `<0x88>`, `<0x91>`。
  >
  > **合并过程**：
  >
  > 1. 发现 `<0xE6>` 和 `<0x88>` 经常在一起 $\rightarrow$ 合并成 `<0xE688>`。
  > 2. 发现 `<0xE688>` 和 `<0x91>` 经常在一起 $\rightarrow$ 合并成 `<0xE68891>` 
  > 3. 形成“我”字的最终 Token `<0xE68891>`。

- **代表模型**：

  - **GPT 系列 (GPT-2, GPT-3, GPT-4, ChatGPT)**：使用 Byte-level BPE。
  - **Llama 系列 (Llama 2, Llama 3)**：配合 SentencePiece 实现的 BPE。
  - **DeepSeek / Qwen**：均采用大规模词表的 Byte-level BPE。



#### 2. WordPiece

这是 Google 在 BERT 时代引入的算法，与 BPE 非常相似，但在合并策略上引入了概率思维。

- **核心逻辑**：**基于语言模型概率的合并**。

  - 它也是自底向上合并，但它不只是看“频率最高”，而是选择能够**最大化语言模型似然值（Likelihood）**的合并方式。
  - 简单来说，它会评估：如果把 A 和 B 合并，是否比不合并更能提高预测文本的准确率？
  - **特征**：WordPiece 分出的 Subword 通常会带有特殊前缀（如 `##`）来表示它是词的后缀（例如 `playing` $\rightarrow$ `play`, `##ing`）。

  > - 实例：
  >
  > 假设我们有一个语料库，里面充满了英文单词。我们要决定下一轮是合并字符 `t` 和 `h`，还是合并字符 `z` 和 `z`。
  >
  > **语料库统计数据：**
  >
  > 1. **"th" 组合**：出现在 *the, that, this, with, math...*
  >    - `th` 一起出现的次数：**1000 次** (非常高频)
  >    - `t` 单独出现的总次数：**10,000 次** (t 无处不在)
  >    - `h` 单独出现的总次数：**5,000 次** (h 也很常见)
  > 2. **"zz" 组合**：出现在 *puzzle, pizza, jazz, blizzard...*
  >    - `zz` 一起出现的次数：**100 次** (相对低频)
  >    - `z` 单独出现的总次数：**120 次** (z 本身就很稀有，且绝大多数时候是成对出现)
  >
  > WordPiece的评分公式：
  > 
  > $$
  > Score = \frac{\text{Freq}(AB)}{\text{Freq}(A) \times \text{Freq}(B)}
  > $$
  > 
  > 那么，
  >
  > - 对于 "th"： $Score_{th} = \frac{1000}{10000 \times 5000} = \frac{1000}{50,000,000} = \mathbf{0.00002}$
  > - 对于 "zz"：$Score_{zz} = \frac{100}{120 \times 120} = \frac{100}{14,400} \approx \mathbf{0.0069}$
  >
  > 所以，WordPiece 选择合并 `z` + `z` $\rightarrow$ `zz`， 而BPE则会只根据出现频率选择合并`th`

- **代表模型**：

  - **BERT 系列 (BERT, DistilBERT)**。
  - **Electra**。
  - **MobileBERT**。



#### 3. Unigram LM (Unigram Language Model)

与 BPE 和 WordPiece 截然不同，它采用的是“减法”思维。

- **核心逻辑**：**自顶向下（Top-down）的概率剪枝**。

  1. **初始化**：构建一个极其巨大的初始词表（包含所有可能的子串）。
  2. **评估**：使用一个简单的 Unigram 语言模型计算 loss。
  3. **剪枝**：计算如果去掉某个 Token，总 Loss 会增加多少。将那些对 Loss 影响最小（即最没用）的 Token 删掉。
  4. **迭代**：直到词表缩减到预设大小。

- **优势**：由于是基于概率图模型，它在分词时可以输出多种切分可能及其概率（用于通过 Subword Regularization 增强模型鲁棒性）。

  >- 实例：
  >
  >**输入单词**：`"unnatural"` (不自然的)
  >
  >切分方案竞争：
  >
  >模型会计算不同切分路径的联合概率（Joint Probability）：
  >
  >- **方案 A**：`["un", "natural"]`
  >  - P("un") = 0.05, P("natural") = 0.002
  >  - Score = -log(0.05) + -log(0.002) $\approx$ **最优**
  >- **方案 B**：`["unna", "tural"]`
  >  - P("unna") = 0.00001 (极低频), P("tural") = 0.001
  >  - Score = **极差**
  >
  >**最终结果**：
  >
  >```python
  >["un", "natural"]
  >```
  >
  >**特点**：它是唯一一个会有“备选方案”的算法。在训练时，为了增加鲁棒性，模型有时会故意不选最优切分（Subword Regularization），比如偶尔切成 `["u", "n", "natural"]` 给模型看，强迫模型学会更多组合。

- **代表模型**：

  - **T5 (Text-to-Text Transfer Transformer)**。
  - **mBART**。
  - **ALBERT**。

总结如下表：

| **算法**   | **核心思维**    | **合并/剪枝依据**               | **典型应用模型**           |
| ---------- | --------------- | ------------------------------- | -------------------------- |
| BPE / BBPE | 合并 (自底向上) | 频率 (谁出现多合并谁)           | GPT, Llama, DeepSeek, Qwen |
| WordPiece  | 合并 (自底向上) | 似然值 (谁能提升预测概率合并谁) | BERT, DistilBERT           |
| Unigram    | 剪枝 (自顶向下) | Loss 贡献度 (谁没用就删谁)      | T5, mBART                  |



#### 4. Tiktokenizer

Tiktoken 是 OpenAI 开源的一个高性能 BPE (Byte Pair Encoding) 分词器（Tokenizer）。