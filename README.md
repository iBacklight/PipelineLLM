# PipelineLLM

> **ä»é¢„è®­ç»ƒåˆ°æŒç»­å¯¹é½ï¼šå¤§è¯­è¨€æ¨¡å‹åè®­ç»ƒå…¨æµç¨‹å®æˆ˜**

<p align="center">
  <img src="https://img.shields.io/badge/Language-ä¸­æ–‡-red" alt="Language">
  <img src="https://img.shields.io/badge/Framework-PyTorch-orange" alt="Framework">
  <img src="https://img.shields.io/badge/License-MIT-green" alt="License">
  <img src="https://img.shields.io/badge/Status-Active-brightgreen" alt="Status">
</p>

---

## é¡¹ç›®ç®€ä»‹

![pipelinellm](pipelineLLM.png)

**PipelineLLM** æ˜¯ä¸€ä¸ªç³»ç»Ÿæ€§çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åè®­ç»ƒå­¦ä¹ é¡¹ç›®ï¼Œæ¶µç›–ä»ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰åˆ°åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ã€å¼ºåŒ–å­¦ä¹ ï¼ˆRLHF/PPO/GRPOï¼‰å†åˆ°æŒç»­å­¦ä¹ ï¼ˆContinual Learning)çš„**å®Œæ•´æŠ€æœ¯æ ˆ**ã€‚

åœ¨å¤§æ¨¡å‹æ—¶ä»£ï¼Œ**é¢„è®­ç»ƒåªæ˜¯èµ·ç‚¹**ã€‚ä¸€ä¸ªçœŸæ­£å¯ç”¨ã€å¯ä¿¡ã€å¯æ§çš„ LLMï¼Œéœ€è¦ç»å†ï¼š

```
é¢„è®­ç»ƒ â†’ SFTï¼ˆå­¦ä¼šè¯´è¯ï¼‰â†’ DPO/RLï¼ˆè¯´å¾—æ›´å¥½ï¼‰â†’ æŒç»­å­¦ä¹ ï¼ˆä¸æ—¶ä¿±è¿›ï¼‰
```

æœ¬é¡¹ç›®ä»¥**ç†è®º+å®æˆ˜**çš„æ–¹å¼ï¼Œå¸¦ä½ èµ°å®Œè¿™æ¡å®Œæ•´çš„æµæ°´çº¿ã€‚

_ä½œè€…æ³¨ï¼š1ï¼‰æœ¬é¡¹ç›®ä¸AIå…±åŒå¼€å‘å®Œæˆï¼Œéƒ¨åˆ†æ–‡æ¡£å’Œä»£ç ç”±AIæ’°å†™ï¼›2ï¼‰éƒ¨åˆ†æ–‡æ¡£è¿˜åœ¨åŠªåŠ›å®Œå–„ä¸­ï¼Œæ¬¢è¿PRï¼_

---

## é¡¹ç›®æ„ä¹‰

### ä¸ºä»€ä¹ˆéœ€è¦ PipelineLLMï¼Ÿ

| ç—›ç‚¹ | PipelineLLM çš„è§£å†³æ–¹æ¡ˆ |
|------|------------------------|
| ç¢ç‰‡åŒ–å­¦ä¹ ï¼Œéš¾ä»¥å½¢æˆä½“ç³» | 7 å¤©è¯¾ç¨‹ï¼Œç¯ç¯ç›¸æ‰£ï¼Œä»åŸºç¡€åˆ°å‰æ²¿ |
| ç†è®ºä¸å®è·µè„±èŠ‚ | æ¯ä¸ªæ¨¡å—é…å¥—å¯è¿è¡Œä»£ç  |
| ç¼ºä¹ä¸­æ–‡èµ„æ–™ | å…¨ä¸­æ–‡æ–‡æ¡£ï¼Œé€‚åˆå›½å†…å¼€å‘è€… |
| ä¸çŸ¥é“å¦‚ä½•é€‰æ‹©æŠ€æœ¯è·¯çº¿ | å¯¹æ¯”åˆ†æ SFT/DPO/RL çš„é€‚ç”¨åœºæ™¯ |
| æŒç»­å­¦ä¹ è¢«å¿½è§† | ä¸“é—¨ä¸¤å¤©è®²è§£ CL åŠå…¶åœ¨ LLM ä¸­çš„åº”ç”¨ |


### æ ¸å¿ƒä»·å€¼

1. **ç³»ç»Ÿæ€§**ï¼šè¦†ç›–åè®­ç»ƒå…¨æµç¨‹ï¼Œç†è§£æŠ€æœ¯é—´çš„ä¾èµ–å…³ç³»
2. **å®ç”¨æ€§**ï¼šåŸºäº QLoRA çš„å•å¡è®­ç»ƒæ–¹æ¡ˆï¼Œ4080 å³å¯è¿è¡Œ
3. **å‰æ²¿æ€§**ï¼šæ¶µç›– GRPOã€DAPOã€CPPO ç­‰æœ€æ–°ç®—æ³•
4. **å¯æ‰©å±•æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºæ‰©å±•å’Œå®šåˆ¶

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
PipelineLLM/
â”‚
â”œâ”€â”€ ğŸ“š LLM_Background/                    # åŸºç¡€çŸ¥è¯†
â”‚   â”œâ”€â”€ transformer/                      # Transformer ä»é›¶å®ç°
â”‚   â”‚   â”œâ”€â”€ transformer_model.py          # å®Œæ•´ Transformerï¼ˆå«äº¤å‰æ³¨æ„åŠ›ï¼‰
â”‚   â”‚   â””â”€â”€ simple_tokenizer.py           # ç®€æ˜“åˆ†è¯å™¨
â”‚   â””â”€â”€ gpu_mem/                          # æ˜¾å­˜è®¡ç®—ä¸ä¼˜åŒ–
â”‚
â”œâ”€â”€ ğŸ“– LLM_Intro/                         # LLM æ¶æ„å…¥é—¨
â”‚   â”œâ”€â”€ llm_archs/                        # ä¸»æµæ¶æ„è§£æï¼ˆGPT/LLaMA/DeepSeekï¼‰
â”‚   â””â”€â”€ llm_processes/                    # é¢„è®­ç»ƒä¸å¾®è°ƒæµç¨‹
â”‚
â”œâ”€â”€ ğŸ“ Post_train/                        # åè®­ç»ƒæ ¸å¿ƒè¯¾ç¨‹
â”‚   â”œâ”€â”€ Day 1_è¯¾ç¨‹æ€»è§ˆ.md                  # è¯¾ç¨‹å¯¼è§ˆä¸å­¦ä¹ è·¯çº¿
â”‚   â”œâ”€â”€ Day 2_SFT å®æˆ˜.md                  # ç›‘ç£å¾®è°ƒ
â”‚   â”œâ”€â”€ Day 3_åå¥½ä¼˜åŒ–.md                  # DPO ç†è®ºä¸å®è·µ
â”‚   â”œâ”€â”€ Day 4_å¼ºåŒ–å­¦ä¹ ï¼ˆä¸Šï¼‰.md             # RL ç†è®ºåŸºç¡€
â”‚   â”œâ”€â”€ Day 5_å¼ºåŒ–å­¦ä¹ ï¼ˆä¸‹ï¼‰.md             # LLM-RL å®æˆ˜
â”‚   â”œâ”€â”€ Day 6_æŒç»­å­¦ä¹ .md                  # CL ç»å…¸æ–¹æ³•
â”‚   â”œâ”€â”€ Day 7 LLMä¸æŒç»­å­¦ä¹ .md             # LLM-CL å‰æ²¿
â”‚   â”œâ”€â”€ Appendix_RL ç®—æ³•æ€»ç»“.md            # RL ç®—æ³•é€ŸæŸ¥è¡¨
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ”§ sft_qlora/                     # SFT å®æˆ˜ä»£ç 
â”‚   â”‚   â”œâ”€â”€ train.py                      # QLoRA è®­ç»ƒè„šæœ¬
â”‚   â”‚   â”œâ”€â”€ config.py                     # è®­ç»ƒé…ç½®
â”‚   â”‚   â””â”€â”€ merge_model.py                # æƒé‡åˆå¹¶
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ¯ dpo/                           # DPO å®æˆ˜ä»£ç 
â”‚   â”‚   â”œâ”€â”€ train_dpo.py                  # DPO è®­ç»ƒè„šæœ¬
â”‚   â”‚   â”œâ”€â”€ mini_dpo.py                   # æç®€ DPO å®ç°
â”‚   â”‚   â””â”€â”€ dataset/                      # åå¥½æ•°æ®å¤„ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ® rlhf/                          # å¼ºåŒ–å­¦ä¹ æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ Qlearning/                    # Q-Learning å…¥é—¨
â”‚   â”‚   â”œâ”€â”€ PPO/                          # PPO ç®—æ³•å®ç°
â”‚   â”‚   â”œâ”€â”€ GRPO/                         # GRPO ç®—æ³•å®ç°
â”‚   â”‚   â””â”€â”€ RewardModel/                  # å¥–åŠ±æ¨¡å‹è®­ç»ƒ
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“„ papers/                        # å‚è€ƒè®ºæ–‡
â”‚
â””â”€â”€ README.md                             # æœ¬æ–‡ä»¶
```

---

## è¯¾ç¨‹å†…å®¹

### ç¬¬ä¸€é˜¶æ®µï¼šæŒ‡ä»¤å¯¹é½

| Day | ä¸»é¢˜ | æ ¸å¿ƒå†…å®¹ | å…³é”®æŠ€æœ¯ |
|-----|------|----------|----------|
| **Day 2** | SFT ç›‘ç£å¾®è°ƒ | è®©æ¨¡å‹"å­¦ä¼šè¯´è¯" | QLoRA, Assistant-only Loss, Packing |
| **Day 3** | åå¥½ä¼˜åŒ– | è®©æ¨¡å‹"è¯´å¾—æ›´å¥½" | DPO, Bradley-Terry Model |

### ç¬¬äºŒé˜¶æ®µï¼šå¼ºåŒ–å­¦ä¹ 

| Day | ä¸»é¢˜ | æ ¸å¿ƒå†…å®¹ | å…³é”®æŠ€æœ¯ |
|-----|------|----------|----------|
| **Day 4** | RL ç†è®º | ç†è®ºæ­¦è£… | MDP, Bellman, ç­–ç•¥æ¢¯åº¦, PPO |
| **Day 5** | LLM-RL | å®æˆ˜è½åœ° | RLHF, å¥–åŠ±æ¨¡å‹, GRPO, DAPO |

### ç¬¬ä¸‰é˜¶æ®µï¼šæŒç»­è¿›åŒ–

| Day | ä¸»é¢˜ | æ ¸å¿ƒå†…å®¹ | å…³é”®æŠ€æœ¯ |
|-----|------|----------|----------|
| **Day 6** | æŒç»­å­¦ä¹  | å­¦æ–°ä¸å¿˜æ—§ | EWC, LwF, iCaRL, MoE |
| **Day 7** | LLM-CL | ç»ˆèº«å­¦ä¹  | CPT, CFT, CA, CPPO, InsCL |

---

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

```bash
# Python ç‰ˆæœ¬
Python >= 3.10

# æ ¸å¿ƒä¾èµ–
torch >= 2.1.0
transformers >= 4.40.0
peft >= 0.10.0
trl >= 0.8.0
bitsandbytes >= 0.43.0
datasets >= 2.18.0

# å¯é€‰ï¼šåŠ é€Ÿè®­ç»ƒ
unsloth  # 2x åŠ é€Ÿ
flash-attn  # FlashAttention
```

### å®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-username/PipelineLLM.git
cd PipelineLLM

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n pipelinellm python=3.10
conda activate pipelinellm

# å®‰è£…ä¾èµ–
pip install torch transformers peft trl bitsandbytes datasets
pip install unsloth  # å¯é€‰ï¼ŒåŠ é€Ÿè®­ç»ƒ
```

### è¿è¡Œç¤ºä¾‹

```bash
# 1. SFT è®­ç»ƒ
cd Post_train/sft_qlora
python train.py

# 2. DPO è®­ç»ƒ
cd Post_train/dpo
python train_dpo.py

# 3. Q-Learning å¯è§†åŒ–
cd Post_train/rlhf/Qlearning
python train_qlearning.py

# 4. å¥–åŠ±æ¨¡å‹è®­ç»ƒ
cd Post_train/rlhf/RewardModel
python train_reward_model.py
```

---

## æ ¸å¿ƒæŠ€æœ¯æ ˆ

### è®­ç»ƒæŠ€æœ¯

| æŠ€æœ¯ | ç”¨é€” | æ˜¾å­˜èŠ‚çœ |
|------|------|----------|
| **QLoRA** | 4-bit é‡åŒ– + LoRA | ~75% |
| **Gradient Checkpointing** | ç”¨è®¡ç®—æ¢æ˜¾å­˜ | ~30% |
| **Flash Attention** | é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®— | ~20% |
| **Packing** | å‡å°‘ padding æµªè´¹ | ~15% |

### ç®—æ³•å®ç°

| ç®—æ³• | æ–‡ä»¶ä½ç½® | è¯´æ˜ |
|------|----------|------|
| Transformer | `LLM_Background/transformer/` | ä»é›¶å®ç°ï¼Œå«äº¤å‰æ³¨æ„åŠ› |
| LoRA | `LLM_Intro/llm_processes/lora/` | ä½ç§©é€‚é…è¯¦è§£ |
| Q-Learning | `Post_train/rlhf/Qlearning/` | RL å…¥é—¨ |
| PPO | `Post_train/rlhf/PPO/` | ç­–ç•¥ä¼˜åŒ–æ ¸å¿ƒç®—æ³• |
| GRPO | `Post_train/rlhf/GRPO/` | ç»„å†…ç›¸å¯¹ä¼˜åŒ– |
| DPO | `Post_train/dpo/` | ç›´æ¥åå¥½ä¼˜åŒ– |
| Reward Model | `Post_train/rlhf/RewardModel/` | å¥–åŠ±æ¨¡å‹è®­ç»ƒ |

---

## å­¦ä¹ è·¯çº¿å›¾

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                    PipelineLLM å­¦ä¹ è·¯çº¿                  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                         â”‚                         â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
              â”‚  é˜¶æ®µä¸€    â”‚            â”‚  é˜¶æ®µäºŒ    â”‚            â”‚  é˜¶æ®µä¸‰    â”‚
              â”‚ æŒ‡ä»¤å¯¹é½   â”‚            â”‚  å¼ºåŒ–å­¦ä¹    â”‚            â”‚  æŒç»­è¿›åŒ–  â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                    â”‚                        â”‚                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                     â”‚    â”‚                 â”‚       â”‚                 â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚  Day 2  â”‚          â”‚  Day 3  â”‚    â”‚  Day 4  â”‚   â”‚  Day 5  â”‚    â”‚  Day 6  â”‚   â”‚  Day 7  â”‚
    â”‚   SFT   â”‚    â†’     â”‚   DPO   â”‚ â†’  â”‚ RL ç†è®º  â”‚ â†’ â”‚ LLM-RL  â”‚ â†’  â”‚   CL    â”‚ â†’ â”‚ LLM-CL  â”‚
    â”‚ ç›‘ç£å¾®è°ƒ â”‚           â”‚ åå¥½ä¼˜åŒ– â”‚    â”‚ åŸºç¡€ç†è®º â”‚   â”‚ å®æˆ˜åº”ç”¨ â”‚     â”‚ ç»å…¸æ–¹æ³• â”‚   â”‚ å‰æ²¿ç®—æ³• â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                     â†“              â†“            â†“               â†“             â†“
    "æ¨¡ä»¿ä¸“å®¶"              "åŒºåˆ†å¥½å"      "ç†è§£åŸç†"     "è·å¾—å¥–åŠ±"       "é˜²æ­¢é—å¿˜"     "æŒç»­è¿›åŒ–"
```

---

## å®éªŒè®°å½•

### ç¡¬ä»¶è¦æ±‚

| æ¨¡å‹è§„æ¨¡ | è®­ç»ƒæ–¹å¼ | æœ€ä½æ˜¾å­˜ | æ¨èæ˜¾å­˜ |
|----------|----------|----------|----------|
| 4B | QLoRA | 8 GB | 12 GB |
| 8B | QLoRA | 12 GB | 16 GB |
| 14B | QLoRA | 18 GB | 24 GB |
| 72B | QLoRA | 48 GB | 80 GB |

### åŸºå‡†æ¨¡å‹

æœ¬é¡¹ç›®ä¸»è¦ä½¿ç”¨ä»¥ä¸‹æ¨¡å‹è¿›è¡Œå®éªŒï¼š

- Qwen3-1.7B/**Qwen3-4B** / **Qwen3-8B**ï¼šé€‚åˆå•å¡è®­ç»ƒ

  

---

## ğŸ“š å‚è€ƒèµ„æ–™

### æ ¸å¿ƒè®ºæ–‡

1. [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (InstructGPT)
2. [Direct Preference Optimization](https://arxiv.org/abs/2305.18290) (DPO)
3. [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) (PPO)
4. [DeepSeekMath: Pushing the Limits of Mathematical Reasoning](https://arxiv.org/abs/2402.03300) (GRPO)
5. [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
6. [Overcoming catastrophic forgetting in neural networks](https://www.pnas.org/doi/10.1073/pnas.1611835114) (EWC)
7. [iCaRL: Incremental Classifier and Representation Learning](https://arxiv.org/abs/1611.07725)
8. [Continual Learning for Large Language Models: A Survey](https://arxiv.org/abs/2402.01364)

### æ¨èé˜…è¯»

- [Hugging Face TRL æ–‡æ¡£](https://huggingface.co/docs/trl)
- [Unsloth å®˜æ–¹æ–‡æ¡£](https://docs.unsloth.ai)
- [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)

---

## è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

<p align="center">
  <b>ğŸ¯ ç›®æ ‡ï¼šè®©æ¯ä¸ªäººéƒ½èƒ½è®­ç»ƒå‡ºè‡ªå·±çš„ LLM</b>
</p>
<p align="center">
  Made with â¤ï¸ by Haoran
</p>

